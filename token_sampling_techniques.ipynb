{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMpyUZMsmNIFEjHeHOWnhjz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndreSlavescu/Token-Sampling/blob/main/token_sampling_techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gumbel Max-Trick Sampling"
      ],
      "metadata": {
        "id": "nTOncJMpPYfc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ihPVIGIIPFoE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import urllib\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "response = urllib.request.urlopen(url)\n",
        "shakespeare_text = response.read().decode('utf-8')[:100000]\n",
        "\n",
        "chars = sorted(list(set(shakespeare_text)))\n",
        "vocab_size = len(chars)\n",
        "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
        "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "data = [char_to_idx[ch] for ch in shakespeare_text]\n",
        "\n",
        "seq_length = 64\n",
        "\n",
        "class DatasetLoader(Dataset):\n",
        "    def __init__(self, data, seq_length):\n",
        "        self.data = data\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.seq_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx:idx + self.seq_length]\n",
        "        y = self.data[idx + 1:idx + self.seq_length + 1]\n",
        "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "batch_size = seq_length\n",
        "dataset = DatasetLoader(data, seq_length)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
        "        super(LSTMLanguageModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        out, hidden = self.lstm(x, hidden)\n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        return (weight.new(self.lstm.num_layers, batch_size, self.lstm.hidden_size).zero_(),\n",
        "                weight.new(self.lstm.num_layers, batch_size, self.lstm.hidden_size).zero_())\n",
        "\n",
        "embedding_dim = 128\n",
        "hidden_dim = 256\n",
        "num_layers = 2\n",
        "model = LSTMLanguageModel(vocab_size, embedding_dim, hidden_dim, num_layers)"
      ],
      "metadata": {
        "id": "ed5Z4u9iP09x"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "num_epochs = 5\n",
        "learning_rate = 0.001\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    start_time = time.time()\n",
        "\n",
        "    total_loss = 0\n",
        "    batch_count = len(dataloader)\n",
        "\n",
        "    for batch_idx, (x, y) in enumerate(dataloader):\n",
        "        hidden = model.init_hidden(x.size(0))\n",
        "        hidden = tuple([h.data for h in hidden])\n",
        "\n",
        "        model.zero_grad()\n",
        "        output, hidden = model(x, hidden)\n",
        "        loss = criterion(output.view(-1, vocab_size), y.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            avg_loss = total_loss / (batch_idx + 1)\n",
        "            elapsed_time = time.time() - start_time\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{batch_count}], '\n",
        "                  f'Loss: {loss.item():.4f}, Avg Loss: {avg_loss:.4f}, Time: {elapsed_time:.2f}s')\n",
        "\n",
        "    avg_epoch_loss = total_loss / batch_count\n",
        "    print(f'End of Epoch [{epoch+1}/{num_epochs}], Avg Loss: {avg_epoch_loss:.4f}, '\n",
        "          f'Total Time: {time.time() - start_time:.2f}s')\n",
        "\n",
        "torch.save(model.state_dict(), 'gumbel_sampling_shakespeare_lstm.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Qh-NERiP91k",
        "outputId": "1277e358-95fc-45ac-d8db-602df6adecf8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Batch [10/1562], Loss: 3.3346, Avg Loss: 3.7402, Time: 1.29s\n",
            "Epoch [1/5], Batch [20/1562], Loss: 3.2499, Avg Loss: 3.5262, Time: 2.57s\n",
            "Epoch [1/5], Batch [30/1562], Loss: 3.0540, Avg Loss: 3.4009, Time: 3.87s\n",
            "Epoch [1/5], Batch [40/1562], Loss: 2.8428, Avg Loss: 3.2850, Time: 5.13s\n",
            "Epoch [1/5], Batch [50/1562], Loss: 2.5955, Avg Loss: 3.1703, Time: 6.68s\n",
            "Epoch [1/5], Batch [60/1562], Loss: 2.4825, Avg Loss: 3.0662, Time: 8.18s\n",
            "Epoch [1/5], Batch [70/1562], Loss: 2.3920, Avg Loss: 2.9747, Time: 9.46s\n",
            "Epoch [1/5], Batch [80/1562], Loss: 2.2790, Avg Loss: 2.8958, Time: 10.71s\n",
            "Epoch [1/5], Batch [90/1562], Loss: 2.2375, Avg Loss: 2.8270, Time: 11.97s\n",
            "Epoch [1/5], Batch [100/1562], Loss: 2.1851, Avg Loss: 2.7663, Time: 13.25s\n",
            "Epoch [1/5], Batch [110/1562], Loss: 2.1339, Avg Loss: 2.7122, Time: 14.52s\n",
            "Epoch [1/5], Batch [120/1562], Loss: 2.1024, Avg Loss: 2.6639, Time: 15.78s\n",
            "Epoch [1/5], Batch [130/1562], Loss: 2.0444, Avg Loss: 2.6188, Time: 17.05s\n",
            "Epoch [1/5], Batch [140/1562], Loss: 2.0374, Avg Loss: 2.5776, Time: 18.44s\n",
            "Epoch [1/5], Batch [150/1562], Loss: 2.0430, Avg Loss: 2.5397, Time: 20.04s\n",
            "Epoch [1/5], Batch [160/1562], Loss: 1.9468, Avg Loss: 2.5049, Time: 21.34s\n",
            "Epoch [1/5], Batch [170/1562], Loss: 1.9258, Avg Loss: 2.4713, Time: 22.63s\n",
            "Epoch [1/5], Batch [180/1562], Loss: 1.9254, Avg Loss: 2.4406, Time: 23.96s\n",
            "Epoch [1/5], Batch [190/1562], Loss: 1.9139, Avg Loss: 2.4120, Time: 25.26s\n",
            "Epoch [1/5], Batch [200/1562], Loss: 1.8828, Avg Loss: 2.3845, Time: 26.58s\n",
            "Epoch [1/5], Batch [210/1562], Loss: 1.8349, Avg Loss: 2.3587, Time: 27.89s\n",
            "Epoch [1/5], Batch [220/1562], Loss: 1.8794, Avg Loss: 2.3346, Time: 29.20s\n",
            "Epoch [1/5], Batch [230/1562], Loss: 1.7413, Avg Loss: 2.3117, Time: 30.58s\n",
            "Epoch [1/5], Batch [240/1562], Loss: 1.7664, Avg Loss: 2.2899, Time: 32.17s\n",
            "Epoch [1/5], Batch [250/1562], Loss: 1.8048, Avg Loss: 2.2692, Time: 33.51s\n",
            "Epoch [1/5], Batch [260/1562], Loss: 1.7560, Avg Loss: 2.2493, Time: 34.79s\n",
            "Epoch [1/5], Batch [270/1562], Loss: 1.7135, Avg Loss: 2.2300, Time: 36.06s\n",
            "Epoch [1/5], Batch [280/1562], Loss: 1.6554, Avg Loss: 2.2116, Time: 37.33s\n",
            "Epoch [1/5], Batch [290/1562], Loss: 1.7230, Avg Loss: 2.1939, Time: 38.57s\n",
            "Epoch [1/5], Batch [300/1562], Loss: 1.6590, Avg Loss: 2.1770, Time: 39.85s\n",
            "Epoch [1/5], Batch [310/1562], Loss: 1.6770, Avg Loss: 2.1607, Time: 41.12s\n",
            "Epoch [1/5], Batch [320/1562], Loss: 1.6879, Avg Loss: 2.1450, Time: 42.39s\n",
            "Epoch [1/5], Batch [330/1562], Loss: 1.6135, Avg Loss: 2.1300, Time: 44.00s\n",
            "Epoch [1/5], Batch [340/1562], Loss: 1.6771, Avg Loss: 2.1154, Time: 45.46s\n",
            "Epoch [1/5], Batch [350/1562], Loss: 1.6166, Avg Loss: 2.1014, Time: 46.73s\n",
            "Epoch [1/5], Batch [360/1562], Loss: 1.6003, Avg Loss: 2.0880, Time: 48.02s\n",
            "Epoch [1/5], Batch [370/1562], Loss: 1.5283, Avg Loss: 2.0744, Time: 49.32s\n",
            "Epoch [1/5], Batch [380/1562], Loss: 1.5727, Avg Loss: 2.0618, Time: 50.64s\n",
            "Epoch [1/5], Batch [390/1562], Loss: 1.5326, Avg Loss: 2.0499, Time: 51.98s\n",
            "Epoch [1/5], Batch [400/1562], Loss: 1.5808, Avg Loss: 2.0380, Time: 53.30s\n",
            "Epoch [1/5], Batch [410/1562], Loss: 1.5921, Avg Loss: 2.0268, Time: 54.60s\n",
            "Epoch [1/5], Batch [420/1562], Loss: 1.5632, Avg Loss: 2.0155, Time: 56.17s\n",
            "Epoch [1/5], Batch [430/1562], Loss: 1.5025, Avg Loss: 2.0044, Time: 57.68s\n",
            "Epoch [1/5], Batch [440/1562], Loss: 1.4765, Avg Loss: 1.9939, Time: 58.96s\n",
            "Epoch [1/5], Batch [450/1562], Loss: 1.5240, Avg Loss: 1.9835, Time: 60.25s\n",
            "Epoch [1/5], Batch [460/1562], Loss: 1.4780, Avg Loss: 1.9731, Time: 61.56s\n",
            "Epoch [1/5], Batch [470/1562], Loss: 1.5008, Avg Loss: 1.9630, Time: 62.88s\n",
            "Epoch [1/5], Batch [480/1562], Loss: 1.4840, Avg Loss: 1.9534, Time: 64.21s\n",
            "Epoch [1/5], Batch [490/1562], Loss: 1.4856, Avg Loss: 1.9440, Time: 65.52s\n",
            "Epoch [1/5], Batch [500/1562], Loss: 1.4340, Avg Loss: 1.9346, Time: 66.84s\n",
            "Epoch [1/5], Batch [510/1562], Loss: 1.4657, Avg Loss: 1.9255, Time: 68.39s\n",
            "Epoch [1/5], Batch [520/1562], Loss: 1.5000, Avg Loss: 1.9169, Time: 69.93s\n",
            "Epoch [1/5], Batch [530/1562], Loss: 1.5148, Avg Loss: 1.9085, Time: 71.21s\n",
            "Epoch [1/5], Batch [540/1562], Loss: 1.4832, Avg Loss: 1.9002, Time: 72.48s\n",
            "Epoch [1/5], Batch [550/1562], Loss: 1.4456, Avg Loss: 1.8920, Time: 73.79s\n",
            "Epoch [1/5], Batch [560/1562], Loss: 1.4658, Avg Loss: 1.8841, Time: 75.12s\n",
            "Epoch [1/5], Batch [570/1562], Loss: 1.4432, Avg Loss: 1.8765, Time: 76.43s\n",
            "Epoch [1/5], Batch [580/1562], Loss: 1.4388, Avg Loss: 1.8688, Time: 77.73s\n",
            "Epoch [1/5], Batch [590/1562], Loss: 1.3703, Avg Loss: 1.8610, Time: 79.03s\n",
            "Epoch [1/5], Batch [600/1562], Loss: 1.4049, Avg Loss: 1.8535, Time: 80.51s\n",
            "Epoch [1/5], Batch [610/1562], Loss: 1.4317, Avg Loss: 1.8460, Time: 82.08s\n",
            "Epoch [1/5], Batch [620/1562], Loss: 1.3772, Avg Loss: 1.8388, Time: 83.37s\n",
            "Epoch [1/5], Batch [630/1562], Loss: 1.3652, Avg Loss: 1.8319, Time: 84.64s\n",
            "Epoch [1/5], Batch [640/1562], Loss: 1.3787, Avg Loss: 1.8249, Time: 85.94s\n",
            "Epoch [1/5], Batch [650/1562], Loss: 1.4031, Avg Loss: 1.8177, Time: 87.21s\n",
            "Epoch [1/5], Batch [660/1562], Loss: 1.3297, Avg Loss: 1.8109, Time: 88.51s\n",
            "Epoch [1/5], Batch [670/1562], Loss: 1.3970, Avg Loss: 1.8043, Time: 89.82s\n",
            "Epoch [1/5], Batch [680/1562], Loss: 1.3633, Avg Loss: 1.7976, Time: 91.13s\n",
            "Epoch [1/5], Batch [690/1562], Loss: 1.3535, Avg Loss: 1.7913, Time: 92.57s\n",
            "Epoch [1/5], Batch [700/1562], Loss: 1.3244, Avg Loss: 1.7848, Time: 94.22s\n",
            "Epoch [1/5], Batch [710/1562], Loss: 1.2809, Avg Loss: 1.7787, Time: 95.57s\n",
            "Epoch [1/5], Batch [720/1562], Loss: 1.3652, Avg Loss: 1.7727, Time: 96.89s\n",
            "Epoch [1/5], Batch [730/1562], Loss: 1.3281, Avg Loss: 1.7669, Time: 98.22s\n",
            "Epoch [1/5], Batch [740/1562], Loss: 1.3546, Avg Loss: 1.7609, Time: 99.53s\n",
            "Epoch [1/5], Batch [750/1562], Loss: 1.2971, Avg Loss: 1.7550, Time: 100.83s\n",
            "Epoch [1/5], Batch [760/1562], Loss: 1.2827, Avg Loss: 1.7491, Time: 102.15s\n",
            "Epoch [1/5], Batch [770/1562], Loss: 1.3237, Avg Loss: 1.7433, Time: 103.43s\n",
            "Epoch [1/5], Batch [780/1562], Loss: 1.2686, Avg Loss: 1.7376, Time: 104.78s\n",
            "Epoch [1/5], Batch [790/1562], Loss: 1.2865, Avg Loss: 1.7320, Time: 106.37s\n",
            "Epoch [1/5], Batch [800/1562], Loss: 1.2637, Avg Loss: 1.7263, Time: 107.73s\n",
            "Epoch [1/5], Batch [810/1562], Loss: 1.2721, Avg Loss: 1.7207, Time: 109.01s\n",
            "Epoch [1/5], Batch [820/1562], Loss: 1.2506, Avg Loss: 1.7155, Time: 110.28s\n",
            "Epoch [1/5], Batch [830/1562], Loss: 1.2740, Avg Loss: 1.7101, Time: 111.55s\n",
            "Epoch [1/5], Batch [840/1562], Loss: 1.2654, Avg Loss: 1.7048, Time: 112.80s\n",
            "Epoch [1/5], Batch [850/1562], Loss: 1.2374, Avg Loss: 1.6994, Time: 114.07s\n",
            "Epoch [1/5], Batch [860/1562], Loss: 1.1899, Avg Loss: 1.6940, Time: 115.38s\n",
            "Epoch [1/5], Batch [870/1562], Loss: 1.2385, Avg Loss: 1.6888, Time: 116.65s\n",
            "Epoch [1/5], Batch [880/1562], Loss: 1.2620, Avg Loss: 1.6838, Time: 118.29s\n",
            "Epoch [1/5], Batch [890/1562], Loss: 1.1737, Avg Loss: 1.6787, Time: 119.77s\n",
            "Epoch [1/5], Batch [900/1562], Loss: 1.2663, Avg Loss: 1.6738, Time: 121.07s\n",
            "Epoch [1/5], Batch [910/1562], Loss: 1.2217, Avg Loss: 1.6688, Time: 122.36s\n",
            "Epoch [1/5], Batch [920/1562], Loss: 1.2525, Avg Loss: 1.6640, Time: 123.65s\n",
            "Epoch [1/5], Batch [930/1562], Loss: 1.2471, Avg Loss: 1.6592, Time: 124.94s\n",
            "Epoch [1/5], Batch [940/1562], Loss: 1.1933, Avg Loss: 1.6543, Time: 126.18s\n",
            "Epoch [1/5], Batch [950/1562], Loss: 1.2324, Avg Loss: 1.6497, Time: 127.46s\n",
            "Epoch [1/5], Batch [960/1562], Loss: 1.2106, Avg Loss: 1.6449, Time: 128.73s\n",
            "Epoch [1/5], Batch [970/1562], Loss: 1.1650, Avg Loss: 1.6402, Time: 130.20s\n",
            "Epoch [1/5], Batch [980/1562], Loss: 1.1757, Avg Loss: 1.6357, Time: 131.77s\n",
            "Epoch [1/5], Batch [990/1562], Loss: 1.1392, Avg Loss: 1.6310, Time: 133.07s\n",
            "Epoch [1/5], Batch [1000/1562], Loss: 1.1834, Avg Loss: 1.6264, Time: 134.33s\n",
            "Epoch [1/5], Batch [1010/1562], Loss: 1.1743, Avg Loss: 1.6218, Time: 135.59s\n",
            "Epoch [1/5], Batch [1020/1562], Loss: 1.1694, Avg Loss: 1.6174, Time: 136.84s\n",
            "Epoch [1/5], Batch [1030/1562], Loss: 1.1321, Avg Loss: 1.6130, Time: 138.10s\n",
            "Epoch [1/5], Batch [1040/1562], Loss: 1.1250, Avg Loss: 1.6086, Time: 139.37s\n",
            "Epoch [1/5], Batch [1050/1562], Loss: 1.1703, Avg Loss: 1.6041, Time: 140.67s\n",
            "Epoch [1/5], Batch [1060/1562], Loss: 1.1517, Avg Loss: 1.5998, Time: 142.03s\n",
            "Epoch [1/5], Batch [1070/1562], Loss: 1.1508, Avg Loss: 1.5955, Time: 143.63s\n",
            "Epoch [1/5], Batch [1080/1562], Loss: 1.1587, Avg Loss: 1.5911, Time: 144.94s\n",
            "Epoch [1/5], Batch [1090/1562], Loss: 1.1150, Avg Loss: 1.5868, Time: 146.23s\n",
            "Epoch [1/5], Batch [1100/1562], Loss: 1.1178, Avg Loss: 1.5824, Time: 147.52s\n",
            "Epoch [1/5], Batch [1110/1562], Loss: 1.1169, Avg Loss: 1.5782, Time: 148.81s\n",
            "Epoch [1/5], Batch [1120/1562], Loss: 1.1063, Avg Loss: 1.5740, Time: 150.09s\n",
            "Epoch [1/5], Batch [1130/1562], Loss: 1.0462, Avg Loss: 1.5697, Time: 151.38s\n",
            "Epoch [1/5], Batch [1140/1562], Loss: 1.0775, Avg Loss: 1.5655, Time: 152.64s\n",
            "Epoch [1/5], Batch [1150/1562], Loss: 1.0609, Avg Loss: 1.5612, Time: 153.88s\n",
            "Epoch [1/5], Batch [1160/1562], Loss: 1.0912, Avg Loss: 1.5571, Time: 155.44s\n",
            "Epoch [1/5], Batch [1170/1562], Loss: 1.1098, Avg Loss: 1.5530, Time: 156.91s\n",
            "Epoch [1/5], Batch [1180/1562], Loss: 1.0812, Avg Loss: 1.5489, Time: 158.18s\n",
            "Epoch [1/5], Batch [1190/1562], Loss: 1.0785, Avg Loss: 1.5449, Time: 159.43s\n",
            "Epoch [1/5], Batch [1200/1562], Loss: 1.0706, Avg Loss: 1.5410, Time: 160.68s\n",
            "Epoch [1/5], Batch [1210/1562], Loss: 1.0891, Avg Loss: 1.5370, Time: 161.95s\n",
            "Epoch [1/5], Batch [1220/1562], Loss: 1.0772, Avg Loss: 1.5330, Time: 163.24s\n",
            "Epoch [1/5], Batch [1230/1562], Loss: 1.0689, Avg Loss: 1.5290, Time: 164.53s\n",
            "Epoch [1/5], Batch [1240/1562], Loss: 1.0764, Avg Loss: 1.5251, Time: 165.84s\n",
            "Epoch [1/5], Batch [1250/1562], Loss: 1.0041, Avg Loss: 1.5213, Time: 167.37s\n",
            "Epoch [1/5], Batch [1260/1562], Loss: 1.0033, Avg Loss: 1.5174, Time: 168.95s\n",
            "Epoch [1/5], Batch [1270/1562], Loss: 1.0270, Avg Loss: 1.5135, Time: 170.23s\n",
            "Epoch [1/5], Batch [1280/1562], Loss: 1.0682, Avg Loss: 1.5097, Time: 171.51s\n",
            "Epoch [1/5], Batch [1290/1562], Loss: 1.0034, Avg Loss: 1.5059, Time: 172.82s\n",
            "Epoch [1/5], Batch [1300/1562], Loss: 0.9943, Avg Loss: 1.5021, Time: 174.10s\n",
            "Epoch [1/5], Batch [1310/1562], Loss: 0.9846, Avg Loss: 1.4981, Time: 175.38s\n",
            "Epoch [1/5], Batch [1320/1562], Loss: 1.0345, Avg Loss: 1.4943, Time: 176.65s\n",
            "Epoch [1/5], Batch [1330/1562], Loss: 0.9643, Avg Loss: 1.4904, Time: 177.89s\n",
            "Epoch [1/5], Batch [1340/1562], Loss: 0.9538, Avg Loss: 1.4866, Time: 179.27s\n",
            "Epoch [1/5], Batch [1350/1562], Loss: 0.9839, Avg Loss: 1.4829, Time: 180.82s\n",
            "Epoch [1/5], Batch [1360/1562], Loss: 0.9541, Avg Loss: 1.4793, Time: 182.10s\n",
            "Epoch [1/5], Batch [1370/1562], Loss: 0.9719, Avg Loss: 1.4756, Time: 183.36s\n",
            "Epoch [1/5], Batch [1380/1562], Loss: 0.9692, Avg Loss: 1.4719, Time: 184.62s\n",
            "Epoch [1/5], Batch [1390/1562], Loss: 0.9775, Avg Loss: 1.4683, Time: 185.91s\n",
            "Epoch [1/5], Batch [1400/1562], Loss: 0.9748, Avg Loss: 1.4646, Time: 187.17s\n",
            "Epoch [1/5], Batch [1410/1562], Loss: 0.9950, Avg Loss: 1.4610, Time: 188.43s\n",
            "Epoch [1/5], Batch [1420/1562], Loss: 0.9458, Avg Loss: 1.4574, Time: 189.72s\n",
            "Epoch [1/5], Batch [1430/1562], Loss: 0.9494, Avg Loss: 1.4538, Time: 190.97s\n",
            "Epoch [1/5], Batch [1440/1562], Loss: 0.9486, Avg Loss: 1.4503, Time: 192.54s\n",
            "Epoch [1/5], Batch [1450/1562], Loss: 0.9179, Avg Loss: 1.4467, Time: 193.96s\n",
            "Epoch [1/5], Batch [1460/1562], Loss: 0.9163, Avg Loss: 1.4431, Time: 195.20s\n",
            "Epoch [1/5], Batch [1470/1562], Loss: 0.9037, Avg Loss: 1.4395, Time: 196.48s\n",
            "Epoch [1/5], Batch [1480/1562], Loss: 0.9170, Avg Loss: 1.4359, Time: 197.73s\n",
            "Epoch [1/5], Batch [1490/1562], Loss: 0.8920, Avg Loss: 1.4323, Time: 198.99s\n",
            "Epoch [1/5], Batch [1500/1562], Loss: 0.9164, Avg Loss: 1.4288, Time: 200.25s\n",
            "Epoch [1/5], Batch [1510/1562], Loss: 0.9237, Avg Loss: 1.4253, Time: 201.49s\n",
            "Epoch [1/5], Batch [1520/1562], Loss: 0.8802, Avg Loss: 1.4218, Time: 202.77s\n",
            "Epoch [1/5], Batch [1530/1562], Loss: 0.8989, Avg Loss: 1.4183, Time: 204.27s\n",
            "Epoch [1/5], Batch [1540/1562], Loss: 0.8825, Avg Loss: 1.4148, Time: 205.84s\n",
            "Epoch [1/5], Batch [1550/1562], Loss: 0.8604, Avg Loss: 1.4113, Time: 207.11s\n",
            "Epoch [1/5], Batch [1560/1562], Loss: 0.8798, Avg Loss: 1.4078, Time: 208.39s\n",
            "End of Epoch [1/5], Avg Loss: 1.4071, Total Time: 208.59s\n",
            "Epoch [2/5], Batch [10/1562], Loss: 0.8589, Avg Loss: 0.8570, Time: 1.27s\n",
            "Epoch [2/5], Batch [20/1562], Loss: 0.8436, Avg Loss: 0.8557, Time: 2.52s\n",
            "Epoch [2/5], Batch [30/1562], Loss: 0.8402, Avg Loss: 0.8501, Time: 3.77s\n",
            "Epoch [2/5], Batch [40/1562], Loss: 0.8575, Avg Loss: 0.8466, Time: 5.07s\n",
            "Epoch [2/5], Batch [50/1562], Loss: 0.8583, Avg Loss: 0.8441, Time: 6.37s\n",
            "Epoch [2/5], Batch [60/1562], Loss: 0.8079, Avg Loss: 0.8412, Time: 7.84s\n",
            "Epoch [2/5], Batch [70/1562], Loss: 0.8155, Avg Loss: 0.8372, Time: 9.45s\n",
            "Epoch [2/5], Batch [80/1562], Loss: 0.8041, Avg Loss: 0.8342, Time: 10.71s\n",
            "Epoch [2/5], Batch [90/1562], Loss: 0.8077, Avg Loss: 0.8314, Time: 11.98s\n",
            "Epoch [2/5], Batch [100/1562], Loss: 0.8106, Avg Loss: 0.8288, Time: 13.23s\n",
            "Epoch [2/5], Batch [110/1562], Loss: 0.7922, Avg Loss: 0.8263, Time: 14.51s\n",
            "Epoch [2/5], Batch [120/1562], Loss: 0.8120, Avg Loss: 0.8240, Time: 15.79s\n",
            "Epoch [2/5], Batch [130/1562], Loss: 0.8079, Avg Loss: 0.8215, Time: 17.07s\n",
            "Epoch [2/5], Batch [140/1562], Loss: 0.7707, Avg Loss: 0.8191, Time: 18.38s\n",
            "Epoch [2/5], Batch [150/1562], Loss: 0.7591, Avg Loss: 0.8162, Time: 19.74s\n",
            "Epoch [2/5], Batch [160/1562], Loss: 0.7618, Avg Loss: 0.8137, Time: 21.32s\n",
            "Epoch [2/5], Batch [170/1562], Loss: 0.7349, Avg Loss: 0.8111, Time: 22.66s\n",
            "Epoch [2/5], Batch [180/1562], Loss: 0.7922, Avg Loss: 0.8094, Time: 23.93s\n",
            "Epoch [2/5], Batch [190/1562], Loss: 0.7384, Avg Loss: 0.8067, Time: 25.18s\n",
            "Epoch [2/5], Batch [200/1562], Loss: 0.7715, Avg Loss: 0.8044, Time: 26.45s\n",
            "Epoch [2/5], Batch [210/1562], Loss: 0.7537, Avg Loss: 0.8020, Time: 27.71s\n",
            "Epoch [2/5], Batch [220/1562], Loss: 0.7556, Avg Loss: 0.7995, Time: 28.97s\n",
            "Epoch [2/5], Batch [230/1562], Loss: 0.7312, Avg Loss: 0.7969, Time: 30.29s\n",
            "Epoch [2/5], Batch [240/1562], Loss: 0.7312, Avg Loss: 0.7945, Time: 31.57s\n",
            "Epoch [2/5], Batch [250/1562], Loss: 0.7447, Avg Loss: 0.7924, Time: 33.13s\n",
            "Epoch [2/5], Batch [260/1562], Loss: 0.7365, Avg Loss: 0.7900, Time: 34.61s\n",
            "Epoch [2/5], Batch [270/1562], Loss: 0.7199, Avg Loss: 0.7877, Time: 35.89s\n",
            "Epoch [2/5], Batch [280/1562], Loss: 0.7307, Avg Loss: 0.7851, Time: 37.15s\n",
            "Epoch [2/5], Batch [290/1562], Loss: 0.7125, Avg Loss: 0.7826, Time: 38.40s\n",
            "Epoch [2/5], Batch [300/1562], Loss: 0.6744, Avg Loss: 0.7801, Time: 39.67s\n",
            "Epoch [2/5], Batch [310/1562], Loss: 0.7025, Avg Loss: 0.7779, Time: 40.94s\n",
            "Epoch [2/5], Batch [320/1562], Loss: 0.7069, Avg Loss: 0.7756, Time: 42.21s\n",
            "Epoch [2/5], Batch [330/1562], Loss: 0.6887, Avg Loss: 0.7731, Time: 43.43s\n",
            "Epoch [2/5], Batch [340/1562], Loss: 0.7048, Avg Loss: 0.7709, Time: 44.86s\n",
            "Epoch [2/5], Batch [350/1562], Loss: 0.6535, Avg Loss: 0.7682, Time: 46.46s\n",
            "Epoch [2/5], Batch [360/1562], Loss: 0.6938, Avg Loss: 0.7658, Time: 47.72s\n",
            "Epoch [2/5], Batch [370/1562], Loss: 0.6665, Avg Loss: 0.7637, Time: 48.97s\n",
            "Epoch [2/5], Batch [380/1562], Loss: 0.6671, Avg Loss: 0.7615, Time: 50.22s\n",
            "Epoch [2/5], Batch [390/1562], Loss: 0.6376, Avg Loss: 0.7592, Time: 51.48s\n",
            "Epoch [2/5], Batch [400/1562], Loss: 0.6672, Avg Loss: 0.7570, Time: 52.79s\n",
            "Epoch [2/5], Batch [410/1562], Loss: 0.6574, Avg Loss: 0.7547, Time: 54.06s\n",
            "Epoch [2/5], Batch [420/1562], Loss: 0.6587, Avg Loss: 0.7522, Time: 55.34s\n",
            "Epoch [2/5], Batch [430/1562], Loss: 0.6558, Avg Loss: 0.7501, Time: 56.67s\n",
            "Epoch [2/5], Batch [440/1562], Loss: 0.6729, Avg Loss: 0.7479, Time: 58.28s\n",
            "Epoch [2/5], Batch [450/1562], Loss: 0.6427, Avg Loss: 0.7456, Time: 59.66s\n",
            "Epoch [2/5], Batch [460/1562], Loss: 0.6447, Avg Loss: 0.7434, Time: 60.92s\n",
            "Epoch [2/5], Batch [470/1562], Loss: 0.6411, Avg Loss: 0.7412, Time: 62.19s\n",
            "Epoch [2/5], Batch [480/1562], Loss: 0.6378, Avg Loss: 0.7389, Time: 63.50s\n",
            "Epoch [2/5], Batch [490/1562], Loss: 0.6142, Avg Loss: 0.7367, Time: 64.77s\n",
            "Epoch [2/5], Batch [500/1562], Loss: 0.6153, Avg Loss: 0.7344, Time: 66.04s\n",
            "Epoch [2/5], Batch [510/1562], Loss: 0.6108, Avg Loss: 0.7322, Time: 67.30s\n",
            "Epoch [2/5], Batch [520/1562], Loss: 0.6197, Avg Loss: 0.7299, Time: 68.56s\n",
            "Epoch [2/5], Batch [530/1562], Loss: 0.6134, Avg Loss: 0.7276, Time: 70.09s\n",
            "Epoch [2/5], Batch [540/1562], Loss: 0.6227, Avg Loss: 0.7255, Time: 71.55s\n",
            "Epoch [2/5], Batch [550/1562], Loss: 0.5753, Avg Loss: 0.7233, Time: 72.82s\n",
            "Epoch [2/5], Batch [560/1562], Loss: 0.6023, Avg Loss: 0.7213, Time: 74.10s\n",
            "Epoch [2/5], Batch [570/1562], Loss: 0.6095, Avg Loss: 0.7191, Time: 75.39s\n",
            "Epoch [2/5], Batch [580/1562], Loss: 0.5790, Avg Loss: 0.7169, Time: 76.66s\n",
            "Epoch [2/5], Batch [590/1562], Loss: 0.5776, Avg Loss: 0.7148, Time: 77.93s\n",
            "Epoch [2/5], Batch [600/1562], Loss: 0.5801, Avg Loss: 0.7127, Time: 79.22s\n",
            "Epoch [2/5], Batch [610/1562], Loss: 0.5900, Avg Loss: 0.7104, Time: 80.48s\n",
            "Epoch [2/5], Batch [620/1562], Loss: 0.5572, Avg Loss: 0.7083, Time: 81.94s\n",
            "Epoch [2/5], Batch [630/1562], Loss: 0.5592, Avg Loss: 0.7062, Time: 83.49s\n",
            "Epoch [2/5], Batch [640/1562], Loss: 0.5793, Avg Loss: 0.7041, Time: 84.74s\n",
            "Epoch [2/5], Batch [650/1562], Loss: 0.5823, Avg Loss: 0.7020, Time: 86.04s\n",
            "Epoch [2/5], Batch [660/1562], Loss: 0.5550, Avg Loss: 0.6999, Time: 87.32s\n",
            "Epoch [2/5], Batch [670/1562], Loss: 0.5781, Avg Loss: 0.6978, Time: 88.58s\n",
            "Epoch [2/5], Batch [680/1562], Loss: 0.5593, Avg Loss: 0.6956, Time: 89.87s\n",
            "Epoch [2/5], Batch [690/1562], Loss: 0.5396, Avg Loss: 0.6935, Time: 91.13s\n",
            "Epoch [2/5], Batch [700/1562], Loss: 0.5549, Avg Loss: 0.6914, Time: 92.38s\n",
            "Epoch [2/5], Batch [710/1562], Loss: 0.5439, Avg Loss: 0.6894, Time: 93.74s\n",
            "Epoch [2/5], Batch [720/1562], Loss: 0.5159, Avg Loss: 0.6872, Time: 95.30s\n",
            "Epoch [2/5], Batch [730/1562], Loss: 0.5159, Avg Loss: 0.6850, Time: 96.67s\n",
            "Epoch [2/5], Batch [740/1562], Loss: 0.5337, Avg Loss: 0.6830, Time: 97.94s\n",
            "Epoch [2/5], Batch [750/1562], Loss: 0.5171, Avg Loss: 0.6810, Time: 99.21s\n",
            "Epoch [2/5], Batch [760/1562], Loss: 0.5384, Avg Loss: 0.6790, Time: 100.47s\n",
            "Epoch [2/5], Batch [770/1562], Loss: 0.5191, Avg Loss: 0.6770, Time: 101.73s\n",
            "Epoch [2/5], Batch [780/1562], Loss: 0.5164, Avg Loss: 0.6750, Time: 103.00s\n",
            "Epoch [2/5], Batch [790/1562], Loss: 0.4961, Avg Loss: 0.6730, Time: 104.26s\n",
            "Epoch [2/5], Batch [800/1562], Loss: 0.5197, Avg Loss: 0.6711, Time: 105.54s\n",
            "Epoch [2/5], Batch [810/1562], Loss: 0.5000, Avg Loss: 0.6692, Time: 107.13s\n",
            "Epoch [2/5], Batch [820/1562], Loss: 0.4946, Avg Loss: 0.6672, Time: 108.59s\n",
            "Epoch [2/5], Batch [830/1562], Loss: 0.5105, Avg Loss: 0.6653, Time: 109.86s\n",
            "Epoch [2/5], Batch [840/1562], Loss: 0.5284, Avg Loss: 0.6633, Time: 111.13s\n",
            "Epoch [2/5], Batch [850/1562], Loss: 0.5180, Avg Loss: 0.6613, Time: 112.42s\n",
            "Epoch [2/5], Batch [860/1562], Loss: 0.4829, Avg Loss: 0.6594, Time: 113.71s\n",
            "Epoch [2/5], Batch [870/1562], Loss: 0.5070, Avg Loss: 0.6574, Time: 114.99s\n",
            "Epoch [2/5], Batch [880/1562], Loss: 0.4925, Avg Loss: 0.6556, Time: 116.27s\n",
            "Epoch [2/5], Batch [890/1562], Loss: 0.4765, Avg Loss: 0.6537, Time: 117.53s\n",
            "Epoch [2/5], Batch [900/1562], Loss: 0.4902, Avg Loss: 0.6518, Time: 119.04s\n",
            "Epoch [2/5], Batch [910/1562], Loss: 0.4578, Avg Loss: 0.6499, Time: 120.63s\n",
            "Epoch [2/5], Batch [920/1562], Loss: 0.4784, Avg Loss: 0.6481, Time: 121.88s\n",
            "Epoch [2/5], Batch [930/1562], Loss: 0.4929, Avg Loss: 0.6463, Time: 123.14s\n",
            "Epoch [2/5], Batch [940/1562], Loss: 0.4612, Avg Loss: 0.6445, Time: 124.44s\n",
            "Epoch [2/5], Batch [950/1562], Loss: 0.4587, Avg Loss: 0.6427, Time: 125.70s\n",
            "Epoch [2/5], Batch [960/1562], Loss: 0.4705, Avg Loss: 0.6409, Time: 126.96s\n",
            "Epoch [2/5], Batch [970/1562], Loss: 0.4296, Avg Loss: 0.6391, Time: 128.23s\n",
            "Epoch [2/5], Batch [980/1562], Loss: 0.4536, Avg Loss: 0.6373, Time: 129.47s\n",
            "Epoch [2/5], Batch [990/1562], Loss: 0.4609, Avg Loss: 0.6354, Time: 130.79s\n",
            "Epoch [2/5], Batch [1000/1562], Loss: 0.4384, Avg Loss: 0.6336, Time: 132.38s\n",
            "Epoch [2/5], Batch [1010/1562], Loss: 0.4843, Avg Loss: 0.6319, Time: 133.69s\n",
            "Epoch [2/5], Batch [1020/1562], Loss: 0.4639, Avg Loss: 0.6301, Time: 134.94s\n",
            "Epoch [2/5], Batch [1030/1562], Loss: 0.4493, Avg Loss: 0.6284, Time: 136.21s\n",
            "Epoch [2/5], Batch [1040/1562], Loss: 0.4607, Avg Loss: 0.6267, Time: 137.46s\n",
            "Epoch [2/5], Batch [1050/1562], Loss: 0.4540, Avg Loss: 0.6249, Time: 138.72s\n",
            "Epoch [2/5], Batch [1060/1562], Loss: 0.4098, Avg Loss: 0.6232, Time: 140.02s\n",
            "Epoch [2/5], Batch [1070/1562], Loss: 0.4407, Avg Loss: 0.6215, Time: 141.31s\n",
            "Epoch [2/5], Batch [1080/1562], Loss: 0.4366, Avg Loss: 0.6198, Time: 142.58s\n",
            "Epoch [2/5], Batch [1090/1562], Loss: 0.4263, Avg Loss: 0.6181, Time: 144.13s\n",
            "Epoch [2/5], Batch [1100/1562], Loss: 0.4244, Avg Loss: 0.6165, Time: 145.59s\n",
            "Epoch [2/5], Batch [1110/1562], Loss: 0.4359, Avg Loss: 0.6148, Time: 146.84s\n",
            "Epoch [2/5], Batch [1120/1562], Loss: 0.4166, Avg Loss: 0.6131, Time: 148.12s\n",
            "Epoch [2/5], Batch [1130/1562], Loss: 0.4351, Avg Loss: 0.6114, Time: 149.36s\n",
            "Epoch [2/5], Batch [1140/1562], Loss: 0.4154, Avg Loss: 0.6098, Time: 150.66s\n",
            "Epoch [2/5], Batch [1150/1562], Loss: 0.4147, Avg Loss: 0.6081, Time: 151.93s\n",
            "Epoch [2/5], Batch [1160/1562], Loss: 0.4045, Avg Loss: 0.6065, Time: 153.20s\n",
            "Epoch [2/5], Batch [1170/1562], Loss: 0.4266, Avg Loss: 0.6049, Time: 154.47s\n",
            "Epoch [2/5], Batch [1180/1562], Loss: 0.4192, Avg Loss: 0.6033, Time: 155.94s\n",
            "Epoch [2/5], Batch [1190/1562], Loss: 0.4126, Avg Loss: 0.6017, Time: 157.54s\n",
            "Epoch [2/5], Batch [1200/1562], Loss: 0.4079, Avg Loss: 0.6001, Time: 158.78s\n",
            "Epoch [2/5], Batch [1210/1562], Loss: 0.4159, Avg Loss: 0.5985, Time: 160.03s\n",
            "Epoch [2/5], Batch [1220/1562], Loss: 0.3976, Avg Loss: 0.5969, Time: 161.33s\n",
            "Epoch [2/5], Batch [1230/1562], Loss: 0.3972, Avg Loss: 0.5954, Time: 162.62s\n",
            "Epoch [2/5], Batch [1240/1562], Loss: 0.3974, Avg Loss: 0.5938, Time: 163.87s\n",
            "Epoch [2/5], Batch [1250/1562], Loss: 0.3970, Avg Loss: 0.5923, Time: 165.15s\n",
            "Epoch [2/5], Batch [1260/1562], Loss: 0.3945, Avg Loss: 0.5908, Time: 166.45s\n",
            "Epoch [2/5], Batch [1270/1562], Loss: 0.4071, Avg Loss: 0.5893, Time: 167.80s\n",
            "Epoch [2/5], Batch [1280/1562], Loss: 0.4052, Avg Loss: 0.5878, Time: 169.45s\n",
            "Epoch [2/5], Batch [1290/1562], Loss: 0.3829, Avg Loss: 0.5862, Time: 170.75s\n",
            "Epoch [2/5], Batch [1300/1562], Loss: 0.3803, Avg Loss: 0.5847, Time: 172.03s\n",
            "Epoch [2/5], Batch [1310/1562], Loss: 0.4080, Avg Loss: 0.5833, Time: 173.31s\n",
            "Epoch [2/5], Batch [1320/1562], Loss: 0.3812, Avg Loss: 0.5818, Time: 174.59s\n",
            "Epoch [2/5], Batch [1330/1562], Loss: 0.3813, Avg Loss: 0.5804, Time: 175.85s\n",
            "Epoch [2/5], Batch [1340/1562], Loss: 0.3740, Avg Loss: 0.5789, Time: 177.13s\n",
            "Epoch [2/5], Batch [1350/1562], Loss: 0.3885, Avg Loss: 0.5775, Time: 178.41s\n",
            "Epoch [2/5], Batch [1360/1562], Loss: 0.3745, Avg Loss: 0.5761, Time: 179.79s\n",
            "Epoch [2/5], Batch [1370/1562], Loss: 0.3716, Avg Loss: 0.5746, Time: 181.43s\n",
            "Epoch [2/5], Batch [1380/1562], Loss: 0.3866, Avg Loss: 0.5732, Time: 182.80s\n",
            "Epoch [2/5], Batch [1390/1562], Loss: 0.3760, Avg Loss: 0.5718, Time: 184.10s\n",
            "Epoch [2/5], Batch [1400/1562], Loss: 0.3785, Avg Loss: 0.5704, Time: 185.40s\n",
            "Epoch [2/5], Batch [1410/1562], Loss: 0.3483, Avg Loss: 0.5690, Time: 186.72s\n",
            "Epoch [2/5], Batch [1420/1562], Loss: 0.3733, Avg Loss: 0.5676, Time: 188.06s\n",
            "Epoch [2/5], Batch [1430/1562], Loss: 0.3707, Avg Loss: 0.5662, Time: 189.36s\n",
            "Epoch [2/5], Batch [1440/1562], Loss: 0.3627, Avg Loss: 0.5649, Time: 190.65s\n",
            "Epoch [2/5], Batch [1450/1562], Loss: 0.3706, Avg Loss: 0.5635, Time: 192.07s\n",
            "Epoch [2/5], Batch [1460/1562], Loss: 0.3650, Avg Loss: 0.5622, Time: 193.75s\n",
            "Epoch [2/5], Batch [1470/1562], Loss: 0.3619, Avg Loss: 0.5608, Time: 195.10s\n",
            "Epoch [2/5], Batch [1480/1562], Loss: 0.3691, Avg Loss: 0.5595, Time: 196.39s\n",
            "Epoch [2/5], Batch [1490/1562], Loss: 0.3595, Avg Loss: 0.5582, Time: 197.68s\n",
            "Epoch [2/5], Batch [1500/1562], Loss: 0.3511, Avg Loss: 0.5568, Time: 198.97s\n",
            "Epoch [2/5], Batch [1510/1562], Loss: 0.3625, Avg Loss: 0.5555, Time: 200.28s\n",
            "Epoch [2/5], Batch [1520/1562], Loss: 0.3471, Avg Loss: 0.5542, Time: 201.57s\n",
            "Epoch [2/5], Batch [1530/1562], Loss: 0.3698, Avg Loss: 0.5529, Time: 202.88s\n",
            "Epoch [2/5], Batch [1540/1562], Loss: 0.3611, Avg Loss: 0.5516, Time: 204.25s\n",
            "Epoch [2/5], Batch [1550/1562], Loss: 0.3469, Avg Loss: 0.5503, Time: 205.92s\n",
            "Epoch [2/5], Batch [1560/1562], Loss: 0.3620, Avg Loss: 0.5490, Time: 207.33s\n",
            "End of Epoch [2/5], Avg Loss: 0.5487, Total Time: 207.54s\n",
            "Epoch [3/5], Batch [10/1562], Loss: 0.3429, Avg Loss: 0.3317, Time: 1.28s\n",
            "Epoch [3/5], Batch [20/1562], Loss: 0.3453, Avg Loss: 0.3344, Time: 2.59s\n",
            "Epoch [3/5], Batch [30/1562], Loss: 0.3272, Avg Loss: 0.3345, Time: 3.87s\n",
            "Epoch [3/5], Batch [40/1562], Loss: 0.3257, Avg Loss: 0.3325, Time: 5.12s\n",
            "Epoch [3/5], Batch [50/1562], Loss: 0.3254, Avg Loss: 0.3318, Time: 6.38s\n",
            "Epoch [3/5], Batch [60/1562], Loss: 0.3224, Avg Loss: 0.3316, Time: 7.65s\n",
            "Epoch [3/5], Batch [70/1562], Loss: 0.3386, Avg Loss: 0.3306, Time: 8.96s\n",
            "Epoch [3/5], Batch [80/1562], Loss: 0.3410, Avg Loss: 0.3308, Time: 10.71s\n",
            "Epoch [3/5], Batch [90/1562], Loss: 0.3216, Avg Loss: 0.3301, Time: 11.96s\n",
            "Epoch [3/5], Batch [100/1562], Loss: 0.3357, Avg Loss: 0.3298, Time: 13.23s\n",
            "Epoch [3/5], Batch [110/1562], Loss: 0.3306, Avg Loss: 0.3295, Time: 14.49s\n",
            "Epoch [3/5], Batch [120/1562], Loss: 0.3195, Avg Loss: 0.3294, Time: 15.75s\n",
            "Epoch [3/5], Batch [130/1562], Loss: 0.3308, Avg Loss: 0.3290, Time: 17.00s\n",
            "Epoch [3/5], Batch [140/1562], Loss: 0.3293, Avg Loss: 0.3283, Time: 18.23s\n",
            "Epoch [3/5], Batch [150/1562], Loss: 0.3235, Avg Loss: 0.3278, Time: 19.47s\n",
            "Epoch [3/5], Batch [160/1562], Loss: 0.3249, Avg Loss: 0.3272, Time: 20.82s\n",
            "Epoch [3/5], Batch [170/1562], Loss: 0.3415, Avg Loss: 0.3271, Time: 22.42s\n",
            "Epoch [3/5], Batch [180/1562], Loss: 0.3345, Avg Loss: 0.3266, Time: 23.78s\n",
            "Epoch [3/5], Batch [190/1562], Loss: 0.3245, Avg Loss: 0.3260, Time: 25.04s\n",
            "Epoch [3/5], Batch [200/1562], Loss: 0.3355, Avg Loss: 0.3259, Time: 26.30s\n",
            "Epoch [3/5], Batch [210/1562], Loss: 0.3198, Avg Loss: 0.3258, Time: 27.54s\n",
            "Epoch [3/5], Batch [220/1562], Loss: 0.3154, Avg Loss: 0.3256, Time: 28.82s\n",
            "Epoch [3/5], Batch [230/1562], Loss: 0.3254, Avg Loss: 0.3253, Time: 30.12s\n",
            "Epoch [3/5], Batch [240/1562], Loss: 0.3143, Avg Loss: 0.3252, Time: 31.40s\n",
            "Epoch [3/5], Batch [250/1562], Loss: 0.3123, Avg Loss: 0.3250, Time: 32.67s\n",
            "Epoch [3/5], Batch [260/1562], Loss: 0.3113, Avg Loss: 0.3247, Time: 34.22s\n",
            "Epoch [3/5], Batch [270/1562], Loss: 0.3134, Avg Loss: 0.3243, Time: 35.68s\n",
            "Epoch [3/5], Batch [280/1562], Loss: 0.3176, Avg Loss: 0.3240, Time: 36.95s\n",
            "Epoch [3/5], Batch [290/1562], Loss: 0.2962, Avg Loss: 0.3236, Time: 38.23s\n",
            "Epoch [3/5], Batch [300/1562], Loss: 0.3114, Avg Loss: 0.3234, Time: 39.48s\n",
            "Epoch [3/5], Batch [310/1562], Loss: 0.3069, Avg Loss: 0.3230, Time: 40.76s\n",
            "Epoch [3/5], Batch [320/1562], Loss: 0.3095, Avg Loss: 0.3225, Time: 42.02s\n",
            "Epoch [3/5], Batch [330/1562], Loss: 0.3155, Avg Loss: 0.3222, Time: 43.29s\n",
            "Epoch [3/5], Batch [340/1562], Loss: 0.3049, Avg Loss: 0.3219, Time: 44.58s\n",
            "Epoch [3/5], Batch [350/1562], Loss: 0.3058, Avg Loss: 0.3215, Time: 46.04s\n",
            "Epoch [3/5], Batch [360/1562], Loss: 0.3120, Avg Loss: 0.3211, Time: 47.58s\n",
            "Epoch [3/5], Batch [370/1562], Loss: 0.2870, Avg Loss: 0.3207, Time: 48.84s\n",
            "Epoch [3/5], Batch [380/1562], Loss: 0.3071, Avg Loss: 0.3202, Time: 50.11s\n",
            "Epoch [3/5], Batch [390/1562], Loss: 0.2928, Avg Loss: 0.3199, Time: 51.37s\n",
            "Epoch [3/5], Batch [400/1562], Loss: 0.3077, Avg Loss: 0.3194, Time: 52.64s\n",
            "Epoch [3/5], Batch [410/1562], Loss: 0.3010, Avg Loss: 0.3190, Time: 53.92s\n",
            "Epoch [3/5], Batch [420/1562], Loss: 0.2997, Avg Loss: 0.3185, Time: 55.19s\n",
            "Epoch [3/5], Batch [430/1562], Loss: 0.2826, Avg Loss: 0.3181, Time: 56.46s\n",
            "Epoch [3/5], Batch [440/1562], Loss: 0.3033, Avg Loss: 0.3177, Time: 57.81s\n",
            "Epoch [3/5], Batch [450/1562], Loss: 0.3239, Avg Loss: 0.3172, Time: 59.43s\n",
            "Epoch [3/5], Batch [460/1562], Loss: 0.3011, Avg Loss: 0.3169, Time: 60.74s\n",
            "Epoch [3/5], Batch [470/1562], Loss: 0.2978, Avg Loss: 0.3166, Time: 62.00s\n",
            "Epoch [3/5], Batch [480/1562], Loss: 0.2871, Avg Loss: 0.3161, Time: 63.29s\n",
            "Epoch [3/5], Batch [490/1562], Loss: 0.2914, Avg Loss: 0.3157, Time: 64.57s\n",
            "Epoch [3/5], Batch [500/1562], Loss: 0.2799, Avg Loss: 0.3154, Time: 65.81s\n",
            "Epoch [3/5], Batch [510/1562], Loss: 0.2880, Avg Loss: 0.3149, Time: 67.05s\n",
            "Epoch [3/5], Batch [520/1562], Loss: 0.3041, Avg Loss: 0.3145, Time: 68.31s\n",
            "Epoch [3/5], Batch [530/1562], Loss: 0.2990, Avg Loss: 0.3141, Time: 69.59s\n",
            "Epoch [3/5], Batch [540/1562], Loss: 0.2882, Avg Loss: 0.3136, Time: 71.13s\n",
            "Epoch [3/5], Batch [550/1562], Loss: 0.2861, Avg Loss: 0.3133, Time: 72.59s\n",
            "Epoch [3/5], Batch [560/1562], Loss: 0.3057, Avg Loss: 0.3130, Time: 73.85s\n",
            "Epoch [3/5], Batch [570/1562], Loss: 0.3056, Avg Loss: 0.3127, Time: 75.11s\n",
            "Epoch [3/5], Batch [580/1562], Loss: 0.2923, Avg Loss: 0.3124, Time: 76.35s\n",
            "Epoch [3/5], Batch [590/1562], Loss: 0.2859, Avg Loss: 0.3120, Time: 77.61s\n",
            "Epoch [3/5], Batch [600/1562], Loss: 0.2758, Avg Loss: 0.3116, Time: 78.89s\n",
            "Epoch [3/5], Batch [610/1562], Loss: 0.2961, Avg Loss: 0.3113, Time: 80.14s\n",
            "Epoch [3/5], Batch [620/1562], Loss: 0.2940, Avg Loss: 0.3109, Time: 81.39s\n",
            "Epoch [3/5], Batch [630/1562], Loss: 0.2874, Avg Loss: 0.3105, Time: 82.78s\n",
            "Epoch [3/5], Batch [640/1562], Loss: 0.2674, Avg Loss: 0.3101, Time: 84.38s\n",
            "Epoch [3/5], Batch [650/1562], Loss: 0.2841, Avg Loss: 0.3098, Time: 85.64s\n",
            "Epoch [3/5], Batch [660/1562], Loss: 0.2931, Avg Loss: 0.3094, Time: 86.89s\n",
            "Epoch [3/5], Batch [670/1562], Loss: 0.2824, Avg Loss: 0.3091, Time: 88.15s\n",
            "Epoch [3/5], Batch [680/1562], Loss: 0.2689, Avg Loss: 0.3087, Time: 89.38s\n",
            "Epoch [3/5], Batch [690/1562], Loss: 0.2970, Avg Loss: 0.3083, Time: 90.61s\n",
            "Epoch [3/5], Batch [700/1562], Loss: 0.2684, Avg Loss: 0.3079, Time: 91.87s\n",
            "Epoch [3/5], Batch [710/1562], Loss: 0.2849, Avg Loss: 0.3076, Time: 93.15s\n",
            "Epoch [3/5], Batch [720/1562], Loss: 0.2822, Avg Loss: 0.3072, Time: 94.43s\n",
            "Epoch [3/5], Batch [730/1562], Loss: 0.2880, Avg Loss: 0.3069, Time: 96.01s\n",
            "Epoch [3/5], Batch [740/1562], Loss: 0.2802, Avg Loss: 0.3066, Time: 97.45s\n",
            "Epoch [3/5], Batch [750/1562], Loss: 0.2690, Avg Loss: 0.3063, Time: 98.71s\n",
            "Epoch [3/5], Batch [760/1562], Loss: 0.2819, Avg Loss: 0.3060, Time: 99.98s\n",
            "Epoch [3/5], Batch [770/1562], Loss: 0.2945, Avg Loss: 0.3057, Time: 101.26s\n",
            "Epoch [3/5], Batch [780/1562], Loss: 0.2663, Avg Loss: 0.3053, Time: 102.51s\n",
            "Epoch [3/5], Batch [790/1562], Loss: 0.2947, Avg Loss: 0.3050, Time: 103.79s\n",
            "Epoch [3/5], Batch [800/1562], Loss: 0.2750, Avg Loss: 0.3047, Time: 105.05s\n",
            "Epoch [3/5], Batch [810/1562], Loss: 0.2890, Avg Loss: 0.3044, Time: 106.30s\n",
            "Epoch [3/5], Batch [820/1562], Loss: 0.2729, Avg Loss: 0.3041, Time: 107.77s\n",
            "Epoch [3/5], Batch [830/1562], Loss: 0.2924, Avg Loss: 0.3039, Time: 109.35s\n",
            "Epoch [3/5], Batch [840/1562], Loss: 0.2745, Avg Loss: 0.3035, Time: 110.66s\n",
            "Epoch [3/5], Batch [850/1562], Loss: 0.2948, Avg Loss: 0.3033, Time: 111.91s\n",
            "Epoch [3/5], Batch [860/1562], Loss: 0.2716, Avg Loss: 0.3029, Time: 113.15s\n",
            "Epoch [3/5], Batch [870/1562], Loss: 0.2683, Avg Loss: 0.3026, Time: 114.44s\n",
            "Epoch [3/5], Batch [880/1562], Loss: 0.2811, Avg Loss: 0.3024, Time: 115.71s\n",
            "Epoch [3/5], Batch [890/1562], Loss: 0.2849, Avg Loss: 0.3021, Time: 117.00s\n",
            "Epoch [3/5], Batch [900/1562], Loss: 0.2639, Avg Loss: 0.3018, Time: 118.29s\n",
            "Epoch [3/5], Batch [910/1562], Loss: 0.2734, Avg Loss: 0.3015, Time: 119.65s\n",
            "Epoch [3/5], Batch [920/1562], Loss: 0.2708, Avg Loss: 0.3012, Time: 121.23s\n",
            "Epoch [3/5], Batch [930/1562], Loss: 0.2899, Avg Loss: 0.3010, Time: 122.56s\n",
            "Epoch [3/5], Batch [940/1562], Loss: 0.2767, Avg Loss: 0.3007, Time: 123.82s\n",
            "Epoch [3/5], Batch [950/1562], Loss: 0.2735, Avg Loss: 0.3004, Time: 125.09s\n",
            "Epoch [3/5], Batch [960/1562], Loss: 0.2807, Avg Loss: 0.3002, Time: 126.36s\n",
            "Epoch [3/5], Batch [970/1562], Loss: 0.2796, Avg Loss: 0.2999, Time: 127.63s\n",
            "Epoch [3/5], Batch [980/1562], Loss: 0.2693, Avg Loss: 0.2996, Time: 128.88s\n",
            "Epoch [3/5], Batch [990/1562], Loss: 0.2691, Avg Loss: 0.2994, Time: 130.15s\n",
            "Epoch [3/5], Batch [1000/1562], Loss: 0.2636, Avg Loss: 0.2991, Time: 131.44s\n",
            "Epoch [3/5], Batch [1010/1562], Loss: 0.2733, Avg Loss: 0.2988, Time: 133.03s\n",
            "Epoch [3/5], Batch [1020/1562], Loss: 0.2637, Avg Loss: 0.2985, Time: 134.42s\n",
            "Epoch [3/5], Batch [1030/1562], Loss: 0.2665, Avg Loss: 0.2982, Time: 135.68s\n",
            "Epoch [3/5], Batch [1040/1562], Loss: 0.2769, Avg Loss: 0.2979, Time: 136.95s\n",
            "Epoch [3/5], Batch [1050/1562], Loss: 0.2618, Avg Loss: 0.2977, Time: 138.21s\n",
            "Epoch [3/5], Batch [1060/1562], Loss: 0.2765, Avg Loss: 0.2974, Time: 139.44s\n",
            "Epoch [3/5], Batch [1070/1562], Loss: 0.2730, Avg Loss: 0.2971, Time: 140.66s\n",
            "Epoch [3/5], Batch [1080/1562], Loss: 0.2565, Avg Loss: 0.2969, Time: 141.91s\n",
            "Epoch [3/5], Batch [1090/1562], Loss: 0.2622, Avg Loss: 0.2966, Time: 143.16s\n",
            "Epoch [3/5], Batch [1100/1562], Loss: 0.2791, Avg Loss: 0.2963, Time: 144.64s\n",
            "Epoch [3/5], Batch [1110/1562], Loss: 0.2745, Avg Loss: 0.2961, Time: 146.24s\n",
            "Epoch [3/5], Batch [1120/1562], Loss: 0.2818, Avg Loss: 0.2958, Time: 147.51s\n",
            "Epoch [3/5], Batch [1130/1562], Loss: 0.2679, Avg Loss: 0.2956, Time: 148.78s\n",
            "Epoch [3/5], Batch [1140/1562], Loss: 0.2556, Avg Loss: 0.2953, Time: 150.07s\n",
            "Epoch [3/5], Batch [1150/1562], Loss: 0.2668, Avg Loss: 0.2950, Time: 151.35s\n",
            "Epoch [3/5], Batch [1160/1562], Loss: 0.2470, Avg Loss: 0.2948, Time: 152.63s\n",
            "Epoch [3/5], Batch [1170/1562], Loss: 0.2625, Avg Loss: 0.2945, Time: 153.90s\n",
            "Epoch [3/5], Batch [1180/1562], Loss: 0.2651, Avg Loss: 0.2943, Time: 155.18s\n",
            "Epoch [3/5], Batch [1190/1562], Loss: 0.2686, Avg Loss: 0.2940, Time: 156.53s\n",
            "Epoch [3/5], Batch [1200/1562], Loss: 0.2390, Avg Loss: 0.2938, Time: 158.12s\n",
            "Epoch [3/5], Batch [1210/1562], Loss: 0.2498, Avg Loss: 0.2935, Time: 159.47s\n",
            "Epoch [3/5], Batch [1220/1562], Loss: 0.2510, Avg Loss: 0.2932, Time: 160.74s\n",
            "Epoch [3/5], Batch [1230/1562], Loss: 0.2695, Avg Loss: 0.2930, Time: 162.01s\n",
            "Epoch [3/5], Batch [1240/1562], Loss: 0.2557, Avg Loss: 0.2927, Time: 163.28s\n",
            "Epoch [3/5], Batch [1250/1562], Loss: 0.2606, Avg Loss: 0.2924, Time: 164.56s\n",
            "Epoch [3/5], Batch [1260/1562], Loss: 0.2532, Avg Loss: 0.2922, Time: 165.81s\n",
            "Epoch [3/5], Batch [1270/1562], Loss: 0.2691, Avg Loss: 0.2919, Time: 167.04s\n",
            "Epoch [3/5], Batch [1280/1562], Loss: 0.2513, Avg Loss: 0.2917, Time: 168.30s\n",
            "Epoch [3/5], Batch [1290/1562], Loss: 0.2681, Avg Loss: 0.2914, Time: 169.80s\n",
            "Epoch [3/5], Batch [1300/1562], Loss: 0.2559, Avg Loss: 0.2912, Time: 171.26s\n",
            "Epoch [3/5], Batch [1310/1562], Loss: 0.2639, Avg Loss: 0.2910, Time: 172.53s\n",
            "Epoch [3/5], Batch [1320/1562], Loss: 0.2519, Avg Loss: 0.2908, Time: 173.82s\n",
            "Epoch [3/5], Batch [1330/1562], Loss: 0.2603, Avg Loss: 0.2906, Time: 175.09s\n",
            "Epoch [3/5], Batch [1340/1562], Loss: 0.2725, Avg Loss: 0.2903, Time: 176.34s\n",
            "Epoch [3/5], Batch [1350/1562], Loss: 0.2590, Avg Loss: 0.2901, Time: 177.59s\n",
            "Epoch [3/5], Batch [1360/1562], Loss: 0.2586, Avg Loss: 0.2899, Time: 178.84s\n",
            "Epoch [3/5], Batch [1370/1562], Loss: 0.2566, Avg Loss: 0.2897, Time: 180.10s\n",
            "Epoch [3/5], Batch [1380/1562], Loss: 0.2554, Avg Loss: 0.2895, Time: 181.59s\n",
            "Epoch [3/5], Batch [1390/1562], Loss: 0.2620, Avg Loss: 0.2892, Time: 183.15s\n",
            "Epoch [3/5], Batch [1400/1562], Loss: 0.2594, Avg Loss: 0.2890, Time: 184.41s\n",
            "Epoch [3/5], Batch [1410/1562], Loss: 0.2602, Avg Loss: 0.2888, Time: 185.67s\n",
            "Epoch [3/5], Batch [1420/1562], Loss: 0.2634, Avg Loss: 0.2886, Time: 186.92s\n",
            "Epoch [3/5], Batch [1430/1562], Loss: 0.2451, Avg Loss: 0.2883, Time: 188.17s\n",
            "Epoch [3/5], Batch [1440/1562], Loss: 0.2535, Avg Loss: 0.2881, Time: 189.42s\n",
            "Epoch [3/5], Batch [1450/1562], Loss: 0.2516, Avg Loss: 0.2879, Time: 190.72s\n",
            "Epoch [3/5], Batch [1460/1562], Loss: 0.2630, Avg Loss: 0.2876, Time: 192.00s\n",
            "Epoch [3/5], Batch [1470/1562], Loss: 0.2654, Avg Loss: 0.2874, Time: 193.35s\n",
            "Epoch [3/5], Batch [1480/1562], Loss: 0.2557, Avg Loss: 0.2872, Time: 195.01s\n",
            "Epoch [3/5], Batch [1490/1562], Loss: 0.2584, Avg Loss: 0.2870, Time: 196.34s\n",
            "Epoch [3/5], Batch [1500/1562], Loss: 0.2435, Avg Loss: 0.2868, Time: 197.64s\n",
            "Epoch [3/5], Batch [1510/1562], Loss: 0.2497, Avg Loss: 0.2865, Time: 198.90s\n",
            "Epoch [3/5], Batch [1520/1562], Loss: 0.2503, Avg Loss: 0.2863, Time: 200.17s\n",
            "Epoch [3/5], Batch [1530/1562], Loss: 0.2525, Avg Loss: 0.2861, Time: 201.43s\n",
            "Epoch [3/5], Batch [1540/1562], Loss: 0.2640, Avg Loss: 0.2859, Time: 202.70s\n",
            "Epoch [3/5], Batch [1550/1562], Loss: 0.2472, Avg Loss: 0.2857, Time: 203.94s\n",
            "Epoch [3/5], Batch [1560/1562], Loss: 0.2467, Avg Loss: 0.2855, Time: 205.24s\n",
            "End of Epoch [3/5], Avg Loss: 0.2855, Total Time: 205.49s\n",
            "Epoch [4/5], Batch [10/1562], Loss: 0.2519, Avg Loss: 0.2429, Time: 1.62s\n",
            "Epoch [4/5], Batch [20/1562], Loss: 0.2564, Avg Loss: 0.2431, Time: 2.99s\n",
            "Epoch [4/5], Batch [30/1562], Loss: 0.2309, Avg Loss: 0.2431, Time: 4.24s\n",
            "Epoch [4/5], Batch [40/1562], Loss: 0.2312, Avg Loss: 0.2421, Time: 5.49s\n",
            "Epoch [4/5], Batch [50/1562], Loss: 0.2420, Avg Loss: 0.2420, Time: 6.77s\n",
            "Epoch [4/5], Batch [60/1562], Loss: 0.2418, Avg Loss: 0.2412, Time: 8.03s\n",
            "Epoch [4/5], Batch [70/1562], Loss: 0.2474, Avg Loss: 0.2411, Time: 9.28s\n",
            "Epoch [4/5], Batch [80/1562], Loss: 0.2504, Avg Loss: 0.2414, Time: 10.54s\n",
            "Epoch [4/5], Batch [90/1562], Loss: 0.2457, Avg Loss: 0.2415, Time: 11.81s\n",
            "Epoch [4/5], Batch [100/1562], Loss: 0.2564, Avg Loss: 0.2414, Time: 13.32s\n",
            "Epoch [4/5], Batch [110/1562], Loss: 0.2400, Avg Loss: 0.2415, Time: 14.82s\n",
            "Epoch [4/5], Batch [120/1562], Loss: 0.2566, Avg Loss: 0.2415, Time: 16.08s\n",
            "Epoch [4/5], Batch [130/1562], Loss: 0.2494, Avg Loss: 0.2417, Time: 17.35s\n",
            "Epoch [4/5], Batch [140/1562], Loss: 0.2588, Avg Loss: 0.2418, Time: 18.59s\n",
            "Epoch [4/5], Batch [150/1562], Loss: 0.2373, Avg Loss: 0.2418, Time: 19.87s\n",
            "Epoch [4/5], Batch [160/1562], Loss: 0.2423, Avg Loss: 0.2418, Time: 21.13s\n",
            "Epoch [4/5], Batch [170/1562], Loss: 0.2353, Avg Loss: 0.2418, Time: 22.40s\n",
            "Epoch [4/5], Batch [180/1562], Loss: 0.2283, Avg Loss: 0.2416, Time: 23.65s\n",
            "Epoch [4/5], Batch [190/1562], Loss: 0.2339, Avg Loss: 0.2414, Time: 25.04s\n",
            "Epoch [4/5], Batch [200/1562], Loss: 0.2422, Avg Loss: 0.2414, Time: 26.69s\n",
            "Epoch [4/5], Batch [210/1562], Loss: 0.2479, Avg Loss: 0.2414, Time: 28.01s\n",
            "Epoch [4/5], Batch [220/1562], Loss: 0.2315, Avg Loss: 0.2413, Time: 29.29s\n",
            "Epoch [4/5], Batch [230/1562], Loss: 0.2344, Avg Loss: 0.2412, Time: 30.58s\n",
            "Epoch [4/5], Batch [240/1562], Loss: 0.2458, Avg Loss: 0.2413, Time: 31.86s\n",
            "Epoch [4/5], Batch [250/1562], Loss: 0.2351, Avg Loss: 0.2415, Time: 33.16s\n",
            "Epoch [4/5], Batch [260/1562], Loss: 0.2512, Avg Loss: 0.2415, Time: 34.46s\n",
            "Epoch [4/5], Batch [270/1562], Loss: 0.2262, Avg Loss: 0.2415, Time: 35.76s\n",
            "Epoch [4/5], Batch [280/1562], Loss: 0.2403, Avg Loss: 0.2416, Time: 37.12s\n",
            "Epoch [4/5], Batch [290/1562], Loss: 0.2410, Avg Loss: 0.2415, Time: 38.72s\n",
            "Epoch [4/5], Batch [300/1562], Loss: 0.2370, Avg Loss: 0.2414, Time: 40.06s\n",
            "Epoch [4/5], Batch [310/1562], Loss: 0.2480, Avg Loss: 0.2415, Time: 41.32s\n",
            "Epoch [4/5], Batch [320/1562], Loss: 0.2379, Avg Loss: 0.2415, Time: 42.56s\n",
            "Epoch [4/5], Batch [330/1562], Loss: 0.2348, Avg Loss: 0.2414, Time: 43.82s\n",
            "Epoch [4/5], Batch [340/1562], Loss: 0.2576, Avg Loss: 0.2414, Time: 45.09s\n",
            "Epoch [4/5], Batch [350/1562], Loss: 0.2404, Avg Loss: 0.2414, Time: 46.34s\n",
            "Epoch [4/5], Batch [360/1562], Loss: 0.2274, Avg Loss: 0.2414, Time: 47.58s\n",
            "Epoch [4/5], Batch [370/1562], Loss: 0.2484, Avg Loss: 0.2413, Time: 48.86s\n",
            "Epoch [4/5], Batch [380/1562], Loss: 0.2348, Avg Loss: 0.2412, Time: 50.44s\n",
            "Epoch [4/5], Batch [390/1562], Loss: 0.2412, Avg Loss: 0.2413, Time: 51.92s\n",
            "Epoch [4/5], Batch [400/1562], Loss: 0.2325, Avg Loss: 0.2411, Time: 53.18s\n",
            "Epoch [4/5], Batch [410/1562], Loss: 0.2422, Avg Loss: 0.2410, Time: 54.42s\n",
            "Epoch [4/5], Batch [420/1562], Loss: 0.2241, Avg Loss: 0.2410, Time: 55.65s\n",
            "Epoch [4/5], Batch [430/1562], Loss: 0.2379, Avg Loss: 0.2409, Time: 56.90s\n",
            "Epoch [4/5], Batch [440/1562], Loss: 0.2340, Avg Loss: 0.2408, Time: 58.17s\n",
            "Epoch [4/5], Batch [450/1562], Loss: 0.2389, Avg Loss: 0.2407, Time: 59.43s\n",
            "Epoch [4/5], Batch [460/1562], Loss: 0.2456, Avg Loss: 0.2406, Time: 60.74s\n",
            "Epoch [4/5], Batch [470/1562], Loss: 0.2473, Avg Loss: 0.2405, Time: 62.14s\n",
            "Epoch [4/5], Batch [480/1562], Loss: 0.2256, Avg Loss: 0.2405, Time: 63.71s\n",
            "Epoch [4/5], Batch [490/1562], Loss: 0.2370, Avg Loss: 0.2404, Time: 64.99s\n",
            "Epoch [4/5], Batch [500/1562], Loss: 0.2359, Avg Loss: 0.2403, Time: 66.28s\n",
            "Epoch [4/5], Batch [510/1562], Loss: 0.2338, Avg Loss: 0.2402, Time: 67.56s\n",
            "Epoch [4/5], Batch [520/1562], Loss: 0.2359, Avg Loss: 0.2401, Time: 68.83s\n",
            "Epoch [4/5], Batch [530/1562], Loss: 0.2365, Avg Loss: 0.2400, Time: 70.07s\n",
            "Epoch [4/5], Batch [540/1562], Loss: 0.2363, Avg Loss: 0.2400, Time: 71.33s\n",
            "Epoch [4/5], Batch [550/1562], Loss: 0.2322, Avg Loss: 0.2399, Time: 72.60s\n",
            "Epoch [4/5], Batch [560/1562], Loss: 0.2441, Avg Loss: 0.2398, Time: 73.90s\n",
            "Epoch [4/5], Batch [570/1562], Loss: 0.2297, Avg Loss: 0.2397, Time: 75.53s\n",
            "Epoch [4/5], Batch [580/1562], Loss: 0.2484, Avg Loss: 0.2396, Time: 76.90s\n",
            "Epoch [4/5], Batch [590/1562], Loss: 0.2549, Avg Loss: 0.2396, Time: 78.13s\n",
            "Epoch [4/5], Batch [600/1562], Loss: 0.2419, Avg Loss: 0.2397, Time: 79.38s\n",
            "Epoch [4/5], Batch [610/1562], Loss: 0.2394, Avg Loss: 0.2397, Time: 80.65s\n",
            "Epoch [4/5], Batch [620/1562], Loss: 0.2453, Avg Loss: 0.2396, Time: 81.92s\n",
            "Epoch [4/5], Batch [630/1562], Loss: 0.2310, Avg Loss: 0.2395, Time: 83.16s\n",
            "Epoch [4/5], Batch [640/1562], Loss: 0.2314, Avg Loss: 0.2395, Time: 84.41s\n",
            "Epoch [4/5], Batch [650/1562], Loss: 0.2258, Avg Loss: 0.2394, Time: 85.69s\n",
            "Epoch [4/5], Batch [660/1562], Loss: 0.2413, Avg Loss: 0.2394, Time: 87.28s\n",
            "Epoch [4/5], Batch [670/1562], Loss: 0.2402, Avg Loss: 0.2393, Time: 88.77s\n",
            "Epoch [4/5], Batch [680/1562], Loss: 0.2369, Avg Loss: 0.2393, Time: 90.04s\n",
            "Epoch [4/5], Batch [690/1562], Loss: 0.2310, Avg Loss: 0.2392, Time: 91.30s\n",
            "Epoch [4/5], Batch [700/1562], Loss: 0.2189, Avg Loss: 0.2392, Time: 92.55s\n",
            "Epoch [4/5], Batch [710/1562], Loss: 0.2322, Avg Loss: 0.2392, Time: 93.82s\n",
            "Epoch [4/5], Batch [720/1562], Loss: 0.2304, Avg Loss: 0.2391, Time: 95.08s\n",
            "Epoch [4/5], Batch [730/1562], Loss: 0.2247, Avg Loss: 0.2391, Time: 96.35s\n",
            "Epoch [4/5], Batch [740/1562], Loss: 0.2412, Avg Loss: 0.2390, Time: 97.64s\n",
            "Epoch [4/5], Batch [750/1562], Loss: 0.2522, Avg Loss: 0.2390, Time: 99.07s\n",
            "Epoch [4/5], Batch [760/1562], Loss: 0.2362, Avg Loss: 0.2389, Time: 100.63s\n",
            "Epoch [4/5], Batch [770/1562], Loss: 0.2341, Avg Loss: 0.2389, Time: 101.88s\n",
            "Epoch [4/5], Batch [780/1562], Loss: 0.2456, Avg Loss: 0.2389, Time: 103.14s\n",
            "Epoch [4/5], Batch [790/1562], Loss: 0.2363, Avg Loss: 0.2389, Time: 104.43s\n",
            "Epoch [4/5], Batch [800/1562], Loss: 0.2420, Avg Loss: 0.2388, Time: 105.70s\n",
            "Epoch [4/5], Batch [810/1562], Loss: 0.2293, Avg Loss: 0.2387, Time: 106.99s\n",
            "Epoch [4/5], Batch [820/1562], Loss: 0.2322, Avg Loss: 0.2387, Time: 108.26s\n",
            "Epoch [4/5], Batch [830/1562], Loss: 0.2405, Avg Loss: 0.2386, Time: 109.52s\n",
            "Epoch [4/5], Batch [840/1562], Loss: 0.2418, Avg Loss: 0.2386, Time: 110.85s\n",
            "Epoch [4/5], Batch [850/1562], Loss: 0.2452, Avg Loss: 0.2385, Time: 112.47s\n",
            "Epoch [4/5], Batch [860/1562], Loss: 0.2281, Avg Loss: 0.2385, Time: 113.83s\n",
            "Epoch [4/5], Batch [870/1562], Loss: 0.2277, Avg Loss: 0.2385, Time: 115.09s\n",
            "Epoch [4/5], Batch [880/1562], Loss: 0.2392, Avg Loss: 0.2384, Time: 116.35s\n",
            "Epoch [4/5], Batch [890/1562], Loss: 0.2344, Avg Loss: 0.2383, Time: 117.61s\n",
            "Epoch [4/5], Batch [900/1562], Loss: 0.2448, Avg Loss: 0.2383, Time: 118.89s\n",
            "Epoch [4/5], Batch [910/1562], Loss: 0.2409, Avg Loss: 0.2383, Time: 120.15s\n",
            "Epoch [4/5], Batch [920/1562], Loss: 0.2293, Avg Loss: 0.2382, Time: 121.39s\n",
            "Epoch [4/5], Batch [930/1562], Loss: 0.2252, Avg Loss: 0.2382, Time: 122.62s\n",
            "Epoch [4/5], Batch [940/1562], Loss: 0.2349, Avg Loss: 0.2381, Time: 124.15s\n",
            "Epoch [4/5], Batch [950/1562], Loss: 0.2338, Avg Loss: 0.2380, Time: 125.62s\n",
            "Epoch [4/5], Batch [960/1562], Loss: 0.2256, Avg Loss: 0.2380, Time: 126.88s\n",
            "Epoch [4/5], Batch [970/1562], Loss: 0.2192, Avg Loss: 0.2379, Time: 128.13s\n",
            "Epoch [4/5], Batch [980/1562], Loss: 0.2313, Avg Loss: 0.2379, Time: 129.38s\n",
            "Epoch [4/5], Batch [990/1562], Loss: 0.2383, Avg Loss: 0.2378, Time: 130.63s\n",
            "Epoch [4/5], Batch [1000/1562], Loss: 0.2365, Avg Loss: 0.2378, Time: 131.89s\n",
            "Epoch [4/5], Batch [1010/1562], Loss: 0.2429, Avg Loss: 0.2377, Time: 133.13s\n",
            "Epoch [4/5], Batch [1020/1562], Loss: 0.2457, Avg Loss: 0.2377, Time: 134.39s\n",
            "Epoch [4/5], Batch [1030/1562], Loss: 0.2366, Avg Loss: 0.2376, Time: 135.77s\n",
            "Epoch [4/5], Batch [1040/1562], Loss: 0.2334, Avg Loss: 0.2376, Time: 137.37s\n",
            "Epoch [4/5], Batch [1050/1562], Loss: 0.2503, Avg Loss: 0.2375, Time: 138.67s\n",
            "Epoch [4/5], Batch [1060/1562], Loss: 0.2203, Avg Loss: 0.2375, Time: 139.95s\n",
            "Epoch [4/5], Batch [1070/1562], Loss: 0.2311, Avg Loss: 0.2374, Time: 141.20s\n",
            "Epoch [4/5], Batch [1080/1562], Loss: 0.2203, Avg Loss: 0.2374, Time: 142.47s\n",
            "Epoch [4/5], Batch [1090/1562], Loss: 0.2325, Avg Loss: 0.2373, Time: 143.75s\n",
            "Epoch [4/5], Batch [1100/1562], Loss: 0.2277, Avg Loss: 0.2373, Time: 145.00s\n",
            "Epoch [4/5], Batch [1110/1562], Loss: 0.2386, Avg Loss: 0.2372, Time: 146.24s\n",
            "Epoch [4/5], Batch [1120/1562], Loss: 0.2399, Avg Loss: 0.2371, Time: 147.52s\n",
            "Epoch [4/5], Batch [1130/1562], Loss: 0.2404, Avg Loss: 0.2371, Time: 149.11s\n",
            "Epoch [4/5], Batch [1140/1562], Loss: 0.2340, Avg Loss: 0.2371, Time: 150.53s\n",
            "Epoch [4/5], Batch [1150/1562], Loss: 0.2433, Avg Loss: 0.2371, Time: 151.81s\n",
            "Epoch [4/5], Batch [1160/1562], Loss: 0.2250, Avg Loss: 0.2370, Time: 153.05s\n",
            "Epoch [4/5], Batch [1170/1562], Loss: 0.2279, Avg Loss: 0.2370, Time: 154.34s\n",
            "Epoch [4/5], Batch [1180/1562], Loss: 0.2292, Avg Loss: 0.2369, Time: 155.59s\n",
            "Epoch [4/5], Batch [1190/1562], Loss: 0.2253, Avg Loss: 0.2369, Time: 156.84s\n",
            "Epoch [4/5], Batch [1200/1562], Loss: 0.2192, Avg Loss: 0.2368, Time: 158.10s\n",
            "Epoch [4/5], Batch [1210/1562], Loss: 0.2368, Avg Loss: 0.2367, Time: 159.34s\n",
            "Epoch [4/5], Batch [1220/1562], Loss: 0.2306, Avg Loss: 0.2367, Time: 160.81s\n",
            "Epoch [4/5], Batch [1230/1562], Loss: 0.2217, Avg Loss: 0.2366, Time: 162.36s\n",
            "Epoch [4/5], Batch [1240/1562], Loss: 0.2331, Avg Loss: 0.2366, Time: 163.60s\n",
            "Epoch [4/5], Batch [1250/1562], Loss: 0.2198, Avg Loss: 0.2365, Time: 164.84s\n",
            "Epoch [4/5], Batch [1260/1562], Loss: 0.2329, Avg Loss: 0.2364, Time: 166.09s\n",
            "Epoch [4/5], Batch [1270/1562], Loss: 0.2189, Avg Loss: 0.2364, Time: 167.38s\n",
            "Epoch [4/5], Batch [1280/1562], Loss: 0.2258, Avg Loss: 0.2363, Time: 168.63s\n",
            "Epoch [4/5], Batch [1290/1562], Loss: 0.2356, Avg Loss: 0.2363, Time: 169.87s\n",
            "Epoch [4/5], Batch [1300/1562], Loss: 0.2249, Avg Loss: 0.2363, Time: 171.13s\n",
            "Epoch [4/5], Batch [1310/1562], Loss: 0.2345, Avg Loss: 0.2362, Time: 172.43s\n",
            "Epoch [4/5], Batch [1320/1562], Loss: 0.2404, Avg Loss: 0.2362, Time: 174.02s\n",
            "Epoch [4/5], Batch [1330/1562], Loss: 0.2319, Avg Loss: 0.2361, Time: 175.40s\n",
            "Epoch [4/5], Batch [1340/1562], Loss: 0.2375, Avg Loss: 0.2361, Time: 176.66s\n",
            "Epoch [4/5], Batch [1350/1562], Loss: 0.2432, Avg Loss: 0.2360, Time: 177.92s\n",
            "Epoch [4/5], Batch [1360/1562], Loss: 0.2321, Avg Loss: 0.2360, Time: 179.19s\n",
            "Epoch [4/5], Batch [1370/1562], Loss: 0.2368, Avg Loss: 0.2359, Time: 180.42s\n",
            "Epoch [4/5], Batch [1380/1562], Loss: 0.2324, Avg Loss: 0.2359, Time: 181.69s\n",
            "Epoch [4/5], Batch [1390/1562], Loss: 0.2154, Avg Loss: 0.2359, Time: 182.95s\n",
            "Epoch [4/5], Batch [1400/1562], Loss: 0.2263, Avg Loss: 0.2358, Time: 184.20s\n",
            "Epoch [4/5], Batch [1410/1562], Loss: 0.2281, Avg Loss: 0.2358, Time: 185.79s\n",
            "Epoch [4/5], Batch [1420/1562], Loss: 0.2444, Avg Loss: 0.2358, Time: 187.19s\n",
            "Epoch [4/5], Batch [1430/1562], Loss: 0.2207, Avg Loss: 0.2357, Time: 188.43s\n",
            "Epoch [4/5], Batch [1440/1562], Loss: 0.2392, Avg Loss: 0.2357, Time: 189.70s\n",
            "Epoch [4/5], Batch [1450/1562], Loss: 0.2295, Avg Loss: 0.2356, Time: 190.98s\n",
            "Epoch [4/5], Batch [1460/1562], Loss: 0.2277, Avg Loss: 0.2356, Time: 192.23s\n",
            "Epoch [4/5], Batch [1470/1562], Loss: 0.2299, Avg Loss: 0.2355, Time: 193.48s\n",
            "Epoch [4/5], Batch [1480/1562], Loss: 0.2220, Avg Loss: 0.2354, Time: 194.75s\n",
            "Epoch [4/5], Batch [1490/1562], Loss: 0.2313, Avg Loss: 0.2354, Time: 196.02s\n",
            "Epoch [4/5], Batch [1500/1562], Loss: 0.2271, Avg Loss: 0.2353, Time: 197.53s\n",
            "Epoch [4/5], Batch [1510/1562], Loss: 0.2212, Avg Loss: 0.2352, Time: 199.02s\n",
            "Epoch [4/5], Batch [1520/1562], Loss: 0.2289, Avg Loss: 0.2352, Time: 200.30s\n",
            "Epoch [4/5], Batch [1530/1562], Loss: 0.2266, Avg Loss: 0.2351, Time: 201.57s\n",
            "Epoch [4/5], Batch [1540/1562], Loss: 0.2218, Avg Loss: 0.2351, Time: 202.83s\n",
            "Epoch [4/5], Batch [1550/1562], Loss: 0.2220, Avg Loss: 0.2350, Time: 204.07s\n",
            "Epoch [4/5], Batch [1560/1562], Loss: 0.2191, Avg Loss: 0.2350, Time: 205.34s\n",
            "End of Epoch [4/5], Avg Loss: 0.2350, Total Time: 205.55s\n",
            "Epoch [5/5], Batch [10/1562], Loss: 0.2223, Avg Loss: 0.2158, Time: 1.27s\n",
            "Epoch [5/5], Batch [20/1562], Loss: 0.2081, Avg Loss: 0.2161, Time: 2.52s\n",
            "Epoch [5/5], Batch [30/1562], Loss: 0.1976, Avg Loss: 0.2151, Time: 4.01s\n",
            "Epoch [5/5], Batch [40/1562], Loss: 0.2108, Avg Loss: 0.2155, Time: 5.62s\n",
            "Epoch [5/5], Batch [50/1562], Loss: 0.2172, Avg Loss: 0.2158, Time: 6.88s\n",
            "Epoch [5/5], Batch [60/1562], Loss: 0.2055, Avg Loss: 0.2164, Time: 8.15s\n",
            "Epoch [5/5], Batch [70/1562], Loss: 0.2158, Avg Loss: 0.2163, Time: 9.43s\n",
            "Epoch [5/5], Batch [80/1562], Loss: 0.2209, Avg Loss: 0.2165, Time: 10.70s\n",
            "Epoch [5/5], Batch [90/1562], Loss: 0.2211, Avg Loss: 0.2165, Time: 11.99s\n",
            "Epoch [5/5], Batch [100/1562], Loss: 0.2332, Avg Loss: 0.2168, Time: 13.28s\n",
            "Epoch [5/5], Batch [110/1562], Loss: 0.2232, Avg Loss: 0.2169, Time: 14.54s\n",
            "Epoch [5/5], Batch [120/1562], Loss: 0.2147, Avg Loss: 0.2168, Time: 15.89s\n",
            "Epoch [5/5], Batch [130/1562], Loss: 0.2085, Avg Loss: 0.2164, Time: 17.47s\n",
            "Epoch [5/5], Batch [140/1562], Loss: 0.2115, Avg Loss: 0.2166, Time: 18.81s\n",
            "Epoch [5/5], Batch [150/1562], Loss: 0.2281, Avg Loss: 0.2166, Time: 20.08s\n",
            "Epoch [5/5], Batch [160/1562], Loss: 0.2168, Avg Loss: 0.2165, Time: 21.34s\n",
            "Epoch [5/5], Batch [170/1562], Loss: 0.2203, Avg Loss: 0.2164, Time: 22.65s\n",
            "Epoch [5/5], Batch [180/1562], Loss: 0.2289, Avg Loss: 0.2163, Time: 23.92s\n",
            "Epoch [5/5], Batch [190/1562], Loss: 0.2198, Avg Loss: 0.2165, Time: 25.17s\n",
            "Epoch [5/5], Batch [200/1562], Loss: 0.2180, Avg Loss: 0.2164, Time: 26.43s\n",
            "Epoch [5/5], Batch [210/1562], Loss: 0.2178, Avg Loss: 0.2165, Time: 27.69s\n",
            "Epoch [5/5], Batch [220/1562], Loss: 0.2133, Avg Loss: 0.2165, Time: 29.21s\n",
            "Epoch [5/5], Batch [230/1562], Loss: 0.2014, Avg Loss: 0.2162, Time: 30.66s\n",
            "Epoch [5/5], Batch [240/1562], Loss: 0.2165, Avg Loss: 0.2163, Time: 31.91s\n",
            "Epoch [5/5], Batch [250/1562], Loss: 0.2223, Avg Loss: 0.2163, Time: 33.14s\n",
            "Epoch [5/5], Batch [260/1562], Loss: 0.2120, Avg Loss: 0.2162, Time: 34.38s\n",
            "Epoch [5/5], Batch [270/1562], Loss: 0.2182, Avg Loss: 0.2163, Time: 35.63s\n",
            "Epoch [5/5], Batch [280/1562], Loss: 0.2174, Avg Loss: 0.2164, Time: 36.86s\n",
            "Epoch [5/5], Batch [290/1562], Loss: 0.2143, Avg Loss: 0.2163, Time: 38.13s\n",
            "Epoch [5/5], Batch [300/1562], Loss: 0.2229, Avg Loss: 0.2162, Time: 39.39s\n",
            "Epoch [5/5], Batch [310/1562], Loss: 0.2222, Avg Loss: 0.2163, Time: 40.77s\n",
            "Epoch [5/5], Batch [320/1562], Loss: 0.2236, Avg Loss: 0.2164, Time: 42.39s\n",
            "Epoch [5/5], Batch [330/1562], Loss: 0.2223, Avg Loss: 0.2165, Time: 43.69s\n",
            "Epoch [5/5], Batch [340/1562], Loss: 0.2242, Avg Loss: 0.2166, Time: 44.97s\n",
            "Epoch [5/5], Batch [350/1562], Loss: 0.2263, Avg Loss: 0.2167, Time: 46.26s\n",
            "Epoch [5/5], Batch [360/1562], Loss: 0.2182, Avg Loss: 0.2168, Time: 47.56s\n",
            "Epoch [5/5], Batch [370/1562], Loss: 0.2233, Avg Loss: 0.2168, Time: 48.88s\n",
            "Epoch [5/5], Batch [380/1562], Loss: 0.2128, Avg Loss: 0.2169, Time: 50.17s\n",
            "Epoch [5/5], Batch [390/1562], Loss: 0.2171, Avg Loss: 0.2170, Time: 51.47s\n",
            "Epoch [5/5], Batch [400/1562], Loss: 0.2225, Avg Loss: 0.2170, Time: 52.83s\n",
            "Epoch [5/5], Batch [410/1562], Loss: 0.2243, Avg Loss: 0.2171, Time: 54.52s\n",
            "Epoch [5/5], Batch [420/1562], Loss: 0.2223, Avg Loss: 0.2171, Time: 55.90s\n",
            "Epoch [5/5], Batch [430/1562], Loss: 0.2208, Avg Loss: 0.2171, Time: 57.19s\n",
            "Epoch [5/5], Batch [440/1562], Loss: 0.2158, Avg Loss: 0.2171, Time: 58.49s\n",
            "Epoch [5/5], Batch [450/1562], Loss: 0.2302, Avg Loss: 0.2171, Time: 59.78s\n",
            "Epoch [5/5], Batch [460/1562], Loss: 0.2143, Avg Loss: 0.2172, Time: 61.06s\n",
            "Epoch [5/5], Batch [470/1562], Loss: 0.2124, Avg Loss: 0.2172, Time: 62.35s\n",
            "Epoch [5/5], Batch [480/1562], Loss: 0.2108, Avg Loss: 0.2173, Time: 63.65s\n",
            "Epoch [5/5], Batch [490/1562], Loss: 0.2146, Avg Loss: 0.2173, Time: 64.93s\n",
            "Epoch [5/5], Batch [500/1562], Loss: 0.1961, Avg Loss: 0.2173, Time: 66.57s\n",
            "Epoch [5/5], Batch [510/1562], Loss: 0.2176, Avg Loss: 0.2173, Time: 68.03s\n",
            "Epoch [5/5], Batch [520/1562], Loss: 0.2123, Avg Loss: 0.2173, Time: 69.30s\n",
            "Epoch [5/5], Batch [530/1562], Loss: 0.2181, Avg Loss: 0.2174, Time: 70.61s\n",
            "Epoch [5/5], Batch [540/1562], Loss: 0.2198, Avg Loss: 0.2174, Time: 71.90s\n",
            "Epoch [5/5], Batch [550/1562], Loss: 0.2196, Avg Loss: 0.2174, Time: 73.20s\n",
            "Epoch [5/5], Batch [560/1562], Loss: 0.2139, Avg Loss: 0.2174, Time: 74.47s\n",
            "Epoch [5/5], Batch [570/1562], Loss: 0.2184, Avg Loss: 0.2174, Time: 75.75s\n",
            "Epoch [5/5], Batch [580/1562], Loss: 0.2147, Avg Loss: 0.2173, Time: 77.02s\n",
            "Epoch [5/5], Batch [590/1562], Loss: 0.2147, Avg Loss: 0.2173, Time: 78.54s\n",
            "Epoch [5/5], Batch [600/1562], Loss: 0.2209, Avg Loss: 0.2173, Time: 80.05s\n",
            "Epoch [5/5], Batch [610/1562], Loss: 0.2167, Avg Loss: 0.2173, Time: 81.33s\n",
            "Epoch [5/5], Batch [620/1562], Loss: 0.2056, Avg Loss: 0.2172, Time: 82.63s\n",
            "Epoch [5/5], Batch [630/1562], Loss: 0.2234, Avg Loss: 0.2173, Time: 83.91s\n",
            "Epoch [5/5], Batch [640/1562], Loss: 0.2153, Avg Loss: 0.2172, Time: 85.22s\n",
            "Epoch [5/5], Batch [650/1562], Loss: 0.2228, Avg Loss: 0.2173, Time: 86.54s\n",
            "Epoch [5/5], Batch [660/1562], Loss: 0.2178, Avg Loss: 0.2173, Time: 87.83s\n",
            "Epoch [5/5], Batch [670/1562], Loss: 0.2234, Avg Loss: 0.2173, Time: 89.13s\n",
            "Epoch [5/5], Batch [680/1562], Loss: 0.2095, Avg Loss: 0.2173, Time: 90.61s\n",
            "Epoch [5/5], Batch [690/1562], Loss: 0.2074, Avg Loss: 0.2172, Time: 92.20s\n",
            "Epoch [5/5], Batch [700/1562], Loss: 0.2092, Avg Loss: 0.2172, Time: 93.47s\n",
            "Epoch [5/5], Batch [710/1562], Loss: 0.2203, Avg Loss: 0.2172, Time: 94.75s\n",
            "Epoch [5/5], Batch [720/1562], Loss: 0.2180, Avg Loss: 0.2172, Time: 96.03s\n",
            "Epoch [5/5], Batch [730/1562], Loss: 0.2123, Avg Loss: 0.2172, Time: 97.30s\n",
            "Epoch [5/5], Batch [740/1562], Loss: 0.2199, Avg Loss: 0.2172, Time: 98.57s\n",
            "Epoch [5/5], Batch [750/1562], Loss: 0.2168, Avg Loss: 0.2172, Time: 99.86s\n",
            "Epoch [5/5], Batch [760/1562], Loss: 0.2163, Avg Loss: 0.2172, Time: 101.17s\n",
            "Epoch [5/5], Batch [770/1562], Loss: 0.2190, Avg Loss: 0.2172, Time: 102.54s\n",
            "Epoch [5/5], Batch [780/1562], Loss: 0.2143, Avg Loss: 0.2172, Time: 104.16s\n",
            "Epoch [5/5], Batch [790/1562], Loss: 0.2087, Avg Loss: 0.2172, Time: 105.50s\n",
            "Epoch [5/5], Batch [800/1562], Loss: 0.2105, Avg Loss: 0.2172, Time: 106.76s\n",
            "Epoch [5/5], Batch [810/1562], Loss: 0.2189, Avg Loss: 0.2172, Time: 108.04s\n",
            "Epoch [5/5], Batch [820/1562], Loss: 0.2210, Avg Loss: 0.2172, Time: 109.33s\n",
            "Epoch [5/5], Batch [830/1562], Loss: 0.2112, Avg Loss: 0.2172, Time: 110.62s\n",
            "Epoch [5/5], Batch [840/1562], Loss: 0.2168, Avg Loss: 0.2172, Time: 111.91s\n",
            "Epoch [5/5], Batch [850/1562], Loss: 0.2261, Avg Loss: 0.2172, Time: 113.23s\n",
            "Epoch [5/5], Batch [860/1562], Loss: 0.2220, Avg Loss: 0.2173, Time: 114.52s\n",
            "Epoch [5/5], Batch [870/1562], Loss: 0.2165, Avg Loss: 0.2173, Time: 116.12s\n",
            "Epoch [5/5], Batch [880/1562], Loss: 0.2087, Avg Loss: 0.2173, Time: 117.53s\n",
            "Epoch [5/5], Batch [890/1562], Loss: 0.2083, Avg Loss: 0.2173, Time: 118.81s\n",
            "Epoch [5/5], Batch [900/1562], Loss: 0.2161, Avg Loss: 0.2173, Time: 120.08s\n",
            "Epoch [5/5], Batch [910/1562], Loss: 0.2200, Avg Loss: 0.2173, Time: 121.34s\n",
            "Epoch [5/5], Batch [920/1562], Loss: 0.2211, Avg Loss: 0.2173, Time: 122.63s\n",
            "Epoch [5/5], Batch [930/1562], Loss: 0.2231, Avg Loss: 0.2173, Time: 123.93s\n",
            "Epoch [5/5], Batch [940/1562], Loss: 0.2132, Avg Loss: 0.2173, Time: 125.23s\n",
            "Epoch [5/5], Batch [950/1562], Loss: 0.2092, Avg Loss: 0.2172, Time: 126.53s\n",
            "Epoch [5/5], Batch [960/1562], Loss: 0.2137, Avg Loss: 0.2172, Time: 128.16s\n",
            "Epoch [5/5], Batch [970/1562], Loss: 0.2221, Avg Loss: 0.2172, Time: 129.65s\n",
            "Epoch [5/5], Batch [980/1562], Loss: 0.2177, Avg Loss: 0.2172, Time: 130.97s\n",
            "Epoch [5/5], Batch [990/1562], Loss: 0.2270, Avg Loss: 0.2172, Time: 132.26s\n",
            "Epoch [5/5], Batch [1000/1562], Loss: 0.2269, Avg Loss: 0.2171, Time: 133.56s\n",
            "Epoch [5/5], Batch [1010/1562], Loss: 0.2255, Avg Loss: 0.2171, Time: 134.83s\n",
            "Epoch [5/5], Batch [1020/1562], Loss: 0.2200, Avg Loss: 0.2171, Time: 136.14s\n",
            "Epoch [5/5], Batch [1030/1562], Loss: 0.2108, Avg Loss: 0.2171, Time: 137.45s\n",
            "Epoch [5/5], Batch [1040/1562], Loss: 0.2095, Avg Loss: 0.2170, Time: 138.74s\n",
            "Epoch [5/5], Batch [1050/1562], Loss: 0.2217, Avg Loss: 0.2170, Time: 140.26s\n",
            "Epoch [5/5], Batch [1060/1562], Loss: 0.2190, Avg Loss: 0.2171, Time: 141.80s\n",
            "Epoch [5/5], Batch [1070/1562], Loss: 0.2128, Avg Loss: 0.2170, Time: 143.13s\n",
            "Epoch [5/5], Batch [1080/1562], Loss: 0.2276, Avg Loss: 0.2170, Time: 144.43s\n",
            "Epoch [5/5], Batch [1090/1562], Loss: 0.2231, Avg Loss: 0.2170, Time: 145.70s\n",
            "Epoch [5/5], Batch [1100/1562], Loss: 0.2146, Avg Loss: 0.2170, Time: 146.97s\n",
            "Epoch [5/5], Batch [1110/1562], Loss: 0.2295, Avg Loss: 0.2170, Time: 148.27s\n",
            "Epoch [5/5], Batch [1120/1562], Loss: 0.2096, Avg Loss: 0.2170, Time: 149.56s\n",
            "Epoch [5/5], Batch [1130/1562], Loss: 0.2099, Avg Loss: 0.2170, Time: 150.83s\n",
            "Epoch [5/5], Batch [1140/1562], Loss: 0.2177, Avg Loss: 0.2169, Time: 152.27s\n",
            "Epoch [5/5], Batch [1150/1562], Loss: 0.2155, Avg Loss: 0.2169, Time: 153.93s\n",
            "Epoch [5/5], Batch [1160/1562], Loss: 0.2187, Avg Loss: 0.2169, Time: 155.20s\n",
            "Epoch [5/5], Batch [1170/1562], Loss: 0.2310, Avg Loss: 0.2169, Time: 156.47s\n",
            "Epoch [5/5], Batch [1180/1562], Loss: 0.2218, Avg Loss: 0.2169, Time: 157.76s\n",
            "Epoch [5/5], Batch [1190/1562], Loss: 0.2194, Avg Loss: 0.2169, Time: 159.02s\n",
            "Epoch [5/5], Batch [1200/1562], Loss: 0.2149, Avg Loss: 0.2169, Time: 160.30s\n",
            "Epoch [5/5], Batch [1210/1562], Loss: 0.2191, Avg Loss: 0.2169, Time: 161.60s\n",
            "Epoch [5/5], Batch [1220/1562], Loss: 0.2095, Avg Loss: 0.2169, Time: 162.89s\n",
            "Epoch [5/5], Batch [1230/1562], Loss: 0.2071, Avg Loss: 0.2169, Time: 164.20s\n",
            "Epoch [5/5], Batch [1240/1562], Loss: 0.2163, Avg Loss: 0.2168, Time: 165.84s\n",
            "Epoch [5/5], Batch [1250/1562], Loss: 0.2225, Avg Loss: 0.2168, Time: 167.24s\n",
            "Epoch [5/5], Batch [1260/1562], Loss: 0.2134, Avg Loss: 0.2168, Time: 168.52s\n",
            "Epoch [5/5], Batch [1270/1562], Loss: 0.2205, Avg Loss: 0.2168, Time: 169.81s\n",
            "Epoch [5/5], Batch [1280/1562], Loss: 0.2201, Avg Loss: 0.2168, Time: 171.08s\n",
            "Epoch [5/5], Batch [1290/1562], Loss: 0.2199, Avg Loss: 0.2168, Time: 172.37s\n",
            "Epoch [5/5], Batch [1300/1562], Loss: 0.1955, Avg Loss: 0.2168, Time: 173.66s\n",
            "Epoch [5/5], Batch [1310/1562], Loss: 0.2213, Avg Loss: 0.2168, Time: 174.93s\n",
            "Epoch [5/5], Batch [1320/1562], Loss: 0.2173, Avg Loss: 0.2168, Time: 176.22s\n",
            "Epoch [5/5], Batch [1330/1562], Loss: 0.2091, Avg Loss: 0.2168, Time: 177.83s\n",
            "Epoch [5/5], Batch [1340/1562], Loss: 0.2196, Avg Loss: 0.2168, Time: 179.31s\n",
            "Epoch [5/5], Batch [1350/1562], Loss: 0.2161, Avg Loss: 0.2168, Time: 180.58s\n",
            "Epoch [5/5], Batch [1360/1562], Loss: 0.2219, Avg Loss: 0.2168, Time: 181.85s\n",
            "Epoch [5/5], Batch [1370/1562], Loss: 0.2258, Avg Loss: 0.2168, Time: 183.13s\n",
            "Epoch [5/5], Batch [1380/1562], Loss: 0.2063, Avg Loss: 0.2168, Time: 184.41s\n",
            "Epoch [5/5], Batch [1390/1562], Loss: 0.2114, Avg Loss: 0.2167, Time: 185.72s\n",
            "Epoch [5/5], Batch [1400/1562], Loss: 0.2076, Avg Loss: 0.2167, Time: 187.03s\n",
            "Epoch [5/5], Batch [1410/1562], Loss: 0.2105, Avg Loss: 0.2167, Time: 188.32s\n",
            "Epoch [5/5], Batch [1420/1562], Loss: 0.2198, Avg Loss: 0.2167, Time: 189.84s\n",
            "Epoch [5/5], Batch [1430/1562], Loss: 0.2271, Avg Loss: 0.2167, Time: 191.41s\n",
            "Epoch [5/5], Batch [1440/1562], Loss: 0.2188, Avg Loss: 0.2167, Time: 192.70s\n",
            "Epoch [5/5], Batch [1450/1562], Loss: 0.2224, Avg Loss: 0.2167, Time: 193.99s\n",
            "Epoch [5/5], Batch [1460/1562], Loss: 0.2076, Avg Loss: 0.2166, Time: 195.27s\n",
            "Epoch [5/5], Batch [1470/1562], Loss: 0.2161, Avg Loss: 0.2166, Time: 196.56s\n",
            "Epoch [5/5], Batch [1480/1562], Loss: 0.2021, Avg Loss: 0.2166, Time: 197.82s\n",
            "Epoch [5/5], Batch [1490/1562], Loss: 0.2172, Avg Loss: 0.2166, Time: 199.08s\n",
            "Epoch [5/5], Batch [1500/1562], Loss: 0.2143, Avg Loss: 0.2166, Time: 200.34s\n",
            "Epoch [5/5], Batch [1510/1562], Loss: 0.2051, Avg Loss: 0.2165, Time: 201.73s\n",
            "Epoch [5/5], Batch [1520/1562], Loss: 0.2083, Avg Loss: 0.2165, Time: 203.33s\n",
            "Epoch [5/5], Batch [1530/1562], Loss: 0.2294, Avg Loss: 0.2165, Time: 204.65s\n",
            "Epoch [5/5], Batch [1540/1562], Loss: 0.2127, Avg Loss: 0.2165, Time: 205.93s\n",
            "Epoch [5/5], Batch [1550/1562], Loss: 0.2126, Avg Loss: 0.2165, Time: 207.23s\n",
            "Epoch [5/5], Batch [1560/1562], Loss: 0.2161, Avg Loss: 0.2165, Time: 208.51s\n",
            "End of Epoch [5/5], Avg Loss: 0.2165, Total Time: 208.73s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(model, start_seq, max_len, temperature=1.0):\n",
        "    model.eval()\n",
        "    chars = [char_to_idx[ch] for ch in start_seq]\n",
        "    input_seq = torch.tensor(chars, dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "    hidden = model.init_hidden(1)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        output, hidden = model(input_seq, hidden)\n",
        "\n",
        "        logits = output[:, -1, :] / temperature\n",
        "\n",
        "        # Add Gumbel noise\n",
        "        gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits)))\n",
        "        noisy_logits = logits + gumbel_noise\n",
        "\n",
        "        next_char_idx = torch.argmax(noisy_logits).item()\n",
        "\n",
        "        chars.append(next_char_idx)\n",
        "        input_seq = torch.tensor([[next_char_idx]], dtype=torch.long)\n",
        "\n",
        "    return ''.join(idx_to_char[idx] for idx in chars)\n",
        "\n",
        "start_seq = \"To be or not to be\"\n",
        "generated_text = sample(model, start_seq, 100)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOgxXHtSQLZc",
        "outputId": "f96aaf30-03b4-4db9-f417-c3e3ff778371"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To be or not to be his worthy deeds disbench'd you nourive,\n",
            "or peace to all's his:\n",
            "When, by your price o' the consulsh\n"
          ]
        }
      ]
    }
  ]
}