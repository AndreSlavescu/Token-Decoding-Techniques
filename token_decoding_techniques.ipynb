{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNXOTpbnnst9OU1r85HTwd5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndreSlavescu/Token-Sampling/blob/main/token_decoding_techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Token Decoding Methods\n",
        "\n",
        "1. Gumbel Max-Trick\n",
        "2. Top-K Decoding\n",
        "2. Greedy Decoding\n"
      ],
      "metadata": {
        "id": "nTOncJMpPYfc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ihPVIGIIPFoE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import urllib\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "response = urllib.request.urlopen(url)\n",
        "shakespeare_text = response.read().decode('utf-8')[:100000]\n",
        "\n",
        "chars = sorted(list(set(shakespeare_text)))\n",
        "vocab_size = len(chars)\n",
        "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
        "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "data = [char_to_idx[ch] for ch in shakespeare_text]\n",
        "\n",
        "seq_length = 64\n",
        "\n",
        "class DatasetLoader(Dataset):\n",
        "    def __init__(self, data, seq_length):\n",
        "        self.data = data\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.seq_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx:idx + self.seq_length]\n",
        "        y = self.data[idx + 1:idx + self.seq_length + 1]\n",
        "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "batch_size = seq_length\n",
        "dataset = DatasetLoader(data, seq_length)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
        "        super(LSTMLanguageModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        out, hidden = self.lstm(x, hidden)\n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        return (weight.new(self.lstm.num_layers, batch_size, self.lstm.hidden_size).zero_(),\n",
        "                weight.new(self.lstm.num_layers, batch_size, self.lstm.hidden_size).zero_())\n",
        "\n",
        "embedding_dim = 128\n",
        "hidden_dim = 256\n",
        "num_layers = 2\n",
        "model = LSTMLanguageModel(vocab_size, embedding_dim, hidden_dim, num_layers)"
      ],
      "metadata": {
        "id": "ed5Z4u9iP09x"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "num_epochs = 5\n",
        "learning_rate = 0.001\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    start_time = time.time()\n",
        "\n",
        "    total_loss = 0\n",
        "    batch_count = len(dataloader)\n",
        "\n",
        "    for batch_idx, (x, y) in enumerate(dataloader):\n",
        "        hidden = model.init_hidden(x.size(0))\n",
        "        hidden = tuple([h.data for h in hidden])\n",
        "\n",
        "        model.zero_grad()\n",
        "        output, hidden = model(x, hidden)\n",
        "        loss = criterion(output.view(-1, vocab_size), y.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            avg_loss = total_loss / (batch_idx + 1)\n",
        "            elapsed_time = time.time() - start_time\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{batch_count}], '\n",
        "                  f'Loss: {loss.item():.4f}, Avg Loss: {avg_loss:.4f}, Time: {elapsed_time:.2f}s')\n",
        "\n",
        "    avg_epoch_loss = total_loss / batch_count\n",
        "    print(f'End of Epoch [{epoch+1}/{num_epochs}], Avg Loss: {avg_epoch_loss:.4f}, '\n",
        "          f'Total Time: {time.time() - start_time:.2f}s')\n",
        "\n",
        "torch.save(model.state_dict(), 'gumbel_sampling_shakespeare_lstm.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Qh-NERiP91k",
        "outputId": "fba9e79e-a9e1-4c60-870c-ee48c7c7298d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Batch [10/1562], Loss: 1.5700, Avg Loss: 1.5771, Time: 1.65s\n",
            "Epoch [1/5], Batch [20/1562], Loss: 1.4954, Avg Loss: 1.5583, Time: 3.25s\n",
            "Epoch [1/5], Batch [30/1562], Loss: 1.5441, Avg Loss: 1.5519, Time: 4.85s\n",
            "Epoch [1/5], Batch [40/1562], Loss: 1.4940, Avg Loss: 1.5399, Time: 6.53s\n",
            "Epoch [1/5], Batch [50/1562], Loss: 1.5513, Avg Loss: 1.5344, Time: 8.23s\n",
            "Epoch [1/5], Batch [60/1562], Loss: 1.4678, Avg Loss: 1.5286, Time: 10.07s\n",
            "Epoch [1/5], Batch [70/1562], Loss: 1.4679, Avg Loss: 1.5234, Time: 12.03s\n",
            "Epoch [1/5], Batch [80/1562], Loss: 1.4875, Avg Loss: 1.5205, Time: 13.67s\n",
            "Epoch [1/5], Batch [90/1562], Loss: 1.4634, Avg Loss: 1.5162, Time: 15.27s\n",
            "Epoch [1/5], Batch [100/1562], Loss: 1.4520, Avg Loss: 1.5114, Time: 16.86s\n",
            "Epoch [1/5], Batch [110/1562], Loss: 1.4274, Avg Loss: 1.5073, Time: 18.48s\n",
            "Epoch [1/5], Batch [120/1562], Loss: 1.4642, Avg Loss: 1.5031, Time: 20.12s\n",
            "Epoch [1/5], Batch [130/1562], Loss: 1.4498, Avg Loss: 1.4987, Time: 21.97s\n",
            "Epoch [1/5], Batch [140/1562], Loss: 1.4080, Avg Loss: 1.4948, Time: 24.01s\n",
            "Epoch [1/5], Batch [150/1562], Loss: 1.4269, Avg Loss: 1.4906, Time: 25.65s\n",
            "Epoch [1/5], Batch [160/1562], Loss: 1.3719, Avg Loss: 1.4866, Time: 27.29s\n",
            "Epoch [1/5], Batch [170/1562], Loss: 1.4098, Avg Loss: 1.4824, Time: 28.92s\n",
            "Epoch [1/5], Batch [180/1562], Loss: 1.3827, Avg Loss: 1.4782, Time: 30.53s\n",
            "Epoch [1/5], Batch [190/1562], Loss: 1.3788, Avg Loss: 1.4747, Time: 32.17s\n",
            "Epoch [1/5], Batch [200/1562], Loss: 1.4066, Avg Loss: 1.4710, Time: 34.03s\n",
            "Epoch [1/5], Batch [210/1562], Loss: 1.4036, Avg Loss: 1.4678, Time: 36.00s\n",
            "Epoch [1/5], Batch [220/1562], Loss: 1.3955, Avg Loss: 1.4640, Time: 37.61s\n",
            "Epoch [1/5], Batch [230/1562], Loss: 1.3545, Avg Loss: 1.4605, Time: 39.24s\n",
            "Epoch [1/5], Batch [240/1562], Loss: 1.3815, Avg Loss: 1.4570, Time: 40.91s\n",
            "Epoch [1/5], Batch [250/1562], Loss: 1.3814, Avg Loss: 1.4537, Time: 42.54s\n",
            "Epoch [1/5], Batch [260/1562], Loss: 1.3991, Avg Loss: 1.4508, Time: 44.19s\n",
            "Epoch [1/5], Batch [270/1562], Loss: 1.3148, Avg Loss: 1.4471, Time: 46.05s\n",
            "Epoch [1/5], Batch [280/1562], Loss: 1.3637, Avg Loss: 1.4438, Time: 48.02s\n",
            "Epoch [1/5], Batch [290/1562], Loss: 1.3428, Avg Loss: 1.4406, Time: 49.70s\n",
            "Epoch [1/5], Batch [300/1562], Loss: 1.3257, Avg Loss: 1.4377, Time: 51.33s\n",
            "Epoch [1/5], Batch [310/1562], Loss: 1.3408, Avg Loss: 1.4346, Time: 52.95s\n",
            "Epoch [1/5], Batch [320/1562], Loss: 1.2969, Avg Loss: 1.4312, Time: 54.60s\n",
            "Epoch [1/5], Batch [330/1562], Loss: 1.3677, Avg Loss: 1.4283, Time: 56.26s\n",
            "Epoch [1/5], Batch [340/1562], Loss: 1.3678, Avg Loss: 1.4258, Time: 58.19s\n",
            "Epoch [1/5], Batch [350/1562], Loss: 1.3089, Avg Loss: 1.4223, Time: 60.17s\n",
            "Epoch [1/5], Batch [360/1562], Loss: 1.3120, Avg Loss: 1.4194, Time: 61.86s\n",
            "Epoch [1/5], Batch [370/1562], Loss: 1.2923, Avg Loss: 1.4159, Time: 63.52s\n",
            "Epoch [1/5], Batch [380/1562], Loss: 1.3036, Avg Loss: 1.4125, Time: 65.14s\n",
            "Epoch [1/5], Batch [390/1562], Loss: 1.2962, Avg Loss: 1.4098, Time: 66.79s\n",
            "Epoch [1/5], Batch [400/1562], Loss: 1.3544, Avg Loss: 1.4068, Time: 68.42s\n",
            "Epoch [1/5], Batch [410/1562], Loss: 1.2820, Avg Loss: 1.4037, Time: 70.35s\n",
            "Epoch [1/5], Batch [420/1562], Loss: 1.2801, Avg Loss: 1.4007, Time: 72.28s\n",
            "Epoch [1/5], Batch [430/1562], Loss: 1.2256, Avg Loss: 1.3973, Time: 73.99s\n",
            "Epoch [1/5], Batch [440/1562], Loss: 1.2317, Avg Loss: 1.3940, Time: 75.69s\n",
            "Epoch [1/5], Batch [450/1562], Loss: 1.2568, Avg Loss: 1.3912, Time: 77.37s\n",
            "Epoch [1/5], Batch [460/1562], Loss: 1.2398, Avg Loss: 1.3881, Time: 79.00s\n",
            "Epoch [1/5], Batch [470/1562], Loss: 1.2521, Avg Loss: 1.3851, Time: 80.62s\n",
            "Epoch [1/5], Batch [480/1562], Loss: 1.2570, Avg Loss: 1.3820, Time: 82.65s\n",
            "Epoch [1/5], Batch [490/1562], Loss: 1.1965, Avg Loss: 1.3788, Time: 84.47s\n",
            "Epoch [1/5], Batch [500/1562], Loss: 1.2114, Avg Loss: 1.3756, Time: 86.13s\n",
            "Epoch [1/5], Batch [510/1562], Loss: 1.1962, Avg Loss: 1.3725, Time: 87.81s\n",
            "Epoch [1/5], Batch [520/1562], Loss: 1.2371, Avg Loss: 1.3699, Time: 89.53s\n",
            "Epoch [1/5], Batch [530/1562], Loss: 1.2582, Avg Loss: 1.3670, Time: 91.19s\n",
            "Epoch [1/5], Batch [540/1562], Loss: 1.2090, Avg Loss: 1.3639, Time: 92.87s\n",
            "Epoch [1/5], Batch [550/1562], Loss: 1.1890, Avg Loss: 1.3609, Time: 94.92s\n",
            "Epoch [1/5], Batch [560/1562], Loss: 1.2263, Avg Loss: 1.3577, Time: 96.63s\n",
            "Epoch [1/5], Batch [570/1562], Loss: 1.2012, Avg Loss: 1.3546, Time: 98.24s\n",
            "Epoch [1/5], Batch [580/1562], Loss: 1.1717, Avg Loss: 1.3516, Time: 99.87s\n",
            "Epoch [1/5], Batch [590/1562], Loss: 1.2055, Avg Loss: 1.3487, Time: 101.51s\n",
            "Epoch [1/5], Batch [600/1562], Loss: 1.2097, Avg Loss: 1.3457, Time: 103.19s\n",
            "Epoch [1/5], Batch [610/1562], Loss: 1.1032, Avg Loss: 1.3425, Time: 104.87s\n",
            "Epoch [1/5], Batch [620/1562], Loss: 1.1716, Avg Loss: 1.3395, Time: 107.00s\n",
            "Epoch [1/5], Batch [630/1562], Loss: 1.1765, Avg Loss: 1.3364, Time: 108.73s\n",
            "Epoch [1/5], Batch [640/1562], Loss: 1.2013, Avg Loss: 1.3335, Time: 110.39s\n",
            "Epoch [1/5], Batch [650/1562], Loss: 1.1174, Avg Loss: 1.3304, Time: 112.04s\n",
            "Epoch [1/5], Batch [660/1562], Loss: 1.1234, Avg Loss: 1.3274, Time: 113.67s\n",
            "Epoch [1/5], Batch [670/1562], Loss: 1.0817, Avg Loss: 1.3244, Time: 115.34s\n",
            "Epoch [1/5], Batch [680/1562], Loss: 1.1006, Avg Loss: 1.3214, Time: 117.00s\n",
            "Epoch [1/5], Batch [690/1562], Loss: 1.1017, Avg Loss: 1.3184, Time: 119.14s\n",
            "Epoch [1/5], Batch [700/1562], Loss: 1.1621, Avg Loss: 1.3156, Time: 120.84s\n",
            "Epoch [1/5], Batch [710/1562], Loss: 1.0907, Avg Loss: 1.3125, Time: 122.52s\n",
            "Epoch [1/5], Batch [720/1562], Loss: 1.0394, Avg Loss: 1.3095, Time: 124.20s\n",
            "Epoch [1/5], Batch [730/1562], Loss: 1.1211, Avg Loss: 1.3066, Time: 125.86s\n",
            "Epoch [1/5], Batch [740/1562], Loss: 1.0973, Avg Loss: 1.3036, Time: 127.50s\n",
            "Epoch [1/5], Batch [750/1562], Loss: 1.0729, Avg Loss: 1.3005, Time: 129.18s\n",
            "Epoch [1/5], Batch [760/1562], Loss: 1.0444, Avg Loss: 1.2976, Time: 131.39s\n",
            "Epoch [1/5], Batch [770/1562], Loss: 1.0409, Avg Loss: 1.2945, Time: 133.07s\n",
            "Epoch [1/5], Batch [780/1562], Loss: 1.0501, Avg Loss: 1.2916, Time: 134.74s\n",
            "Epoch [1/5], Batch [790/1562], Loss: 1.1001, Avg Loss: 1.2886, Time: 136.40s\n",
            "Epoch [1/5], Batch [800/1562], Loss: 1.0664, Avg Loss: 1.2857, Time: 138.07s\n",
            "Epoch [1/5], Batch [810/1562], Loss: 1.0543, Avg Loss: 1.2827, Time: 139.69s\n",
            "Epoch [1/5], Batch [820/1562], Loss: 1.0479, Avg Loss: 1.2798, Time: 141.35s\n",
            "Epoch [1/5], Batch [830/1562], Loss: 1.0423, Avg Loss: 1.2768, Time: 143.47s\n",
            "Epoch [1/5], Batch [840/1562], Loss: 1.0361, Avg Loss: 1.2738, Time: 145.14s\n",
            "Epoch [1/5], Batch [850/1562], Loss: 0.9893, Avg Loss: 1.2708, Time: 146.76s\n",
            "Epoch [1/5], Batch [860/1562], Loss: 1.0120, Avg Loss: 1.2677, Time: 148.35s\n",
            "Epoch [1/5], Batch [870/1562], Loss: 0.9581, Avg Loss: 1.2648, Time: 149.96s\n",
            "Epoch [1/5], Batch [880/1562], Loss: 1.0025, Avg Loss: 1.2618, Time: 151.57s\n",
            "Epoch [1/5], Batch [890/1562], Loss: 0.9595, Avg Loss: 1.2588, Time: 153.28s\n",
            "Epoch [1/5], Batch [900/1562], Loss: 1.0312, Avg Loss: 1.2558, Time: 155.43s\n",
            "Epoch [1/5], Batch [910/1562], Loss: 1.0116, Avg Loss: 1.2529, Time: 157.12s\n",
            "Epoch [1/5], Batch [920/1562], Loss: 0.9668, Avg Loss: 1.2500, Time: 158.78s\n",
            "Epoch [1/5], Batch [930/1562], Loss: 0.9942, Avg Loss: 1.2470, Time: 160.41s\n",
            "Epoch [1/5], Batch [940/1562], Loss: 0.9495, Avg Loss: 1.2441, Time: 162.02s\n",
            "Epoch [1/5], Batch [950/1562], Loss: 0.9261, Avg Loss: 1.2411, Time: 163.64s\n",
            "Epoch [1/5], Batch [960/1562], Loss: 0.9535, Avg Loss: 1.2381, Time: 165.27s\n",
            "Epoch [1/5], Batch [970/1562], Loss: 0.9555, Avg Loss: 1.2351, Time: 167.40s\n",
            "Epoch [1/5], Batch [980/1562], Loss: 0.9559, Avg Loss: 1.2322, Time: 169.09s\n",
            "Epoch [1/5], Batch [990/1562], Loss: 0.9434, Avg Loss: 1.2292, Time: 170.81s\n",
            "Epoch [1/5], Batch [1000/1562], Loss: 0.9409, Avg Loss: 1.2261, Time: 172.47s\n",
            "Epoch [1/5], Batch [1010/1562], Loss: 0.9024, Avg Loss: 1.2230, Time: 174.11s\n",
            "Epoch [1/5], Batch [1020/1562], Loss: 0.9400, Avg Loss: 1.2201, Time: 175.72s\n",
            "Epoch [1/5], Batch [1030/1562], Loss: 0.9182, Avg Loss: 1.2171, Time: 177.37s\n",
            "Epoch [1/5], Batch [1040/1562], Loss: 0.9114, Avg Loss: 1.2142, Time: 179.43s\n",
            "Epoch [1/5], Batch [1050/1562], Loss: 0.8561, Avg Loss: 1.2111, Time: 181.07s\n",
            "Epoch [1/5], Batch [1060/1562], Loss: 0.9444, Avg Loss: 1.2083, Time: 182.70s\n",
            "Epoch [1/5], Batch [1070/1562], Loss: 0.8872, Avg Loss: 1.2052, Time: 184.37s\n",
            "Epoch [1/5], Batch [1080/1562], Loss: 0.8870, Avg Loss: 1.2022, Time: 186.06s\n",
            "Epoch [1/5], Batch [1090/1562], Loss: 0.8785, Avg Loss: 1.1992, Time: 187.69s\n",
            "Epoch [1/5], Batch [1100/1562], Loss: 0.8976, Avg Loss: 1.1962, Time: 189.43s\n",
            "Epoch [1/5], Batch [1110/1562], Loss: 0.8474, Avg Loss: 1.1933, Time: 191.44s\n",
            "Epoch [1/5], Batch [1120/1562], Loss: 0.8180, Avg Loss: 1.1902, Time: 193.08s\n",
            "Epoch [1/5], Batch [1130/1562], Loss: 0.8571, Avg Loss: 1.1873, Time: 194.70s\n",
            "Epoch [1/5], Batch [1140/1562], Loss: 0.8467, Avg Loss: 1.1844, Time: 196.32s\n",
            "Epoch [1/5], Batch [1150/1562], Loss: 0.8434, Avg Loss: 1.1815, Time: 197.97s\n",
            "Epoch [1/5], Batch [1160/1562], Loss: 0.8131, Avg Loss: 1.1786, Time: 199.63s\n",
            "Epoch [1/5], Batch [1170/1562], Loss: 0.8066, Avg Loss: 1.1756, Time: 201.43s\n",
            "Epoch [1/5], Batch [1180/1562], Loss: 0.8331, Avg Loss: 1.1726, Time: 203.50s\n",
            "Epoch [1/5], Batch [1190/1562], Loss: 0.8070, Avg Loss: 1.1697, Time: 205.15s\n",
            "Epoch [1/5], Batch [1200/1562], Loss: 0.8130, Avg Loss: 1.1667, Time: 206.80s\n",
            "Epoch [1/5], Batch [1210/1562], Loss: 0.8109, Avg Loss: 1.1638, Time: 208.41s\n",
            "Epoch [1/5], Batch [1220/1562], Loss: 0.8036, Avg Loss: 1.1608, Time: 210.04s\n",
            "Epoch [1/5], Batch [1230/1562], Loss: 0.7770, Avg Loss: 1.1579, Time: 211.70s\n",
            "Epoch [1/5], Batch [1240/1562], Loss: 0.7953, Avg Loss: 1.1550, Time: 213.47s\n",
            "Epoch [1/5], Batch [1250/1562], Loss: 0.7618, Avg Loss: 1.1520, Time: 215.42s\n",
            "Epoch [1/5], Batch [1260/1562], Loss: 0.7649, Avg Loss: 1.1490, Time: 217.08s\n",
            "Epoch [1/5], Batch [1270/1562], Loss: 0.7628, Avg Loss: 1.1461, Time: 218.75s\n",
            "Epoch [1/5], Batch [1280/1562], Loss: 0.7644, Avg Loss: 1.1431, Time: 220.38s\n",
            "Epoch [1/5], Batch [1290/1562], Loss: 0.7489, Avg Loss: 1.1401, Time: 222.01s\n",
            "Epoch [1/5], Batch [1300/1562], Loss: 0.7514, Avg Loss: 1.1371, Time: 223.68s\n",
            "Epoch [1/5], Batch [1310/1562], Loss: 0.7478, Avg Loss: 1.1343, Time: 225.56s\n",
            "Epoch [1/5], Batch [1320/1562], Loss: 0.7662, Avg Loss: 1.1313, Time: 227.49s\n",
            "Epoch [1/5], Batch [1330/1562], Loss: 0.7527, Avg Loss: 1.1283, Time: 229.14s\n",
            "Epoch [1/5], Batch [1340/1562], Loss: 0.7529, Avg Loss: 1.1254, Time: 230.75s\n",
            "Epoch [1/5], Batch [1350/1562], Loss: 0.7179, Avg Loss: 1.1224, Time: 232.37s\n",
            "Epoch [1/5], Batch [1360/1562], Loss: 0.7033, Avg Loss: 1.1195, Time: 234.03s\n",
            "Epoch [1/5], Batch [1370/1562], Loss: 0.7380, Avg Loss: 1.1166, Time: 235.67s\n",
            "Epoch [1/5], Batch [1380/1562], Loss: 0.7114, Avg Loss: 1.1138, Time: 237.52s\n",
            "Epoch [1/5], Batch [1390/1562], Loss: 0.7049, Avg Loss: 1.1109, Time: 239.53s\n",
            "Epoch [1/5], Batch [1400/1562], Loss: 0.6796, Avg Loss: 1.1080, Time: 241.18s\n",
            "Epoch [1/5], Batch [1410/1562], Loss: 0.7025, Avg Loss: 1.1052, Time: 242.81s\n",
            "Epoch [1/5], Batch [1420/1562], Loss: 0.6711, Avg Loss: 1.1023, Time: 244.43s\n",
            "Epoch [1/5], Batch [1430/1562], Loss: 0.6958, Avg Loss: 1.0994, Time: 246.04s\n",
            "Epoch [1/5], Batch [1440/1562], Loss: 0.6824, Avg Loss: 1.0966, Time: 247.65s\n",
            "Epoch [1/5], Batch [1450/1562], Loss: 0.6822, Avg Loss: 1.0937, Time: 249.48s\n",
            "Epoch [1/5], Batch [1460/1562], Loss: 0.6567, Avg Loss: 1.0909, Time: 251.55s\n",
            "Epoch [1/5], Batch [1470/1562], Loss: 0.6903, Avg Loss: 1.0881, Time: 253.25s\n",
            "Epoch [1/5], Batch [1480/1562], Loss: 0.6581, Avg Loss: 1.0851, Time: 254.93s\n",
            "Epoch [1/5], Batch [1490/1562], Loss: 0.6449, Avg Loss: 1.0823, Time: 256.55s\n",
            "Epoch [1/5], Batch [1500/1562], Loss: 0.6493, Avg Loss: 1.0795, Time: 258.18s\n",
            "Epoch [1/5], Batch [1510/1562], Loss: 0.6648, Avg Loss: 1.0767, Time: 259.83s\n",
            "Epoch [1/5], Batch [1520/1562], Loss: 0.6810, Avg Loss: 1.0739, Time: 261.67s\n",
            "Epoch [1/5], Batch [1530/1562], Loss: 0.6361, Avg Loss: 1.0711, Time: 263.57s\n",
            "Epoch [1/5], Batch [1540/1562], Loss: 0.6203, Avg Loss: 1.0682, Time: 265.26s\n",
            "Epoch [1/5], Batch [1550/1562], Loss: 0.6389, Avg Loss: 1.0654, Time: 266.99s\n",
            "Epoch [1/5], Batch [1560/1562], Loss: 0.6247, Avg Loss: 1.0626, Time: 268.65s\n",
            "End of Epoch [1/5], Avg Loss: 1.0620, Total Time: 268.92s\n",
            "Epoch [2/5], Batch [10/1562], Loss: 0.5966, Avg Loss: 0.6103, Time: 1.64s\n",
            "Epoch [2/5], Batch [20/1562], Loss: 0.5856, Avg Loss: 0.6127, Time: 3.28s\n",
            "Epoch [2/5], Batch [30/1562], Loss: 0.6197, Avg Loss: 0.6104, Time: 5.28s\n",
            "Epoch [2/5], Batch [40/1562], Loss: 0.6190, Avg Loss: 0.6100, Time: 7.00s\n",
            "Epoch [2/5], Batch [50/1562], Loss: 0.5870, Avg Loss: 0.6059, Time: 8.68s\n",
            "Epoch [2/5], Batch [60/1562], Loss: 0.5878, Avg Loss: 0.6038, Time: 10.37s\n",
            "Epoch [2/5], Batch [70/1562], Loss: 0.5808, Avg Loss: 0.6009, Time: 12.03s\n",
            "Epoch [2/5], Batch [80/1562], Loss: 0.5833, Avg Loss: 0.5987, Time: 13.72s\n",
            "Epoch [2/5], Batch [90/1562], Loss: 0.6061, Avg Loss: 0.5968, Time: 15.40s\n",
            "Epoch [2/5], Batch [100/1562], Loss: 0.5705, Avg Loss: 0.5944, Time: 17.51s\n",
            "Epoch [2/5], Batch [110/1562], Loss: 0.5734, Avg Loss: 0.5919, Time: 19.21s\n",
            "Epoch [2/5], Batch [120/1562], Loss: 0.5426, Avg Loss: 0.5897, Time: 20.83s\n",
            "Epoch [2/5], Batch [130/1562], Loss: 0.5740, Avg Loss: 0.5874, Time: 22.47s\n",
            "Epoch [2/5], Batch [140/1562], Loss: 0.5410, Avg Loss: 0.5851, Time: 24.14s\n",
            "Epoch [2/5], Batch [150/1562], Loss: 0.5391, Avg Loss: 0.5830, Time: 25.79s\n",
            "Epoch [2/5], Batch [160/1562], Loss: 0.5539, Avg Loss: 0.5812, Time: 27.40s\n",
            "Epoch [2/5], Batch [170/1562], Loss: 0.5554, Avg Loss: 0.5794, Time: 29.53s\n",
            "Epoch [2/5], Batch [180/1562], Loss: 0.5594, Avg Loss: 0.5777, Time: 31.22s\n",
            "Epoch [2/5], Batch [190/1562], Loss: 0.5539, Avg Loss: 0.5757, Time: 32.86s\n",
            "Epoch [2/5], Batch [200/1562], Loss: 0.5173, Avg Loss: 0.5736, Time: 34.53s\n",
            "Epoch [2/5], Batch [210/1562], Loss: 0.5155, Avg Loss: 0.5716, Time: 36.17s\n",
            "Epoch [2/5], Batch [220/1562], Loss: 0.5289, Avg Loss: 0.5696, Time: 37.82s\n",
            "Epoch [2/5], Batch [230/1562], Loss: 0.5234, Avg Loss: 0.5680, Time: 39.45s\n",
            "Epoch [2/5], Batch [240/1562], Loss: 0.5400, Avg Loss: 0.5659, Time: 41.54s\n",
            "Epoch [2/5], Batch [250/1562], Loss: 0.5267, Avg Loss: 0.5642, Time: 43.17s\n",
            "Epoch [2/5], Batch [260/1562], Loss: 0.5214, Avg Loss: 0.5626, Time: 44.81s\n",
            "Epoch [2/5], Batch [270/1562], Loss: 0.4991, Avg Loss: 0.5605, Time: 46.50s\n",
            "Epoch [2/5], Batch [280/1562], Loss: 0.5055, Avg Loss: 0.5584, Time: 48.13s\n",
            "Epoch [2/5], Batch [290/1562], Loss: 0.4836, Avg Loss: 0.5566, Time: 49.82s\n",
            "Epoch [2/5], Batch [300/1562], Loss: 0.4936, Avg Loss: 0.5547, Time: 51.51s\n",
            "Epoch [2/5], Batch [310/1562], Loss: 0.5007, Avg Loss: 0.5530, Time: 53.63s\n",
            "Epoch [2/5], Batch [320/1562], Loss: 0.4811, Avg Loss: 0.5509, Time: 55.25s\n",
            "Epoch [2/5], Batch [330/1562], Loss: 0.4975, Avg Loss: 0.5494, Time: 56.88s\n",
            "Epoch [2/5], Batch [340/1562], Loss: 0.4785, Avg Loss: 0.5477, Time: 58.52s\n",
            "Epoch [2/5], Batch [350/1562], Loss: 0.4775, Avg Loss: 0.5460, Time: 60.14s\n",
            "Epoch [2/5], Batch [360/1562], Loss: 0.5059, Avg Loss: 0.5444, Time: 61.80s\n",
            "Epoch [2/5], Batch [370/1562], Loss: 0.4640, Avg Loss: 0.5426, Time: 63.52s\n",
            "Epoch [2/5], Batch [380/1562], Loss: 0.4577, Avg Loss: 0.5409, Time: 65.74s\n",
            "Epoch [2/5], Batch [390/1562], Loss: 0.4854, Avg Loss: 0.5393, Time: 67.37s\n",
            "Epoch [2/5], Batch [400/1562], Loss: 0.4719, Avg Loss: 0.5376, Time: 68.99s\n",
            "Epoch [2/5], Batch [410/1562], Loss: 0.4623, Avg Loss: 0.5361, Time: 70.61s\n",
            "Epoch [2/5], Batch [420/1562], Loss: 0.4583, Avg Loss: 0.5343, Time: 72.24s\n",
            "Epoch [2/5], Batch [430/1562], Loss: 0.4797, Avg Loss: 0.5327, Time: 73.86s\n",
            "Epoch [2/5], Batch [440/1562], Loss: 0.4521, Avg Loss: 0.5308, Time: 75.49s\n",
            "Epoch [2/5], Batch [450/1562], Loss: 0.4621, Avg Loss: 0.5292, Time: 77.68s\n",
            "Epoch [2/5], Batch [460/1562], Loss: 0.4703, Avg Loss: 0.5275, Time: 79.39s\n",
            "Epoch [2/5], Batch [470/1562], Loss: 0.4373, Avg Loss: 0.5258, Time: 81.07s\n",
            "Epoch [2/5], Batch [480/1562], Loss: 0.4082, Avg Loss: 0.5240, Time: 82.72s\n",
            "Epoch [2/5], Batch [490/1562], Loss: 0.4336, Avg Loss: 0.5223, Time: 84.35s\n",
            "Epoch [2/5], Batch [500/1562], Loss: 0.4636, Avg Loss: 0.5206, Time: 85.96s\n",
            "Epoch [2/5], Batch [510/1562], Loss: 0.4391, Avg Loss: 0.5190, Time: 87.60s\n",
            "Epoch [2/5], Batch [520/1562], Loss: 0.4355, Avg Loss: 0.5174, Time: 89.69s\n",
            "Epoch [2/5], Batch [530/1562], Loss: 0.4336, Avg Loss: 0.5158, Time: 91.37s\n",
            "Epoch [2/5], Batch [540/1562], Loss: 0.4186, Avg Loss: 0.5142, Time: 93.03s\n",
            "Epoch [2/5], Batch [550/1562], Loss: 0.4443, Avg Loss: 0.5127, Time: 94.69s\n",
            "Epoch [2/5], Batch [560/1562], Loss: 0.4359, Avg Loss: 0.5112, Time: 96.33s\n",
            "Epoch [2/5], Batch [570/1562], Loss: 0.4091, Avg Loss: 0.5098, Time: 97.98s\n",
            "Epoch [2/5], Batch [580/1562], Loss: 0.4180, Avg Loss: 0.5082, Time: 99.69s\n",
            "Epoch [2/5], Batch [590/1562], Loss: 0.4319, Avg Loss: 0.5067, Time: 101.71s\n",
            "Epoch [2/5], Batch [600/1562], Loss: 0.4232, Avg Loss: 0.5053, Time: 103.33s\n",
            "Epoch [2/5], Batch [610/1562], Loss: 0.4111, Avg Loss: 0.5039, Time: 105.00s\n",
            "Epoch [2/5], Batch [620/1562], Loss: 0.4138, Avg Loss: 0.5025, Time: 106.68s\n",
            "Epoch [2/5], Batch [630/1562], Loss: 0.3986, Avg Loss: 0.5010, Time: 108.30s\n",
            "Epoch [2/5], Batch [640/1562], Loss: 0.4192, Avg Loss: 0.4995, Time: 109.97s\n",
            "Epoch [2/5], Batch [650/1562], Loss: 0.4178, Avg Loss: 0.4981, Time: 111.77s\n",
            "Epoch [2/5], Batch [660/1562], Loss: 0.4124, Avg Loss: 0.4967, Time: 113.88s\n",
            "Epoch [2/5], Batch [670/1562], Loss: 0.3905, Avg Loss: 0.4953, Time: 115.54s\n",
            "Epoch [2/5], Batch [680/1562], Loss: 0.3765, Avg Loss: 0.4939, Time: 117.22s\n",
            "Epoch [2/5], Batch [690/1562], Loss: 0.4068, Avg Loss: 0.4926, Time: 118.90s\n",
            "Epoch [2/5], Batch [700/1562], Loss: 0.3842, Avg Loss: 0.4911, Time: 120.58s\n",
            "Epoch [2/5], Batch [710/1562], Loss: 0.3895, Avg Loss: 0.4897, Time: 122.20s\n",
            "Epoch [2/5], Batch [720/1562], Loss: 0.3760, Avg Loss: 0.4884, Time: 124.06s\n",
            "Epoch [2/5], Batch [730/1562], Loss: 0.3766, Avg Loss: 0.4871, Time: 126.03s\n",
            "Epoch [2/5], Batch [740/1562], Loss: 0.3935, Avg Loss: 0.4857, Time: 127.70s\n",
            "Epoch [2/5], Batch [750/1562], Loss: 0.3799, Avg Loss: 0.4844, Time: 129.38s\n",
            "Epoch [2/5], Batch [760/1562], Loss: 0.3915, Avg Loss: 0.4831, Time: 131.07s\n",
            "Epoch [2/5], Batch [770/1562], Loss: 0.3864, Avg Loss: 0.4817, Time: 132.78s\n",
            "Epoch [2/5], Batch [780/1562], Loss: 0.3788, Avg Loss: 0.4805, Time: 134.44s\n",
            "Epoch [2/5], Batch [790/1562], Loss: 0.3790, Avg Loss: 0.4791, Time: 136.38s\n",
            "Epoch [2/5], Batch [800/1562], Loss: 0.3679, Avg Loss: 0.4778, Time: 138.25s\n",
            "Epoch [2/5], Batch [810/1562], Loss: 0.3845, Avg Loss: 0.4765, Time: 139.88s\n",
            "Epoch [2/5], Batch [820/1562], Loss: 0.3824, Avg Loss: 0.4753, Time: 141.53s\n",
            "Epoch [2/5], Batch [830/1562], Loss: 0.3641, Avg Loss: 0.4740, Time: 143.23s\n",
            "Epoch [2/5], Batch [840/1562], Loss: 0.3669, Avg Loss: 0.4728, Time: 144.94s\n",
            "Epoch [2/5], Batch [850/1562], Loss: 0.3481, Avg Loss: 0.4716, Time: 146.65s\n",
            "Epoch [2/5], Batch [860/1562], Loss: 0.3917, Avg Loss: 0.4704, Time: 148.69s\n",
            "Epoch [2/5], Batch [870/1562], Loss: 0.3639, Avg Loss: 0.4692, Time: 150.46s\n",
            "Epoch [2/5], Batch [880/1562], Loss: 0.3669, Avg Loss: 0.4680, Time: 152.10s\n",
            "Epoch [2/5], Batch [890/1562], Loss: 0.3719, Avg Loss: 0.4668, Time: 153.75s\n",
            "Epoch [2/5], Batch [900/1562], Loss: 0.3665, Avg Loss: 0.4656, Time: 155.41s\n",
            "Epoch [2/5], Batch [910/1562], Loss: 0.3779, Avg Loss: 0.4644, Time: 157.05s\n",
            "Epoch [2/5], Batch [920/1562], Loss: 0.3482, Avg Loss: 0.4633, Time: 158.74s\n",
            "Epoch [2/5], Batch [930/1562], Loss: 0.3629, Avg Loss: 0.4621, Time: 160.93s\n",
            "Epoch [2/5], Batch [940/1562], Loss: 0.3419, Avg Loss: 0.4609, Time: 162.72s\n",
            "Epoch [2/5], Batch [950/1562], Loss: 0.3441, Avg Loss: 0.4599, Time: 164.38s\n",
            "Epoch [2/5], Batch [960/1562], Loss: 0.3592, Avg Loss: 0.4587, Time: 166.05s\n",
            "Epoch [2/5], Batch [970/1562], Loss: 0.3284, Avg Loss: 0.4576, Time: 167.71s\n",
            "Epoch [2/5], Batch [980/1562], Loss: 0.3465, Avg Loss: 0.4565, Time: 169.35s\n",
            "Epoch [2/5], Batch [990/1562], Loss: 0.3455, Avg Loss: 0.4554, Time: 170.99s\n",
            "Epoch [2/5], Batch [1000/1562], Loss: 0.3571, Avg Loss: 0.4543, Time: 173.17s\n",
            "Epoch [2/5], Batch [1010/1562], Loss: 0.3463, Avg Loss: 0.4532, Time: 174.91s\n",
            "Epoch [2/5], Batch [1020/1562], Loss: 0.3407, Avg Loss: 0.4522, Time: 176.58s\n",
            "Epoch [2/5], Batch [1030/1562], Loss: 0.3527, Avg Loss: 0.4511, Time: 178.26s\n",
            "Epoch [2/5], Batch [1040/1562], Loss: 0.3471, Avg Loss: 0.4501, Time: 179.94s\n",
            "Epoch [2/5], Batch [1050/1562], Loss: 0.3449, Avg Loss: 0.4490, Time: 181.61s\n",
            "Epoch [2/5], Batch [1060/1562], Loss: 0.3225, Avg Loss: 0.4480, Time: 183.24s\n",
            "Epoch [2/5], Batch [1070/1562], Loss: 0.3406, Avg Loss: 0.4469, Time: 185.38s\n",
            "Epoch [2/5], Batch [1080/1562], Loss: 0.3292, Avg Loss: 0.4459, Time: 187.07s\n",
            "Epoch [2/5], Batch [1090/1562], Loss: 0.3501, Avg Loss: 0.4449, Time: 188.70s\n",
            "Epoch [2/5], Batch [1100/1562], Loss: 0.3279, Avg Loss: 0.4438, Time: 190.34s\n",
            "Epoch [2/5], Batch [1110/1562], Loss: 0.3128, Avg Loss: 0.4428, Time: 192.02s\n",
            "Epoch [2/5], Batch [1120/1562], Loss: 0.3186, Avg Loss: 0.4418, Time: 193.70s\n",
            "Epoch [2/5], Batch [1130/1562], Loss: 0.3268, Avg Loss: 0.4407, Time: 195.56s\n",
            "Epoch [2/5], Batch [1140/1562], Loss: 0.3423, Avg Loss: 0.4398, Time: 197.53s\n",
            "Epoch [2/5], Batch [1150/1562], Loss: 0.3283, Avg Loss: 0.4388, Time: 199.17s\n",
            "Epoch [2/5], Batch [1160/1562], Loss: 0.3306, Avg Loss: 0.4378, Time: 200.82s\n",
            "Epoch [2/5], Batch [1170/1562], Loss: 0.3264, Avg Loss: 0.4368, Time: 202.47s\n",
            "Epoch [2/5], Batch [1180/1562], Loss: 0.3223, Avg Loss: 0.4358, Time: 204.11s\n",
            "Epoch [2/5], Batch [1190/1562], Loss: 0.3247, Avg Loss: 0.4349, Time: 205.72s\n",
            "Epoch [2/5], Batch [1200/1562], Loss: 0.3339, Avg Loss: 0.4339, Time: 207.54s\n",
            "Epoch [2/5], Batch [1210/1562], Loss: 0.3070, Avg Loss: 0.4330, Time: 209.51s\n",
            "Epoch [2/5], Batch [1220/1562], Loss: 0.3091, Avg Loss: 0.4320, Time: 211.18s\n",
            "Epoch [2/5], Batch [1230/1562], Loss: 0.3031, Avg Loss: 0.4310, Time: 212.84s\n",
            "Epoch [2/5], Batch [1240/1562], Loss: 0.3222, Avg Loss: 0.4301, Time: 214.55s\n",
            "Epoch [2/5], Batch [1250/1562], Loss: 0.3190, Avg Loss: 0.4292, Time: 216.19s\n",
            "Epoch [2/5], Batch [1260/1562], Loss: 0.3220, Avg Loss: 0.4282, Time: 217.80s\n",
            "Epoch [2/5], Batch [1270/1562], Loss: 0.3099, Avg Loss: 0.4273, Time: 219.66s\n",
            "Epoch [2/5], Batch [1280/1562], Loss: 0.2950, Avg Loss: 0.4263, Time: 221.54s\n",
            "Epoch [2/5], Batch [1290/1562], Loss: 0.3000, Avg Loss: 0.4254, Time: 223.15s\n",
            "Epoch [2/5], Batch [1300/1562], Loss: 0.3110, Avg Loss: 0.4245, Time: 224.82s\n",
            "Epoch [2/5], Batch [1310/1562], Loss: 0.3131, Avg Loss: 0.4236, Time: 226.52s\n",
            "Epoch [2/5], Batch [1320/1562], Loss: 0.2942, Avg Loss: 0.4228, Time: 228.21s\n",
            "Epoch [2/5], Batch [1330/1562], Loss: 0.3315, Avg Loss: 0.4219, Time: 229.87s\n",
            "Epoch [2/5], Batch [1340/1562], Loss: 0.3174, Avg Loss: 0.4211, Time: 231.77s\n",
            "Epoch [2/5], Batch [1350/1562], Loss: 0.3126, Avg Loss: 0.4202, Time: 233.61s\n",
            "Epoch [2/5], Batch [1360/1562], Loss: 0.3224, Avg Loss: 0.4194, Time: 235.24s\n",
            "Epoch [2/5], Batch [1370/1562], Loss: 0.3025, Avg Loss: 0.4186, Time: 236.83s\n",
            "Epoch [2/5], Batch [1380/1562], Loss: 0.3210, Avg Loss: 0.4178, Time: 238.46s\n",
            "Epoch [2/5], Batch [1390/1562], Loss: 0.3057, Avg Loss: 0.4169, Time: 240.14s\n",
            "Epoch [2/5], Batch [1400/1562], Loss: 0.2989, Avg Loss: 0.4161, Time: 241.84s\n",
            "Epoch [2/5], Batch [1410/1562], Loss: 0.2958, Avg Loss: 0.4153, Time: 243.84s\n",
            "Epoch [2/5], Batch [1420/1562], Loss: 0.3057, Avg Loss: 0.4146, Time: 245.67s\n",
            "Epoch [2/5], Batch [1430/1562], Loss: 0.2814, Avg Loss: 0.4137, Time: 247.28s\n",
            "Epoch [2/5], Batch [1440/1562], Loss: 0.2918, Avg Loss: 0.4130, Time: 248.90s\n",
            "Epoch [2/5], Batch [1450/1562], Loss: 0.2916, Avg Loss: 0.4122, Time: 250.52s\n",
            "Epoch [2/5], Batch [1460/1562], Loss: 0.2914, Avg Loss: 0.4114, Time: 252.14s\n",
            "Epoch [2/5], Batch [1470/1562], Loss: 0.3069, Avg Loss: 0.4106, Time: 253.79s\n",
            "Epoch [2/5], Batch [1480/1562], Loss: 0.2914, Avg Loss: 0.4099, Time: 255.76s\n",
            "Epoch [2/5], Batch [1490/1562], Loss: 0.2933, Avg Loss: 0.4092, Time: 257.65s\n",
            "Epoch [2/5], Batch [1500/1562], Loss: 0.3106, Avg Loss: 0.4084, Time: 259.29s\n",
            "Epoch [2/5], Batch [1510/1562], Loss: 0.2911, Avg Loss: 0.4077, Time: 260.95s\n",
            "Epoch [2/5], Batch [1520/1562], Loss: 0.2907, Avg Loss: 0.4069, Time: 262.60s\n",
            "Epoch [2/5], Batch [1530/1562], Loss: 0.2871, Avg Loss: 0.4062, Time: 264.24s\n",
            "Epoch [2/5], Batch [1540/1562], Loss: 0.2810, Avg Loss: 0.4055, Time: 265.87s\n",
            "Epoch [2/5], Batch [1550/1562], Loss: 0.2988, Avg Loss: 0.4048, Time: 267.87s\n",
            "Epoch [2/5], Batch [1560/1562], Loss: 0.2999, Avg Loss: 0.4040, Time: 269.69s\n",
            "End of Epoch [2/5], Avg Loss: 0.4039, Total Time: 269.95s\n",
            "Epoch [3/5], Batch [10/1562], Loss: 0.2743, Avg Loss: 0.2809, Time: 1.62s\n",
            "Epoch [3/5], Batch [20/1562], Loss: 0.2644, Avg Loss: 0.2810, Time: 3.28s\n",
            "Epoch [3/5], Batch [30/1562], Loss: 0.2695, Avg Loss: 0.2819, Time: 4.96s\n",
            "Epoch [3/5], Batch [40/1562], Loss: 0.2733, Avg Loss: 0.2811, Time: 6.62s\n",
            "Epoch [3/5], Batch [50/1562], Loss: 0.2701, Avg Loss: 0.2803, Time: 8.28s\n",
            "Epoch [3/5], Batch [60/1562], Loss: 0.2928, Avg Loss: 0.2801, Time: 10.39s\n",
            "Epoch [3/5], Batch [70/1562], Loss: 0.2703, Avg Loss: 0.2796, Time: 12.09s\n",
            "Epoch [3/5], Batch [80/1562], Loss: 0.2722, Avg Loss: 0.2797, Time: 13.74s\n",
            "Epoch [3/5], Batch [90/1562], Loss: 0.2846, Avg Loss: 0.2792, Time: 15.36s\n",
            "Epoch [3/5], Batch [100/1562], Loss: 0.2789, Avg Loss: 0.2792, Time: 17.01s\n",
            "Epoch [3/5], Batch [110/1562], Loss: 0.2962, Avg Loss: 0.2793, Time: 18.65s\n",
            "Epoch [3/5], Batch [120/1562], Loss: 0.2746, Avg Loss: 0.2791, Time: 20.32s\n",
            "Epoch [3/5], Batch [130/1562], Loss: 0.2453, Avg Loss: 0.2786, Time: 22.47s\n",
            "Epoch [3/5], Batch [140/1562], Loss: 0.2746, Avg Loss: 0.2785, Time: 24.14s\n",
            "Epoch [3/5], Batch [150/1562], Loss: 0.2573, Avg Loss: 0.2780, Time: 25.82s\n",
            "Epoch [3/5], Batch [160/1562], Loss: 0.2669, Avg Loss: 0.2775, Time: 27.45s\n",
            "Epoch [3/5], Batch [170/1562], Loss: 0.2824, Avg Loss: 0.2773, Time: 29.10s\n",
            "Epoch [3/5], Batch [180/1562], Loss: 0.2799, Avg Loss: 0.2772, Time: 30.72s\n",
            "Epoch [3/5], Batch [190/1562], Loss: 0.2794, Avg Loss: 0.2770, Time: 32.33s\n",
            "Epoch [3/5], Batch [200/1562], Loss: 0.2839, Avg Loss: 0.2768, Time: 34.44s\n",
            "Epoch [3/5], Batch [210/1562], Loss: 0.2587, Avg Loss: 0.2766, Time: 36.11s\n",
            "Epoch [3/5], Batch [220/1562], Loss: 0.2713, Avg Loss: 0.2764, Time: 37.80s\n",
            "Epoch [3/5], Batch [230/1562], Loss: 0.2793, Avg Loss: 0.2764, Time: 39.49s\n",
            "Epoch [3/5], Batch [240/1562], Loss: 0.2620, Avg Loss: 0.2761, Time: 41.16s\n",
            "Epoch [3/5], Batch [250/1562], Loss: 0.2766, Avg Loss: 0.2758, Time: 42.79s\n",
            "Epoch [3/5], Batch [260/1562], Loss: 0.2744, Avg Loss: 0.2756, Time: 44.46s\n",
            "Epoch [3/5], Batch [270/1562], Loss: 0.2573, Avg Loss: 0.2755, Time: 46.57s\n",
            "Epoch [3/5], Batch [280/1562], Loss: 0.2689, Avg Loss: 0.2754, Time: 48.21s\n",
            "Epoch [3/5], Batch [290/1562], Loss: 0.2724, Avg Loss: 0.2752, Time: 49.83s\n",
            "Epoch [3/5], Batch [300/1562], Loss: 0.2792, Avg Loss: 0.2750, Time: 51.53s\n",
            "Epoch [3/5], Batch [310/1562], Loss: 0.2495, Avg Loss: 0.2747, Time: 53.25s\n",
            "Epoch [3/5], Batch [320/1562], Loss: 0.2653, Avg Loss: 0.2744, Time: 54.94s\n",
            "Epoch [3/5], Batch [330/1562], Loss: 0.2542, Avg Loss: 0.2742, Time: 56.73s\n",
            "Epoch [3/5], Batch [340/1562], Loss: 0.2733, Avg Loss: 0.2738, Time: 58.77s\n",
            "Epoch [3/5], Batch [350/1562], Loss: 0.2569, Avg Loss: 0.2736, Time: 60.41s\n",
            "Epoch [3/5], Batch [360/1562], Loss: 0.2504, Avg Loss: 0.2733, Time: 62.05s\n",
            "Epoch [3/5], Batch [370/1562], Loss: 0.2674, Avg Loss: 0.2731, Time: 63.68s\n",
            "Epoch [3/5], Batch [380/1562], Loss: 0.2651, Avg Loss: 0.2729, Time: 65.35s\n",
            "Epoch [3/5], Batch [390/1562], Loss: 0.2838, Avg Loss: 0.2727, Time: 67.03s\n",
            "Epoch [3/5], Batch [400/1562], Loss: 0.2688, Avg Loss: 0.2726, Time: 68.87s\n",
            "Epoch [3/5], Batch [410/1562], Loss: 0.2741, Avg Loss: 0.2724, Time: 70.86s\n",
            "Epoch [3/5], Batch [420/1562], Loss: 0.2555, Avg Loss: 0.2724, Time: 72.50s\n",
            "Epoch [3/5], Batch [430/1562], Loss: 0.2716, Avg Loss: 0.2723, Time: 74.14s\n",
            "Epoch [3/5], Batch [440/1562], Loss: 0.2596, Avg Loss: 0.2722, Time: 75.77s\n",
            "Epoch [3/5], Batch [450/1562], Loss: 0.2656, Avg Loss: 0.2721, Time: 77.42s\n",
            "Epoch [3/5], Batch [460/1562], Loss: 0.2748, Avg Loss: 0.2720, Time: 79.10s\n",
            "Epoch [3/5], Batch [470/1562], Loss: 0.2744, Avg Loss: 0.2719, Time: 80.96s\n",
            "Epoch [3/5], Batch [480/1562], Loss: 0.2582, Avg Loss: 0.2717, Time: 82.86s\n",
            "Epoch [3/5], Batch [490/1562], Loss: 0.2581, Avg Loss: 0.2715, Time: 84.53s\n",
            "Epoch [3/5], Batch [500/1562], Loss: 0.2573, Avg Loss: 0.2713, Time: 86.20s\n",
            "Epoch [3/5], Batch [510/1562], Loss: 0.2696, Avg Loss: 0.2712, Time: 87.85s\n",
            "Epoch [3/5], Batch [520/1562], Loss: 0.2759, Avg Loss: 0.2711, Time: 89.52s\n",
            "Epoch [3/5], Batch [530/1562], Loss: 0.2740, Avg Loss: 0.2709, Time: 91.15s\n",
            "Epoch [3/5], Batch [540/1562], Loss: 0.2550, Avg Loss: 0.2707, Time: 93.06s\n",
            "Epoch [3/5], Batch [550/1562], Loss: 0.2490, Avg Loss: 0.2704, Time: 95.00s\n",
            "Epoch [3/5], Batch [560/1562], Loss: 0.2374, Avg Loss: 0.2702, Time: 96.61s\n",
            "Epoch [3/5], Batch [570/1562], Loss: 0.2563, Avg Loss: 0.2701, Time: 98.22s\n",
            "Epoch [3/5], Batch [580/1562], Loss: 0.2557, Avg Loss: 0.2699, Time: 99.86s\n",
            "Epoch [3/5], Batch [590/1562], Loss: 0.2594, Avg Loss: 0.2696, Time: 101.56s\n",
            "Epoch [3/5], Batch [600/1562], Loss: 0.2459, Avg Loss: 0.2695, Time: 103.24s\n",
            "Epoch [3/5], Batch [610/1562], Loss: 0.2430, Avg Loss: 0.2693, Time: 105.23s\n",
            "Epoch [3/5], Batch [620/1562], Loss: 0.2560, Avg Loss: 0.2691, Time: 107.16s\n",
            "Epoch [3/5], Batch [630/1562], Loss: 0.2530, Avg Loss: 0.2689, Time: 108.85s\n",
            "Epoch [3/5], Batch [640/1562], Loss: 0.2606, Avg Loss: 0.2687, Time: 110.50s\n",
            "Epoch [3/5], Batch [650/1562], Loss: 0.2684, Avg Loss: 0.2685, Time: 112.17s\n",
            "Epoch [3/5], Batch [660/1562], Loss: 0.2494, Avg Loss: 0.2684, Time: 113.84s\n",
            "Epoch [3/5], Batch [670/1562], Loss: 0.2635, Avg Loss: 0.2682, Time: 115.50s\n",
            "Epoch [3/5], Batch [680/1562], Loss: 0.2599, Avg Loss: 0.2681, Time: 117.57s\n",
            "Epoch [3/5], Batch [690/1562], Loss: 0.2455, Avg Loss: 0.2680, Time: 119.40s\n",
            "Epoch [3/5], Batch [700/1562], Loss: 0.2455, Avg Loss: 0.2678, Time: 121.13s\n",
            "Epoch [3/5], Batch [710/1562], Loss: 0.2546, Avg Loss: 0.2677, Time: 122.80s\n",
            "Epoch [3/5], Batch [720/1562], Loss: 0.2514, Avg Loss: 0.2676, Time: 124.46s\n",
            "Epoch [3/5], Batch [730/1562], Loss: 0.2657, Avg Loss: 0.2674, Time: 126.13s\n",
            "Epoch [3/5], Batch [740/1562], Loss: 0.2517, Avg Loss: 0.2673, Time: 127.79s\n",
            "Epoch [3/5], Batch [750/1562], Loss: 0.2531, Avg Loss: 0.2671, Time: 129.90s\n",
            "Epoch [3/5], Batch [760/1562], Loss: 0.2628, Avg Loss: 0.2669, Time: 131.62s\n",
            "Epoch [3/5], Batch [770/1562], Loss: 0.2542, Avg Loss: 0.2668, Time: 133.37s\n",
            "Epoch [3/5], Batch [780/1562], Loss: 0.2604, Avg Loss: 0.2666, Time: 135.10s\n",
            "Epoch [3/5], Batch [790/1562], Loss: 0.2504, Avg Loss: 0.2665, Time: 136.79s\n",
            "Epoch [3/5], Batch [800/1562], Loss: 0.2618, Avg Loss: 0.2663, Time: 138.48s\n",
            "Epoch [3/5], Batch [810/1562], Loss: 0.2486, Avg Loss: 0.2662, Time: 140.17s\n",
            "Epoch [3/5], Batch [820/1562], Loss: 0.2485, Avg Loss: 0.2660, Time: 142.27s\n",
            "Epoch [3/5], Batch [830/1562], Loss: 0.2574, Avg Loss: 0.2657, Time: 143.95s\n",
            "Epoch [3/5], Batch [840/1562], Loss: 0.2672, Avg Loss: 0.2656, Time: 145.59s\n",
            "Epoch [3/5], Batch [850/1562], Loss: 0.2621, Avg Loss: 0.2655, Time: 147.29s\n",
            "Epoch [3/5], Batch [860/1562], Loss: 0.2409, Avg Loss: 0.2653, Time: 148.99s\n",
            "Epoch [3/5], Batch [870/1562], Loss: 0.2505, Avg Loss: 0.2652, Time: 150.69s\n",
            "Epoch [3/5], Batch [880/1562], Loss: 0.2476, Avg Loss: 0.2651, Time: 152.51s\n",
            "Epoch [3/5], Batch [890/1562], Loss: 0.2529, Avg Loss: 0.2650, Time: 154.58s\n",
            "Epoch [3/5], Batch [900/1562], Loss: 0.2566, Avg Loss: 0.2649, Time: 156.26s\n",
            "Epoch [3/5], Batch [910/1562], Loss: 0.2567, Avg Loss: 0.2647, Time: 157.89s\n",
            "Epoch [3/5], Batch [920/1562], Loss: 0.2624, Avg Loss: 0.2646, Time: 159.58s\n",
            "Epoch [3/5], Batch [930/1562], Loss: 0.2645, Avg Loss: 0.2646, Time: 161.29s\n",
            "Epoch [3/5], Batch [940/1562], Loss: 0.2478, Avg Loss: 0.2645, Time: 163.00s\n",
            "Epoch [3/5], Batch [950/1562], Loss: 0.2424, Avg Loss: 0.2644, Time: 164.91s\n",
            "Epoch [3/5], Batch [960/1562], Loss: 0.2549, Avg Loss: 0.2643, Time: 166.85s\n",
            "Epoch [3/5], Batch [970/1562], Loss: 0.2474, Avg Loss: 0.2641, Time: 168.54s\n",
            "Epoch [3/5], Batch [980/1562], Loss: 0.2527, Avg Loss: 0.2640, Time: 170.21s\n",
            "Epoch [3/5], Batch [990/1562], Loss: 0.2495, Avg Loss: 0.2639, Time: 171.89s\n",
            "Epoch [3/5], Batch [1000/1562], Loss: 0.2555, Avg Loss: 0.2638, Time: 173.58s\n",
            "Epoch [3/5], Batch [1010/1562], Loss: 0.2404, Avg Loss: 0.2636, Time: 175.27s\n",
            "Epoch [3/5], Batch [1020/1562], Loss: 0.2440, Avg Loss: 0.2635, Time: 177.34s\n",
            "Epoch [3/5], Batch [1030/1562], Loss: 0.2432, Avg Loss: 0.2634, Time: 179.13s\n",
            "Epoch [3/5], Batch [1040/1562], Loss: 0.2469, Avg Loss: 0.2632, Time: 180.79s\n",
            "Epoch [3/5], Batch [1050/1562], Loss: 0.2505, Avg Loss: 0.2631, Time: 182.49s\n",
            "Epoch [3/5], Batch [1060/1562], Loss: 0.2483, Avg Loss: 0.2630, Time: 184.18s\n",
            "Epoch [3/5], Batch [1070/1562], Loss: 0.2487, Avg Loss: 0.2628, Time: 185.87s\n",
            "Epoch [3/5], Batch [1080/1562], Loss: 0.2420, Avg Loss: 0.2627, Time: 187.60s\n",
            "Epoch [3/5], Batch [1090/1562], Loss: 0.2388, Avg Loss: 0.2626, Time: 189.76s\n",
            "Epoch [3/5], Batch [1100/1562], Loss: 0.2524, Avg Loss: 0.2624, Time: 191.46s\n",
            "Epoch [3/5], Batch [1110/1562], Loss: 0.2379, Avg Loss: 0.2623, Time: 193.12s\n",
            "Epoch [3/5], Batch [1120/1562], Loss: 0.2383, Avg Loss: 0.2622, Time: 194.76s\n",
            "Epoch [3/5], Batch [1130/1562], Loss: 0.2409, Avg Loss: 0.2620, Time: 196.42s\n",
            "Epoch [3/5], Batch [1140/1562], Loss: 0.2512, Avg Loss: 0.2619, Time: 198.10s\n",
            "Epoch [3/5], Batch [1150/1562], Loss: 0.2404, Avg Loss: 0.2617, Time: 199.81s\n",
            "Epoch [3/5], Batch [1160/1562], Loss: 0.2449, Avg Loss: 0.2616, Time: 202.06s\n",
            "Epoch [3/5], Batch [1170/1562], Loss: 0.2636, Avg Loss: 0.2615, Time: 203.78s\n",
            "Epoch [3/5], Batch [1180/1562], Loss: 0.2529, Avg Loss: 0.2614, Time: 205.45s\n",
            "Epoch [3/5], Batch [1190/1562], Loss: 0.2527, Avg Loss: 0.2613, Time: 207.11s\n",
            "Epoch [3/5], Batch [1200/1562], Loss: 0.2350, Avg Loss: 0.2611, Time: 208.79s\n",
            "Epoch [3/5], Batch [1210/1562], Loss: 0.2332, Avg Loss: 0.2610, Time: 210.46s\n",
            "Epoch [3/5], Batch [1220/1562], Loss: 0.2441, Avg Loss: 0.2609, Time: 212.18s\n",
            "Epoch [3/5], Batch [1230/1562], Loss: 0.2529, Avg Loss: 0.2607, Time: 214.32s\n",
            "Epoch [3/5], Batch [1240/1562], Loss: 0.2508, Avg Loss: 0.2606, Time: 216.06s\n",
            "Epoch [3/5], Batch [1250/1562], Loss: 0.2536, Avg Loss: 0.2606, Time: 217.77s\n",
            "Epoch [3/5], Batch [1260/1562], Loss: 0.2459, Avg Loss: 0.2604, Time: 219.45s\n",
            "Epoch [3/5], Batch [1270/1562], Loss: 0.2415, Avg Loss: 0.2603, Time: 221.12s\n",
            "Epoch [3/5], Batch [1280/1562], Loss: 0.2587, Avg Loss: 0.2601, Time: 222.78s\n",
            "Epoch [3/5], Batch [1290/1562], Loss: 0.2385, Avg Loss: 0.2600, Time: 224.64s\n",
            "Epoch [3/5], Batch [1300/1562], Loss: 0.2365, Avg Loss: 0.2599, Time: 226.62s\n",
            "Epoch [3/5], Batch [1310/1562], Loss: 0.2264, Avg Loss: 0.2597, Time: 228.30s\n",
            "Epoch [3/5], Batch [1320/1562], Loss: 0.2507, Avg Loss: 0.2596, Time: 229.99s\n",
            "Epoch [3/5], Batch [1330/1562], Loss: 0.2475, Avg Loss: 0.2595, Time: 231.66s\n",
            "Epoch [3/5], Batch [1340/1562], Loss: 0.2410, Avg Loss: 0.2593, Time: 233.33s\n",
            "Epoch [3/5], Batch [1350/1562], Loss: 0.2509, Avg Loss: 0.2592, Time: 235.01s\n",
            "Epoch [3/5], Batch [1360/1562], Loss: 0.2489, Avg Loss: 0.2591, Time: 236.97s\n",
            "Epoch [3/5], Batch [1370/1562], Loss: 0.2468, Avg Loss: 0.2590, Time: 238.86s\n",
            "Epoch [3/5], Batch [1380/1562], Loss: 0.2516, Avg Loss: 0.2589, Time: 240.54s\n",
            "Epoch [3/5], Batch [1390/1562], Loss: 0.2387, Avg Loss: 0.2588, Time: 242.21s\n",
            "Epoch [3/5], Batch [1400/1562], Loss: 0.2422, Avg Loss: 0.2587, Time: 243.90s\n",
            "Epoch [3/5], Batch [1410/1562], Loss: 0.2418, Avg Loss: 0.2586, Time: 245.56s\n",
            "Epoch [3/5], Batch [1420/1562], Loss: 0.2500, Avg Loss: 0.2584, Time: 247.23s\n",
            "Epoch [3/5], Batch [1430/1562], Loss: 0.2429, Avg Loss: 0.2583, Time: 249.32s\n",
            "Epoch [3/5], Batch [1440/1562], Loss: 0.2427, Avg Loss: 0.2582, Time: 251.18s\n",
            "Epoch [3/5], Batch [1450/1562], Loss: 0.2593, Avg Loss: 0.2581, Time: 252.86s\n",
            "Epoch [3/5], Batch [1460/1562], Loss: 0.2450, Avg Loss: 0.2580, Time: 254.52s\n",
            "Epoch [3/5], Batch [1470/1562], Loss: 0.2552, Avg Loss: 0.2579, Time: 256.24s\n",
            "Epoch [3/5], Batch [1480/1562], Loss: 0.2399, Avg Loss: 0.2578, Time: 257.92s\n",
            "Epoch [3/5], Batch [1490/1562], Loss: 0.2406, Avg Loss: 0.2576, Time: 259.61s\n",
            "Epoch [3/5], Batch [1500/1562], Loss: 0.2435, Avg Loss: 0.2575, Time: 261.81s\n",
            "Epoch [3/5], Batch [1510/1562], Loss: 0.2196, Avg Loss: 0.2575, Time: 263.48s\n",
            "Epoch [3/5], Batch [1520/1562], Loss: 0.2435, Avg Loss: 0.2574, Time: 265.17s\n",
            "Epoch [3/5], Batch [1530/1562], Loss: 0.2430, Avg Loss: 0.2573, Time: 266.84s\n",
            "Epoch [3/5], Batch [1540/1562], Loss: 0.2384, Avg Loss: 0.2571, Time: 268.53s\n",
            "Epoch [3/5], Batch [1550/1562], Loss: 0.2506, Avg Loss: 0.2571, Time: 270.25s\n",
            "Epoch [3/5], Batch [1560/1562], Loss: 0.2441, Avg Loss: 0.2570, Time: 272.06s\n",
            "End of Epoch [3/5], Avg Loss: 0.2569, Total Time: 272.42s\n",
            "Epoch [4/5], Batch [10/1562], Loss: 0.2365, Avg Loss: 0.2288, Time: 2.03s\n",
            "Epoch [4/5], Batch [20/1562], Loss: 0.2270, Avg Loss: 0.2287, Time: 3.69s\n",
            "Epoch [4/5], Batch [30/1562], Loss: 0.2243, Avg Loss: 0.2303, Time: 5.33s\n",
            "Epoch [4/5], Batch [40/1562], Loss: 0.2197, Avg Loss: 0.2292, Time: 6.99s\n",
            "Epoch [4/5], Batch [50/1562], Loss: 0.2336, Avg Loss: 0.2283, Time: 8.66s\n",
            "Epoch [4/5], Batch [60/1562], Loss: 0.2210, Avg Loss: 0.2281, Time: 10.38s\n",
            "Epoch [4/5], Batch [70/1562], Loss: 0.2289, Avg Loss: 0.2284, Time: 12.41s\n",
            "Epoch [4/5], Batch [80/1562], Loss: 0.2435, Avg Loss: 0.2286, Time: 14.32s\n",
            "Epoch [4/5], Batch [90/1562], Loss: 0.2252, Avg Loss: 0.2286, Time: 15.96s\n",
            "Epoch [4/5], Batch [100/1562], Loss: 0.2332, Avg Loss: 0.2288, Time: 17.62s\n",
            "Epoch [4/5], Batch [110/1562], Loss: 0.2375, Avg Loss: 0.2290, Time: 19.27s\n",
            "Epoch [4/5], Batch [120/1562], Loss: 0.2287, Avg Loss: 0.2293, Time: 20.92s\n",
            "Epoch [4/5], Batch [130/1562], Loss: 0.2260, Avg Loss: 0.2297, Time: 22.58s\n",
            "Epoch [4/5], Batch [140/1562], Loss: 0.2201, Avg Loss: 0.2299, Time: 24.68s\n",
            "Epoch [4/5], Batch [150/1562], Loss: 0.2419, Avg Loss: 0.2297, Time: 26.57s\n",
            "Epoch [4/5], Batch [160/1562], Loss: 0.2236, Avg Loss: 0.2300, Time: 28.24s\n",
            "Epoch [4/5], Batch [170/1562], Loss: 0.2291, Avg Loss: 0.2300, Time: 29.92s\n",
            "Epoch [4/5], Batch [180/1562], Loss: 0.2315, Avg Loss: 0.2302, Time: 31.61s\n",
            "Epoch [4/5], Batch [190/1562], Loss: 0.2332, Avg Loss: 0.2304, Time: 33.26s\n",
            "Epoch [4/5], Batch [200/1562], Loss: 0.2437, Avg Loss: 0.2303, Time: 34.95s\n",
            "Epoch [4/5], Batch [210/1562], Loss: 0.2270, Avg Loss: 0.2302, Time: 37.06s\n",
            "Epoch [4/5], Batch [220/1562], Loss: 0.2147, Avg Loss: 0.2300, Time: 38.84s\n",
            "Epoch [4/5], Batch [230/1562], Loss: 0.2231, Avg Loss: 0.2298, Time: 40.52s\n",
            "Epoch [4/5], Batch [240/1562], Loss: 0.2206, Avg Loss: 0.2298, Time: 42.20s\n",
            "Epoch [4/5], Batch [250/1562], Loss: 0.2229, Avg Loss: 0.2298, Time: 43.90s\n",
            "Epoch [4/5], Batch [260/1562], Loss: 0.2309, Avg Loss: 0.2299, Time: 45.58s\n",
            "Epoch [4/5], Batch [270/1562], Loss: 0.2328, Avg Loss: 0.2299, Time: 47.29s\n",
            "Epoch [4/5], Batch [280/1562], Loss: 0.2230, Avg Loss: 0.2298, Time: 49.42s\n",
            "Epoch [4/5], Batch [290/1562], Loss: 0.2317, Avg Loss: 0.2297, Time: 51.12s\n",
            "Epoch [4/5], Batch [300/1562], Loss: 0.2249, Avg Loss: 0.2296, Time: 52.80s\n",
            "Epoch [4/5], Batch [310/1562], Loss: 0.2328, Avg Loss: 0.2296, Time: 54.49s\n",
            "Epoch [4/5], Batch [320/1562], Loss: 0.2327, Avg Loss: 0.2296, Time: 56.13s\n",
            "Epoch [4/5], Batch [330/1562], Loss: 0.2276, Avg Loss: 0.2295, Time: 57.83s\n",
            "Epoch [4/5], Batch [340/1562], Loss: 0.2296, Avg Loss: 0.2295, Time: 59.63s\n",
            "Epoch [4/5], Batch [350/1562], Loss: 0.2285, Avg Loss: 0.2294, Time: 61.74s\n",
            "Epoch [4/5], Batch [360/1562], Loss: 0.2375, Avg Loss: 0.2294, Time: 63.42s\n",
            "Epoch [4/5], Batch [370/1562], Loss: 0.2326, Avg Loss: 0.2293, Time: 65.10s\n",
            "Epoch [4/5], Batch [380/1562], Loss: 0.2327, Avg Loss: 0.2292, Time: 66.81s\n",
            "Epoch [4/5], Batch [390/1562], Loss: 0.2390, Avg Loss: 0.2293, Time: 68.44s\n",
            "Epoch [4/5], Batch [400/1562], Loss: 0.2220, Avg Loss: 0.2291, Time: 70.10s\n",
            "Epoch [4/5], Batch [410/1562], Loss: 0.2415, Avg Loss: 0.2292, Time: 71.94s\n",
            "Epoch [4/5], Batch [420/1562], Loss: 0.2294, Avg Loss: 0.2291, Time: 73.96s\n",
            "Epoch [4/5], Batch [430/1562], Loss: 0.2308, Avg Loss: 0.2291, Time: 75.63s\n",
            "Epoch [4/5], Batch [440/1562], Loss: 0.2361, Avg Loss: 0.2291, Time: 77.34s\n",
            "Epoch [4/5], Batch [450/1562], Loss: 0.2318, Avg Loss: 0.2291, Time: 79.06s\n",
            "Epoch [4/5], Batch [460/1562], Loss: 0.2344, Avg Loss: 0.2291, Time: 80.74s\n",
            "Epoch [4/5], Batch [470/1562], Loss: 0.2258, Avg Loss: 0.2291, Time: 82.43s\n",
            "Epoch [4/5], Batch [480/1562], Loss: 0.2292, Avg Loss: 0.2291, Time: 84.37s\n",
            "Epoch [4/5], Batch [490/1562], Loss: 0.2306, Avg Loss: 0.2291, Time: 86.24s\n",
            "Epoch [4/5], Batch [500/1562], Loss: 0.2294, Avg Loss: 0.2290, Time: 87.90s\n",
            "Epoch [4/5], Batch [510/1562], Loss: 0.2215, Avg Loss: 0.2290, Time: 89.57s\n",
            "Epoch [4/5], Batch [520/1562], Loss: 0.2265, Avg Loss: 0.2290, Time: 91.29s\n",
            "Epoch [4/5], Batch [530/1562], Loss: 0.2279, Avg Loss: 0.2290, Time: 93.05s\n",
            "Epoch [4/5], Batch [540/1562], Loss: 0.2357, Avg Loss: 0.2289, Time: 94.76s\n",
            "Epoch [4/5], Batch [550/1562], Loss: 0.2252, Avg Loss: 0.2289, Time: 96.84s\n",
            "Epoch [4/5], Batch [560/1562], Loss: 0.2138, Avg Loss: 0.2289, Time: 98.57s\n",
            "Epoch [4/5], Batch [570/1562], Loss: 0.2299, Avg Loss: 0.2290, Time: 100.23s\n",
            "Epoch [4/5], Batch [580/1562], Loss: 0.2266, Avg Loss: 0.2289, Time: 101.87s\n",
            "Epoch [4/5], Batch [590/1562], Loss: 0.2279, Avg Loss: 0.2289, Time: 103.55s\n",
            "Epoch [4/5], Batch [600/1562], Loss: 0.2270, Avg Loss: 0.2288, Time: 105.27s\n",
            "Epoch [4/5], Batch [610/1562], Loss: 0.2202, Avg Loss: 0.2288, Time: 107.01s\n",
            "Epoch [4/5], Batch [620/1562], Loss: 0.2182, Avg Loss: 0.2288, Time: 109.22s\n",
            "Epoch [4/5], Batch [630/1562], Loss: 0.2271, Avg Loss: 0.2287, Time: 110.89s\n",
            "Epoch [4/5], Batch [640/1562], Loss: 0.2358, Avg Loss: 0.2287, Time: 112.56s\n",
            "Epoch [4/5], Batch [650/1562], Loss: 0.2188, Avg Loss: 0.2286, Time: 114.21s\n",
            "Epoch [4/5], Batch [660/1562], Loss: 0.2209, Avg Loss: 0.2287, Time: 115.87s\n",
            "Epoch [4/5], Batch [670/1562], Loss: 0.2208, Avg Loss: 0.2286, Time: 117.52s\n",
            "Epoch [4/5], Batch [680/1562], Loss: 0.2271, Avg Loss: 0.2286, Time: 119.26s\n",
            "Epoch [4/5], Batch [690/1562], Loss: 0.2324, Avg Loss: 0.2286, Time: 121.44s\n",
            "Epoch [4/5], Batch [700/1562], Loss: 0.2277, Avg Loss: 0.2285, Time: 123.15s\n",
            "Epoch [4/5], Batch [710/1562], Loss: 0.2305, Avg Loss: 0.2284, Time: 124.84s\n",
            "Epoch [4/5], Batch [720/1562], Loss: 0.2197, Avg Loss: 0.2284, Time: 126.52s\n",
            "Epoch [4/5], Batch [730/1562], Loss: 0.2234, Avg Loss: 0.2283, Time: 128.20s\n",
            "Epoch [4/5], Batch [740/1562], Loss: 0.2306, Avg Loss: 0.2283, Time: 129.84s\n",
            "Epoch [4/5], Batch [750/1562], Loss: 0.2218, Avg Loss: 0.2283, Time: 131.67s\n",
            "Epoch [4/5], Batch [760/1562], Loss: 0.2171, Avg Loss: 0.2282, Time: 133.78s\n",
            "Epoch [4/5], Batch [770/1562], Loss: 0.2278, Avg Loss: 0.2282, Time: 135.47s\n",
            "Epoch [4/5], Batch [780/1562], Loss: 0.2242, Avg Loss: 0.2282, Time: 137.11s\n",
            "Epoch [4/5], Batch [790/1562], Loss: 0.2322, Avg Loss: 0.2282, Time: 138.80s\n",
            "Epoch [4/5], Batch [800/1562], Loss: 0.2243, Avg Loss: 0.2282, Time: 140.48s\n",
            "Epoch [4/5], Batch [810/1562], Loss: 0.2355, Avg Loss: 0.2282, Time: 142.15s\n",
            "Epoch [4/5], Batch [820/1562], Loss: 0.2263, Avg Loss: 0.2283, Time: 144.09s\n",
            "Epoch [4/5], Batch [830/1562], Loss: 0.2376, Avg Loss: 0.2283, Time: 146.09s\n",
            "Epoch [4/5], Batch [840/1562], Loss: 0.2290, Avg Loss: 0.2282, Time: 147.78s\n",
            "Epoch [4/5], Batch [850/1562], Loss: 0.2299, Avg Loss: 0.2282, Time: 149.45s\n",
            "Epoch [4/5], Batch [860/1562], Loss: 0.2333, Avg Loss: 0.2281, Time: 151.10s\n",
            "Epoch [4/5], Batch [870/1562], Loss: 0.2264, Avg Loss: 0.2281, Time: 152.74s\n",
            "Epoch [4/5], Batch [880/1562], Loss: 0.2258, Avg Loss: 0.2281, Time: 154.39s\n",
            "Epoch [4/5], Batch [890/1562], Loss: 0.2262, Avg Loss: 0.2280, Time: 156.40s\n",
            "Epoch [4/5], Batch [900/1562], Loss: 0.2235, Avg Loss: 0.2279, Time: 158.27s\n",
            "Epoch [4/5], Batch [910/1562], Loss: 0.2224, Avg Loss: 0.2279, Time: 159.99s\n",
            "Epoch [4/5], Batch [920/1562], Loss: 0.2188, Avg Loss: 0.2279, Time: 161.69s\n",
            "Epoch [4/5], Batch [930/1562], Loss: 0.2228, Avg Loss: 0.2279, Time: 163.37s\n",
            "Epoch [4/5], Batch [940/1562], Loss: 0.2133, Avg Loss: 0.2278, Time: 165.02s\n",
            "Epoch [4/5], Batch [950/1562], Loss: 0.2220, Avg Loss: 0.2278, Time: 166.70s\n",
            "Epoch [4/5], Batch [960/1562], Loss: 0.2249, Avg Loss: 0.2277, Time: 168.78s\n",
            "Epoch [4/5], Batch [970/1562], Loss: 0.2207, Avg Loss: 0.2277, Time: 170.55s\n",
            "Epoch [4/5], Batch [980/1562], Loss: 0.2109, Avg Loss: 0.2277, Time: 172.22s\n",
            "Epoch [4/5], Batch [990/1562], Loss: 0.2260, Avg Loss: 0.2276, Time: 173.94s\n",
            "Epoch [4/5], Batch [1000/1562], Loss: 0.2067, Avg Loss: 0.2276, Time: 175.65s\n",
            "Epoch [4/5], Batch [1010/1562], Loss: 0.2248, Avg Loss: 0.2276, Time: 177.33s\n",
            "Epoch [4/5], Batch [1020/1562], Loss: 0.2292, Avg Loss: 0.2276, Time: 179.00s\n",
            "Epoch [4/5], Batch [1030/1562], Loss: 0.2349, Avg Loss: 0.2276, Time: 181.19s\n",
            "Epoch [4/5], Batch [1040/1562], Loss: 0.2255, Avg Loss: 0.2275, Time: 182.89s\n",
            "Epoch [4/5], Batch [1050/1562], Loss: 0.2300, Avg Loss: 0.2275, Time: 184.55s\n",
            "Epoch [4/5], Batch [1060/1562], Loss: 0.2315, Avg Loss: 0.2275, Time: 186.29s\n",
            "Epoch [4/5], Batch [1070/1562], Loss: 0.2143, Avg Loss: 0.2274, Time: 187.98s\n",
            "Epoch [4/5], Batch [1080/1562], Loss: 0.2282, Avg Loss: 0.2274, Time: 189.69s\n",
            "Epoch [4/5], Batch [1090/1562], Loss: 0.2196, Avg Loss: 0.2274, Time: 191.45s\n",
            "Epoch [4/5], Batch [1100/1562], Loss: 0.2291, Avg Loss: 0.2273, Time: 193.56s\n",
            "Epoch [4/5], Batch [1110/1562], Loss: 0.2200, Avg Loss: 0.2272, Time: 195.23s\n",
            "Epoch [4/5], Batch [1120/1562], Loss: 0.2279, Avg Loss: 0.2272, Time: 196.87s\n",
            "Epoch [4/5], Batch [1130/1562], Loss: 0.2167, Avg Loss: 0.2272, Time: 198.54s\n",
            "Epoch [4/5], Batch [1140/1562], Loss: 0.2278, Avg Loss: 0.2271, Time: 200.19s\n",
            "Epoch [4/5], Batch [1150/1562], Loss: 0.2363, Avg Loss: 0.2271, Time: 201.89s\n",
            "Epoch [4/5], Batch [1160/1562], Loss: 0.2240, Avg Loss: 0.2271, Time: 203.74s\n",
            "Epoch [4/5], Batch [1170/1562], Loss: 0.2377, Avg Loss: 0.2271, Time: 205.78s\n",
            "Epoch [4/5], Batch [1180/1562], Loss: 0.2198, Avg Loss: 0.2271, Time: 207.46s\n",
            "Epoch [4/5], Batch [1190/1562], Loss: 0.2278, Avg Loss: 0.2271, Time: 209.15s\n",
            "Epoch [4/5], Batch [1200/1562], Loss: 0.2222, Avg Loss: 0.2271, Time: 210.82s\n",
            "Epoch [4/5], Batch [1210/1562], Loss: 0.2314, Avg Loss: 0.2271, Time: 212.48s\n",
            "Epoch [4/5], Batch [1220/1562], Loss: 0.2250, Avg Loss: 0.2271, Time: 214.15s\n",
            "Epoch [4/5], Batch [1230/1562], Loss: 0.2153, Avg Loss: 0.2271, Time: 216.12s\n",
            "Epoch [4/5], Batch [1240/1562], Loss: 0.2217, Avg Loss: 0.2270, Time: 218.06s\n",
            "Epoch [4/5], Batch [1250/1562], Loss: 0.2338, Avg Loss: 0.2270, Time: 219.72s\n",
            "Epoch [4/5], Batch [1260/1562], Loss: 0.2165, Avg Loss: 0.2270, Time: 221.38s\n",
            "Epoch [4/5], Batch [1270/1562], Loss: 0.2227, Avg Loss: 0.2270, Time: 223.07s\n",
            "Epoch [4/5], Batch [1280/1562], Loss: 0.2236, Avg Loss: 0.2269, Time: 224.74s\n",
            "Epoch [4/5], Batch [1290/1562], Loss: 0.2238, Avg Loss: 0.2269, Time: 226.43s\n",
            "Epoch [4/5], Batch [1300/1562], Loss: 0.2214, Avg Loss: 0.2269, Time: 228.46s\n",
            "Epoch [4/5], Batch [1310/1562], Loss: 0.2235, Avg Loss: 0.2268, Time: 230.34s\n",
            "Epoch [4/5], Batch [1320/1562], Loss: 0.2253, Avg Loss: 0.2268, Time: 231.98s\n",
            "Epoch [4/5], Batch [1330/1562], Loss: 0.2143, Avg Loss: 0.2267, Time: 233.64s\n",
            "Epoch [4/5], Batch [1340/1562], Loss: 0.2305, Avg Loss: 0.2267, Time: 235.31s\n",
            "Epoch [4/5], Batch [1350/1562], Loss: 0.2258, Avg Loss: 0.2267, Time: 236.97s\n",
            "Epoch [4/5], Batch [1360/1562], Loss: 0.2232, Avg Loss: 0.2267, Time: 238.65s\n",
            "Epoch [4/5], Batch [1370/1562], Loss: 0.2324, Avg Loss: 0.2266, Time: 240.78s\n",
            "Epoch [4/5], Batch [1380/1562], Loss: 0.2319, Avg Loss: 0.2266, Time: 242.63s\n",
            "Epoch [4/5], Batch [1390/1562], Loss: 0.2274, Avg Loss: 0.2266, Time: 244.31s\n",
            "Epoch [4/5], Batch [1400/1562], Loss: 0.2183, Avg Loss: 0.2266, Time: 246.01s\n",
            "Epoch [4/5], Batch [1410/1562], Loss: 0.2248, Avg Loss: 0.2266, Time: 247.71s\n",
            "Epoch [4/5], Batch [1420/1562], Loss: 0.2197, Avg Loss: 0.2265, Time: 249.41s\n",
            "Epoch [4/5], Batch [1430/1562], Loss: 0.2217, Avg Loss: 0.2265, Time: 251.11s\n",
            "Epoch [4/5], Batch [1440/1562], Loss: 0.2228, Avg Loss: 0.2265, Time: 253.29s\n",
            "Epoch [4/5], Batch [1450/1562], Loss: 0.2143, Avg Loss: 0.2264, Time: 254.99s\n",
            "Epoch [4/5], Batch [1460/1562], Loss: 0.2311, Avg Loss: 0.2264, Time: 256.72s\n",
            "Epoch [4/5], Batch [1470/1562], Loss: 0.2191, Avg Loss: 0.2264, Time: 258.44s\n",
            "Epoch [4/5], Batch [1480/1562], Loss: 0.2211, Avg Loss: 0.2264, Time: 260.10s\n",
            "Epoch [4/5], Batch [1490/1562], Loss: 0.2359, Avg Loss: 0.2264, Time: 261.77s\n",
            "Epoch [4/5], Batch [1500/1562], Loss: 0.2118, Avg Loss: 0.2263, Time: 263.53s\n",
            "Epoch [4/5], Batch [1510/1562], Loss: 0.2130, Avg Loss: 0.2263, Time: 265.66s\n",
            "Epoch [4/5], Batch [1520/1562], Loss: 0.2163, Avg Loss: 0.2262, Time: 267.35s\n",
            "Epoch [4/5], Batch [1530/1562], Loss: 0.2242, Avg Loss: 0.2262, Time: 269.07s\n",
            "Epoch [4/5], Batch [1540/1562], Loss: 0.2272, Avg Loss: 0.2262, Time: 270.78s\n",
            "Epoch [4/5], Batch [1550/1562], Loss: 0.2242, Avg Loss: 0.2262, Time: 272.50s\n",
            "Epoch [4/5], Batch [1560/1562], Loss: 0.2139, Avg Loss: 0.2261, Time: 274.19s\n",
            "End of Epoch [4/5], Avg Loss: 0.2261, Total Time: 274.46s\n",
            "Epoch [5/5], Batch [10/1562], Loss: 0.1991, Avg Loss: 0.2117, Time: 2.05s\n",
            "Epoch [5/5], Batch [20/1562], Loss: 0.2110, Avg Loss: 0.2116, Time: 3.89s\n",
            "Epoch [5/5], Batch [30/1562], Loss: 0.2156, Avg Loss: 0.2129, Time: 5.53s\n",
            "Epoch [5/5], Batch [40/1562], Loss: 0.2137, Avg Loss: 0.2123, Time: 7.19s\n",
            "Epoch [5/5], Batch [50/1562], Loss: 0.2111, Avg Loss: 0.2120, Time: 8.88s\n",
            "Epoch [5/5], Batch [60/1562], Loss: 0.2220, Avg Loss: 0.2121, Time: 10.55s\n",
            "Epoch [5/5], Batch [70/1562], Loss: 0.2176, Avg Loss: 0.2117, Time: 12.26s\n",
            "Epoch [5/5], Batch [80/1562], Loss: 0.2099, Avg Loss: 0.2117, Time: 14.40s\n",
            "Epoch [5/5], Batch [90/1562], Loss: 0.2074, Avg Loss: 0.2112, Time: 16.16s\n",
            "Epoch [5/5], Batch [100/1562], Loss: 0.2062, Avg Loss: 0.2113, Time: 17.83s\n",
            "Epoch [5/5], Batch [110/1562], Loss: 0.2058, Avg Loss: 0.2113, Time: 19.50s\n",
            "Epoch [5/5], Batch [120/1562], Loss: 0.2170, Avg Loss: 0.2111, Time: 21.17s\n",
            "Epoch [5/5], Batch [130/1562], Loss: 0.2063, Avg Loss: 0.2110, Time: 22.86s\n",
            "Epoch [5/5], Batch [140/1562], Loss: 0.2160, Avg Loss: 0.2111, Time: 24.55s\n",
            "Epoch [5/5], Batch [150/1562], Loss: 0.2240, Avg Loss: 0.2110, Time: 26.74s\n",
            "Epoch [5/5], Batch [160/1562], Loss: 0.2075, Avg Loss: 0.2110, Time: 28.44s\n",
            "Epoch [5/5], Batch [170/1562], Loss: 0.2116, Avg Loss: 0.2110, Time: 30.12s\n",
            "Epoch [5/5], Batch [180/1562], Loss: 0.1944, Avg Loss: 0.2107, Time: 31.81s\n",
            "Epoch [5/5], Batch [190/1562], Loss: 0.2228, Avg Loss: 0.2107, Time: 33.48s\n",
            "Epoch [5/5], Batch [200/1562], Loss: 0.2168, Avg Loss: 0.2107, Time: 35.17s\n",
            "Epoch [5/5], Batch [210/1562], Loss: 0.1999, Avg Loss: 0.2109, Time: 36.92s\n",
            "Epoch [5/5], Batch [220/1562], Loss: 0.2068, Avg Loss: 0.2110, Time: 39.10s\n",
            "Epoch [5/5], Batch [230/1562], Loss: 0.2196, Avg Loss: 0.2111, Time: 40.74s\n",
            "Epoch [5/5], Batch [240/1562], Loss: 0.2126, Avg Loss: 0.2109, Time: 42.41s\n",
            "Epoch [5/5], Batch [250/1562], Loss: 0.2117, Avg Loss: 0.2110, Time: 44.06s\n",
            "Epoch [5/5], Batch [260/1562], Loss: 0.2081, Avg Loss: 0.2109, Time: 45.76s\n",
            "Epoch [5/5], Batch [270/1562], Loss: 0.2222, Avg Loss: 0.2111, Time: 47.44s\n",
            "Epoch [5/5], Batch [280/1562], Loss: 0.2181, Avg Loss: 0.2112, Time: 49.31s\n",
            "Epoch [5/5], Batch [290/1562], Loss: 0.2141, Avg Loss: 0.2114, Time: 51.43s\n",
            "Epoch [5/5], Batch [300/1562], Loss: 0.2049, Avg Loss: 0.2115, Time: 53.09s\n",
            "Epoch [5/5], Batch [310/1562], Loss: 0.2101, Avg Loss: 0.2114, Time: 54.75s\n",
            "Epoch [5/5], Batch [320/1562], Loss: 0.2068, Avg Loss: 0.2116, Time: 56.40s\n",
            "Epoch [5/5], Batch [330/1562], Loss: 0.2210, Avg Loss: 0.2117, Time: 58.06s\n",
            "Epoch [5/5], Batch [340/1562], Loss: 0.2102, Avg Loss: 0.2116, Time: 59.72s\n",
            "Epoch [5/5], Batch [350/1562], Loss: 0.2104, Avg Loss: 0.2116, Time: 61.59s\n",
            "Epoch [5/5], Batch [360/1562], Loss: 0.2231, Avg Loss: 0.2117, Time: 63.64s\n",
            "Epoch [5/5], Batch [370/1562], Loss: 0.2190, Avg Loss: 0.2117, Time: 65.35s\n",
            "Epoch [5/5], Batch [380/1562], Loss: 0.2078, Avg Loss: 0.2118, Time: 67.01s\n",
            "Epoch [5/5], Batch [390/1562], Loss: 0.2153, Avg Loss: 0.2118, Time: 68.66s\n",
            "Epoch [5/5], Batch [400/1562], Loss: 0.2182, Avg Loss: 0.2118, Time: 70.28s\n",
            "Epoch [5/5], Batch [410/1562], Loss: 0.2045, Avg Loss: 0.2117, Time: 71.90s\n",
            "Epoch [5/5], Batch [420/1562], Loss: 0.2123, Avg Loss: 0.2117, Time: 73.84s\n",
            "Epoch [5/5], Batch [430/1562], Loss: 0.2107, Avg Loss: 0.2117, Time: 75.72s\n",
            "Epoch [5/5], Batch [440/1562], Loss: 0.2201, Avg Loss: 0.2116, Time: 77.41s\n",
            "Epoch [5/5], Batch [450/1562], Loss: 0.2242, Avg Loss: 0.2117, Time: 79.13s\n",
            "Epoch [5/5], Batch [460/1562], Loss: 0.2005, Avg Loss: 0.2117, Time: 80.79s\n",
            "Epoch [5/5], Batch [470/1562], Loss: 0.2140, Avg Loss: 0.2118, Time: 82.43s\n",
            "Epoch [5/5], Batch [480/1562], Loss: 0.2140, Avg Loss: 0.2119, Time: 84.07s\n",
            "Epoch [5/5], Batch [490/1562], Loss: 0.2069, Avg Loss: 0.2119, Time: 86.07s\n",
            "Epoch [5/5], Batch [500/1562], Loss: 0.2145, Avg Loss: 0.2119, Time: 87.86s\n",
            "Epoch [5/5], Batch [510/1562], Loss: 0.2307, Avg Loss: 0.2120, Time: 89.51s\n",
            "Epoch [5/5], Batch [520/1562], Loss: 0.2264, Avg Loss: 0.2120, Time: 91.17s\n",
            "Epoch [5/5], Batch [530/1562], Loss: 0.2058, Avg Loss: 0.2119, Time: 92.82s\n",
            "Epoch [5/5], Batch [540/1562], Loss: 0.2181, Avg Loss: 0.2119, Time: 94.51s\n",
            "Epoch [5/5], Batch [550/1562], Loss: 0.2040, Avg Loss: 0.2119, Time: 96.17s\n",
            "Epoch [5/5], Batch [560/1562], Loss: 0.2179, Avg Loss: 0.2119, Time: 98.20s\n",
            "Epoch [5/5], Batch [570/1562], Loss: 0.2136, Avg Loss: 0.2119, Time: 99.96s\n",
            "Epoch [5/5], Batch [580/1562], Loss: 0.2023, Avg Loss: 0.2119, Time: 101.58s\n",
            "Epoch [5/5], Batch [590/1562], Loss: 0.2107, Avg Loss: 0.2119, Time: 103.19s\n",
            "Epoch [5/5], Batch [600/1562], Loss: 0.2179, Avg Loss: 0.2119, Time: 104.84s\n",
            "Epoch [5/5], Batch [610/1562], Loss: 0.2076, Avg Loss: 0.2119, Time: 106.51s\n",
            "Epoch [5/5], Batch [620/1562], Loss: 0.2070, Avg Loss: 0.2118, Time: 108.11s\n",
            "Epoch [5/5], Batch [630/1562], Loss: 0.2249, Avg Loss: 0.2118, Time: 110.17s\n",
            "Epoch [5/5], Batch [640/1562], Loss: 0.2183, Avg Loss: 0.2119, Time: 111.94s\n",
            "Epoch [5/5], Batch [650/1562], Loss: 0.2125, Avg Loss: 0.2119, Time: 113.59s\n",
            "Epoch [5/5], Batch [660/1562], Loss: 0.2069, Avg Loss: 0.2118, Time: 115.23s\n",
            "Epoch [5/5], Batch [670/1562], Loss: 0.2068, Avg Loss: 0.2119, Time: 116.86s\n",
            "Epoch [5/5], Batch [680/1562], Loss: 0.2100, Avg Loss: 0.2118, Time: 118.53s\n",
            "Epoch [5/5], Batch [690/1562], Loss: 0.2076, Avg Loss: 0.2118, Time: 120.23s\n",
            "Epoch [5/5], Batch [700/1562], Loss: 0.2141, Avg Loss: 0.2118, Time: 122.33s\n",
            "Epoch [5/5], Batch [710/1562], Loss: 0.2227, Avg Loss: 0.2119, Time: 124.04s\n",
            "Epoch [5/5], Batch [720/1562], Loss: 0.2049, Avg Loss: 0.2119, Time: 125.67s\n",
            "Epoch [5/5], Batch [730/1562], Loss: 0.2111, Avg Loss: 0.2119, Time: 127.35s\n",
            "Epoch [5/5], Batch [740/1562], Loss: 0.2130, Avg Loss: 0.2119, Time: 129.04s\n",
            "Epoch [5/5], Batch [750/1562], Loss: 0.2247, Avg Loss: 0.2119, Time: 130.70s\n",
            "Epoch [5/5], Batch [760/1562], Loss: 0.2044, Avg Loss: 0.2119, Time: 132.39s\n",
            "Epoch [5/5], Batch [770/1562], Loss: 0.2083, Avg Loss: 0.2119, Time: 134.54s\n",
            "Epoch [5/5], Batch [780/1562], Loss: 0.2010, Avg Loss: 0.2119, Time: 136.19s\n",
            "Epoch [5/5], Batch [790/1562], Loss: 0.2126, Avg Loss: 0.2119, Time: 137.81s\n",
            "Epoch [5/5], Batch [800/1562], Loss: 0.2174, Avg Loss: 0.2119, Time: 139.43s\n",
            "Epoch [5/5], Batch [810/1562], Loss: 0.2114, Avg Loss: 0.2120, Time: 141.05s\n",
            "Epoch [5/5], Batch [820/1562], Loss: 0.2300, Avg Loss: 0.2120, Time: 142.71s\n",
            "Epoch [5/5], Batch [830/1562], Loss: 0.2150, Avg Loss: 0.2120, Time: 144.40s\n",
            "Epoch [5/5], Batch [840/1562], Loss: 0.2150, Avg Loss: 0.2120, Time: 146.65s\n",
            "Epoch [5/5], Batch [850/1562], Loss: 0.2021, Avg Loss: 0.2120, Time: 148.33s\n",
            "Epoch [5/5], Batch [860/1562], Loss: 0.2148, Avg Loss: 0.2120, Time: 149.96s\n",
            "Epoch [5/5], Batch [870/1562], Loss: 0.2031, Avg Loss: 0.2119, Time: 151.62s\n",
            "Epoch [5/5], Batch [880/1562], Loss: 0.2106, Avg Loss: 0.2119, Time: 153.27s\n",
            "Epoch [5/5], Batch [890/1562], Loss: 0.2131, Avg Loss: 0.2119, Time: 154.93s\n",
            "Epoch [5/5], Batch [900/1562], Loss: 0.2185, Avg Loss: 0.2119, Time: 156.59s\n",
            "Epoch [5/5], Batch [910/1562], Loss: 0.2206, Avg Loss: 0.2119, Time: 158.73s\n",
            "Epoch [5/5], Batch [920/1562], Loss: 0.2154, Avg Loss: 0.2119, Time: 160.42s\n",
            "Epoch [5/5], Batch [930/1562], Loss: 0.2117, Avg Loss: 0.2119, Time: 162.06s\n",
            "Epoch [5/5], Batch [940/1562], Loss: 0.2178, Avg Loss: 0.2119, Time: 163.70s\n",
            "Epoch [5/5], Batch [950/1562], Loss: 0.2172, Avg Loss: 0.2119, Time: 165.33s\n",
            "Epoch [5/5], Batch [960/1562], Loss: 0.2130, Avg Loss: 0.2118, Time: 166.96s\n",
            "Epoch [5/5], Batch [970/1562], Loss: 0.2035, Avg Loss: 0.2118, Time: 168.63s\n",
            "Epoch [5/5], Batch [980/1562], Loss: 0.2031, Avg Loss: 0.2118, Time: 170.65s\n",
            "Epoch [5/5], Batch [990/1562], Loss: 0.1988, Avg Loss: 0.2118, Time: 172.32s\n",
            "Epoch [5/5], Batch [1000/1562], Loss: 0.2136, Avg Loss: 0.2117, Time: 173.99s\n",
            "Epoch [5/5], Batch [1010/1562], Loss: 0.2182, Avg Loss: 0.2118, Time: 175.65s\n",
            "Epoch [5/5], Batch [1020/1562], Loss: 0.2103, Avg Loss: 0.2118, Time: 177.31s\n",
            "Epoch [5/5], Batch [1030/1562], Loss: 0.2039, Avg Loss: 0.2117, Time: 178.96s\n",
            "Epoch [5/5], Batch [1040/1562], Loss: 0.2135, Avg Loss: 0.2117, Time: 180.87s\n",
            "Epoch [5/5], Batch [1050/1562], Loss: 0.2194, Avg Loss: 0.2117, Time: 182.72s\n",
            "Epoch [5/5], Batch [1060/1562], Loss: 0.2158, Avg Loss: 0.2118, Time: 184.35s\n",
            "Epoch [5/5], Batch [1070/1562], Loss: 0.2140, Avg Loss: 0.2117, Time: 186.01s\n",
            "Epoch [5/5], Batch [1080/1562], Loss: 0.2225, Avg Loss: 0.2117, Time: 187.67s\n",
            "Epoch [5/5], Batch [1090/1562], Loss: 0.2133, Avg Loss: 0.2117, Time: 189.28s\n",
            "Epoch [5/5], Batch [1100/1562], Loss: 0.2006, Avg Loss: 0.2117, Time: 190.92s\n",
            "Epoch [5/5], Batch [1110/1562], Loss: 0.2107, Avg Loss: 0.2117, Time: 192.87s\n",
            "Epoch [5/5], Batch [1120/1562], Loss: 0.2074, Avg Loss: 0.2117, Time: 194.74s\n",
            "Epoch [5/5], Batch [1130/1562], Loss: 0.2203, Avg Loss: 0.2118, Time: 196.41s\n",
            "Epoch [5/5], Batch [1140/1562], Loss: 0.2104, Avg Loss: 0.2118, Time: 198.03s\n",
            "Epoch [5/5], Batch [1150/1562], Loss: 0.2182, Avg Loss: 0.2118, Time: 199.70s\n",
            "Epoch [5/5], Batch [1160/1562], Loss: 0.2243, Avg Loss: 0.2118, Time: 201.33s\n",
            "Epoch [5/5], Batch [1170/1562], Loss: 0.1926, Avg Loss: 0.2118, Time: 202.96s\n",
            "Epoch [5/5], Batch [1180/1562], Loss: 0.2080, Avg Loss: 0.2118, Time: 204.88s\n",
            "Epoch [5/5], Batch [1190/1562], Loss: 0.2098, Avg Loss: 0.2118, Time: 206.68s\n",
            "Epoch [5/5], Batch [1200/1562], Loss: 0.2070, Avg Loss: 0.2118, Time: 208.38s\n",
            "Epoch [5/5], Batch [1210/1562], Loss: 0.2177, Avg Loss: 0.2118, Time: 210.02s\n",
            "Epoch [5/5], Batch [1220/1562], Loss: 0.2102, Avg Loss: 0.2118, Time: 211.68s\n",
            "Epoch [5/5], Batch [1230/1562], Loss: 0.2125, Avg Loss: 0.2117, Time: 213.35s\n",
            "Epoch [5/5], Batch [1240/1562], Loss: 0.2062, Avg Loss: 0.2117, Time: 215.00s\n",
            "Epoch [5/5], Batch [1250/1562], Loss: 0.2149, Avg Loss: 0.2117, Time: 216.96s\n",
            "Epoch [5/5], Batch [1260/1562], Loss: 0.2018, Avg Loss: 0.2117, Time: 218.71s\n",
            "Epoch [5/5], Batch [1270/1562], Loss: 0.1959, Avg Loss: 0.2117, Time: 220.34s\n",
            "Epoch [5/5], Batch [1280/1562], Loss: 0.2216, Avg Loss: 0.2117, Time: 221.95s\n",
            "Epoch [5/5], Batch [1290/1562], Loss: 0.2127, Avg Loss: 0.2117, Time: 223.62s\n",
            "Epoch [5/5], Batch [1300/1562], Loss: 0.2061, Avg Loss: 0.2117, Time: 225.27s\n",
            "Epoch [5/5], Batch [1310/1562], Loss: 0.2137, Avg Loss: 0.2116, Time: 226.99s\n",
            "Epoch [5/5], Batch [1320/1562], Loss: 0.1985, Avg Loss: 0.2116, Time: 229.07s\n",
            "Epoch [5/5], Batch [1330/1562], Loss: 0.2146, Avg Loss: 0.2116, Time: 230.77s\n",
            "Epoch [5/5], Batch [1340/1562], Loss: 0.2156, Avg Loss: 0.2117, Time: 232.41s\n",
            "Epoch [5/5], Batch [1350/1562], Loss: 0.2170, Avg Loss: 0.2117, Time: 234.05s\n",
            "Epoch [5/5], Batch [1360/1562], Loss: 0.2020, Avg Loss: 0.2116, Time: 235.69s\n",
            "Epoch [5/5], Batch [1370/1562], Loss: 0.2224, Avg Loss: 0.2116, Time: 237.31s\n",
            "Epoch [5/5], Batch [1380/1562], Loss: 0.2141, Avg Loss: 0.2116, Time: 238.95s\n",
            "Epoch [5/5], Batch [1390/1562], Loss: 0.2196, Avg Loss: 0.2116, Time: 241.09s\n",
            "Epoch [5/5], Batch [1400/1562], Loss: 0.2168, Avg Loss: 0.2116, Time: 242.84s\n",
            "Epoch [5/5], Batch [1410/1562], Loss: 0.1984, Avg Loss: 0.2115, Time: 244.49s\n",
            "Epoch [5/5], Batch [1420/1562], Loss: 0.2075, Avg Loss: 0.2115, Time: 246.13s\n",
            "Epoch [5/5], Batch [1430/1562], Loss: 0.2080, Avg Loss: 0.2115, Time: 247.73s\n",
            "Epoch [5/5], Batch [1440/1562], Loss: 0.2051, Avg Loss: 0.2114, Time: 249.35s\n",
            "Epoch [5/5], Batch [1450/1562], Loss: 0.2073, Avg Loss: 0.2114, Time: 250.95s\n",
            "Epoch [5/5], Batch [1460/1562], Loss: 0.2165, Avg Loss: 0.2114, Time: 253.00s\n",
            "Epoch [5/5], Batch [1470/1562], Loss: 0.2099, Avg Loss: 0.2114, Time: 254.76s\n",
            "Epoch [5/5], Batch [1480/1562], Loss: 0.2190, Avg Loss: 0.2114, Time: 256.47s\n",
            "Epoch [5/5], Batch [1490/1562], Loss: 0.2035, Avg Loss: 0.2114, Time: 258.13s\n",
            "Epoch [5/5], Batch [1500/1562], Loss: 0.2044, Avg Loss: 0.2114, Time: 259.78s\n",
            "Epoch [5/5], Batch [1510/1562], Loss: 0.2128, Avg Loss: 0.2114, Time: 261.42s\n",
            "Epoch [5/5], Batch [1520/1562], Loss: 0.2093, Avg Loss: 0.2114, Time: 263.06s\n",
            "Epoch [5/5], Batch [1530/1562], Loss: 0.2084, Avg Loss: 0.2114, Time: 265.16s\n",
            "Epoch [5/5], Batch [1540/1562], Loss: 0.2077, Avg Loss: 0.2113, Time: 266.93s\n",
            "Epoch [5/5], Batch [1550/1562], Loss: 0.2135, Avg Loss: 0.2113, Time: 268.63s\n",
            "Epoch [5/5], Batch [1560/1562], Loss: 0.2118, Avg Loss: 0.2113, Time: 270.29s\n",
            "End of Epoch [5/5], Avg Loss: 0.2113, Total Time: 270.54s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def gumbel_distribution_sample(model, start_seq, max_len, temperature=1.0):\n",
        "    model.eval()\n",
        "    chars = [char_to_idx[ch] for ch in start_seq]\n",
        "    input_seq = torch.tensor(chars, dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "    hidden = model.init_hidden(1)\n",
        "    avg_log_prob = 0\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        output, hidden = model(input_seq, hidden)\n",
        "        logits = output[:, -1, :] / temperature\n",
        "\n",
        "        # Add Gumbel noise\n",
        "        gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits)))\n",
        "        noisy_logits = logits + gumbel_noise\n",
        "        next_char_idx = torch.argmax(noisy_logits).item()\n",
        "        chars.append(next_char_idx)\n",
        "        input_seq = torch.tensor([[next_char_idx]], dtype=torch.long)\n",
        "\n",
        "        # Calculate log probability\n",
        "        log_probs = F.log_softmax(noisy_logits, dim=-1)\n",
        "        avg_log_prob += log_probs[0, next_char_idx].item()\n",
        "\n",
        "    avg_perplexity = torch.exp(torch.tensor(-avg_log_prob / max_len))\n",
        "\n",
        "    return ''.join(idx_to_char[idx] for idx in chars), avg_perplexity.item()\n",
        "\n",
        "def top_k_sample(model, start_seq, max_len, k=5):\n",
        "    model.eval()\n",
        "    chars = [char_to_idx[ch] for ch in start_seq]\n",
        "    input_seq = torch.tensor(chars, dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "    hidden = model.init_hidden(1)\n",
        "    avg_log_prob = 0\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        output, hidden = model(input_seq, hidden)\n",
        "        logits = output[:, -1, :]\n",
        "        probabilities = F.softmax(logits, dim=-1)\n",
        "        top_k_probs, top_k_indices = torch.topk(probabilities, k)\n",
        "        next_char_idx = top_k_indices[0, torch.multinomial(top_k_probs, 1)].item()\n",
        "        chars.append(next_char_idx)\n",
        "        input_seq = torch.tensor([[next_char_idx]], dtype=torch.long)\n",
        "\n",
        "        # Calculate log probability\n",
        "        log_probs = F.log_softmax(logits, dim=-1)\n",
        "        avg_log_prob += log_probs[0, next_char_idx].item()\n",
        "\n",
        "    avg_perplexity = torch.exp(torch.tensor(-avg_log_prob / max_len))\n",
        "\n",
        "    return ''.join(idx_to_char[idx] for idx in chars), avg_perplexity.item()\n",
        "\n",
        "def greedy_sample(model, start_seq, max_len):\n",
        "    model.eval()\n",
        "    chars = [char_to_idx[ch] for ch in start_seq]\n",
        "    input_seq = torch.tensor(chars, dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "    hidden = model.init_hidden(1)\n",
        "    avg_log_prob = 0\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        output, hidden = model(input_seq, hidden)\n",
        "        logits = output[:, -1, :]\n",
        "        next_char_idx = torch.argmax(logits).item()\n",
        "        chars.append(next_char_idx)\n",
        "        input_seq = torch.tensor([[next_char_idx]], dtype=torch.long)\n",
        "\n",
        "        # Calculate log probability\n",
        "        log_probs = F.log_softmax(logits, dim=-1)\n",
        "        avg_log_prob += log_probs[0, next_char_idx].item()\n",
        "\n",
        "    avg_perplexity = torch.exp(torch.tensor(-avg_log_prob / max_len))\n",
        "\n",
        "    return ''.join(idx_to_char[idx] for idx in chars), avg_perplexity.item()\n",
        "\n",
        "start_seq = \"To be or not to be\"\n",
        "\n",
        "gumbel_distribution_text, gumbel_distribution_perplexity = gumbel_distribution_sample(model, start_seq, 100)\n",
        "top_k_text, top_k_perplexity = top_k_sample(model, start_seq, 100)\n",
        "greedy_text, greedy_perplexity = greedy_sample(model, start_seq, 100)\n",
        "\n",
        "gumbel_distribution_generated_text = f\"gumbel max-trick sampling:\\n\\n{gumbel_distribution_text} \\n\\nperplexity: {gumbel_distribution_perplexity}\\n\"\n",
        "top_k_generated_text = f\"top-k sampling:\\n{top_k_text} \\n\\nperplexity: {top_k_perplexity}\\n\"\n",
        "greedy_generated_text = f\"greedy sampling:\\n{greedy_text} \\n\\nperplexity: {greedy_perplexity}\\n\"\n",
        "\n",
        "print(gumbel_distribution_generated_text)\n",
        "print()\n",
        "print(greedy_generated_text)\n",
        "print()\n",
        "print(top_k_generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOgxXHtSQLZc",
        "outputId": "0d5c66fb-442d-488a-d3fc-ec36ee46cd36"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gumbel max-trick sampling:\n",
            "\n",
            "To be or not to be their loves and safeguard\n",
            "Of what that want might ruin.\n",
            "\n",
            "MENENIUS:\n",
            "Noble lady!\n",
            "Come, go with us; sp \n",
            "\n",
            "perplexity: 1.042262077331543\n",
            "\n",
            "\n",
            "greedy sampling:\n",
            "To be or not to be their bedfellow.\n",
            "Worthy Cominius, speak.\n",
            "Nay, keep your place.\n",
            "\n",
            "First Senator:\n",
            "Sit, Coriolanus; nev \n",
            "\n",
            "perplexity: 1.030051350593567\n",
            "\n",
            "\n",
            "top-k sampling:\n",
            "To be or not to be say?\n",
            "\n",
            "First Citizen:\n",
            "It was an answer: how apply you to the people's voices,\n",
            "Allow their officers a \n",
            "\n",
            "perplexity: 1.127898931503296\n",
            "\n"
          ]
        }
      ]
    }
  ]
}